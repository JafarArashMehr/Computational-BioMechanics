<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>7. Fundamentals of Machine Learning &mdash; Jafar Arash Mehr 2022 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="8. Machine Learning in Diagnosis" href="LDA.html" />
    <link rel="prev" title="6. Linear Regression" href="Linear%20Regression.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html">
            <img src="../_static/profile.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Intro.html"><strong>What is this document about?</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="resource.html">Useful Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html">Required Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperelasticity.html"><strong>1. Hyperelasticity</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-physics.html"><strong>2. Multi-Physics Problem</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="drug-delivery.html"><strong>3. Drug Delivery Simulation</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html"><strong>4. Inverse FEM (Optimization)</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Statistical%20Analysis.html"><strong>5. Statistical Analysis</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Linear%20Regression.html"><strong>6. Linear Regression</strong></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#"><strong>7. Fundamentals of Machine Learning</strong></a><ul>
<li class="toctree-l2"><a class="reference internal" href="#feature-engineering">7.1. Feature Engineering</a></li>
<li class="toctree-l2"><a class="reference internal" href="#imbalanced-datasets">7.2. Imbalanced Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="#naive-bayes-classifiers">7.3. Naive Bayes classifiers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#building-a-bayes-model">7.3.1 Building a Bayes’ model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#k-means-algorithm">7.4. K-means algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#example-of-k-means-for-color-compression">7.4.1 Example of K-means for color compression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation-of-the-k-mean">7.4.2 Evaluation of the K-mean</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-a-k-means-model">7.4.3 Build a K-means model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#tree-based-learning">7.5. Tree-based learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#structure-of-a-classification-tree">7.5.1 Structure of a classification tree</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hyperparameter-tuning">7.5.2 Hyperparameter tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-validation">7.5.3 Model validation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-a-decision-tree-in-python">7.5.4 Build a decision tree in Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bagging">7.5.5 Bagging</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#random-forest">7.6. Random Forest</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#build-a-random-forest-model-in-python">7.6.1 Build a random forest model in Python</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="LDA.html"><strong>8. Machine Learning in Diagnosis</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="contact.html">About the Author</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Jafar Arash Mehr</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li><strong>7. Fundamentals of Machine Learning</strong></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/rst/ML.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="fundamentals-of-machine-learning">
<h1><strong>7. Fundamentals of Machine Learning</strong><a class="headerlink" href="#fundamentals-of-machine-learning" title="Permalink to this heading"></a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All of required CSV files are available in the folder: <strong>Machine-Learning</strong></p>
</div>
<p>Machine learning (ML) is a computational tool to discover patterns in data and make informed predictions.
There are two main types of ML including:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>1. Supervised ML:</strong> This technique uses labeled dataset to train algorithms to classify or predict something</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>2. Unsupervised ML:</strong> This technique uses algorithms to analyze and cluster unlabeled dataset</p>
<p>Two types of machine learning are linear regression and decision tree regression. The linear regression is used for continuous variable while the decision tree is used for both continuous and categorical variables.</p>
<section id="feature-engineering">
<h2>7.1. Feature Engineering<a class="headerlink" href="#feature-engineering" title="Permalink to this heading"></a></h2>
<p>The process of using practical, statistical and data science knowledge to select, transform or extract properties from raw data.</p>
<p><strong>1. Feature Selection</strong></p>
<p>Feature selection is the process of picking variables from a dataset that will be used as predictor variables for your model. With very large datasets, there are dozens if not hundreds of features for each observation in the data. Using all of the features in a dataset often doesn’t give any performance boost. In fact, it may actually hurt performance by adding complexity and noise to the model. Therefore, choosing the features to use for the model is an important part of the model development process.</p>
<p>Generally, there are three types of features:</p>
<ol class="arabic simple">
<li><p>Predictive: Features that by themselves contain information useful to predict the target</p></li>
<li><p>Interactive: Features that are not useful by themselves to predict the target variable, but become predictive in conjunction with other features</p></li>
<li><p>Irrelevant: Features that don’t contain any useful information to predict the target</p></li>
</ol>
<p>You want predictive features, but a predictive feature can also be a redundant feature. Redundant features are highly correlated with other features and therefore do not provide the model with any new information—for example, the steps you took in a day, may be highly correlated with the calories you burned. The goal of feature selection is to find the predictive and interactive features and exclude redundant and irrelevant features.</p>
<p><strong>2. Feature Transformation</strong></p>
<p>Feature transformation is a process where you take features that already exist in the dataset, and alter them so that they’re better suited to be used for training the model. Some of the transformation techniques include:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Normalization</strong></p>
<p>Normalization (e.g., MinMaxScaler in scikit-learn) transforms data to reassign each value to fall within the range [0, 1]. When applied to a feature, the feature’s minimum value becomes zero and its maximum value becomes one. All other values scale to somewhere between them. The formula for this transformation is:</p>
<div class="math notranslate nohighlight" id="eq-120">
<span id="equation-eq-120"></span><span class="eqno">(105)<a class="headerlink" href="#eq-120" title="Permalink to this equation"></a></span>\[ x_{i,normalized}= \frac {x_i-x_{min}}{x_{max}-x_{min}}\]</div>
<p>Features with higher magnitudes of scale will be more influential in some machine learning algorithms, like K-means, where  Euclidean distances between data points are calculated with the absolute value of the features (so large feature values have major effects, compared to small feature values). By min-max scaling (normalizing) each feature, they are both reduced to the same range</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Standardization</strong></p>
<p>Another type of scaling is called standardization (e.g., StandardScaler in scikit-learn). Standardization transforms each value within a feature so they collectively have a mean of zero and a standard deviation of one. To do this, for each value, subtract the mean of the feature and divide by the feature’s standard deviation:</p>
<div class="math notranslate nohighlight" id="eq-121">
<span id="equation-eq-121"></span><span class="eqno">(106)<a class="headerlink" href="#eq-121" title="Permalink to this equation"></a></span>\[ x_{i,standardized}= \frac {x_{i}-x_{mean}}{x_{standard.dev}}\]</div>
<p>This method is useful because it centers the feature’s values on zero, which is useful for some machine learning algorithms. It also preserves outliers, since it does not place a hard cap on the range of possible values.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Encoding</strong></p>
<p>Another form of feature transformation is known as encoding. Variable encoding is the process of converting categorical data to numerical data. Consider the bank churn dataset. The original data has a feature called “Geography”, whose values represent each customer’s country of residence—France, Germany, or Spain. Most machine learning methodologies cannot extract meaning from strings. Encoding transforms the strings to numbers that can be interpreted mathematically. the feature would typically be encoded into binary. This process requires that a column be added to represent each possible class contained within the feature.</p>
<p>In this example, if the geography is France, we put 1 under France column and zero under Germany and Spain columns. Tools commonly used to do this include <strong>pandas.get_dummies()</strong> and <strong>OneHotEncoder()</strong>.  Often methods drop one of the columns to avoid having redundant information in the dataset .</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Keep in mind that some features may be inferred to be numerical by Python of other frameworks but still represent a category. For example, suppose you had a dataset with people assigned to different arbitrary groups: 1, 2, and 3:</p>
<p>A different kind of encoding can be used for features that contain discrete or ordinal values. This is called ordinal encoding. It is used when the values do contain inherent order or ranking. For instance, consider a “Temperature” column that has values of cold, warm, and hot. In this case, <strong>ordinal encoding</strong> could reassign these classes to 0, 1, and 2.</p>
</div>
<p><strong>3. Feature Extraction</strong></p>
<p>Feature extraction involves producing new features from existing ones, with the goal of having features that deliver more predictive power to your model. While there is some overlap between extraction and transformation colloquially, the main difference is that a new feature is created from one or more other features rather than simply changing one that already exists.</p>
<p>Feature extraction involves combining existing columns meaningfully to construct new features that would help improve prediction.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is important to check for class balance in a dataset, particularly in the context of feature engineering and predictive modeling. If the target column in a dataset has more than 90% of its values belonging to one class, it is recommended to redistribute the data; otherwise, once a model is trained on the imbalanced data and predictions are made, the predictions may be biased.</p>
</div>
<p>We need to understand more about what class imbalance is, when it becomes problematic, and some issues that can arise if it isn’t addressed. We need to learn two of the general categories for balancing datasets, upsampling, and downsampling.</p>
</section>
<section id="imbalanced-datasets">
<h2>7.2. Imbalanced Datasets<a class="headerlink" href="#imbalanced-datasets" title="Permalink to this heading"></a></h2>
<p>We need to understand more about what class imbalance is, when it becomes problematic, and some issues that can arise if it isn’t addressed. We need to learn two of the general categories for balancing datasets, upsampling, and downsampling.</p>
<p>For categorical variables, the different possible values that each can take are known as classes. This is true for both predictor variables and target variables. If you were trying to classify the weather on a given day as rainy or sunny, these would be considered two classes. The number of classes is equal to the number of unique values in the variable.</p>
<p>The number of occurrences of each class in the target variable is known as the <strong>class distribution</strong>. When predicting a categorical target, problems can arise when the class distribution is highly imbalanced. If there are not enough instances of certain outcomes, the resulting model might not be very good at predicting that class.</p>
<p>This is where the process of class balancing comes in, a process that allows you to manipulate the dataset, or the model fitting process, in a way that the class imbalance that exists doesn’t affect the performance of the resulting model. In general, in an unbalanced dataset,  at least one of the classes in the target variable occurs much less frequently than another.</p>
<p>For example if we want to create a model that will classify emails that are sent to the company either as “spam” or “not spam.” The company receives thousands and thousands of emails daily, not to mention all the emails they’ve received in the past. However, the number of examples of spam is very small. For the sake of this example, say that 10 emails per day are identified as spam manually. This doesn’t have the makings of a very good model. With so few examples of spam relative to examples of not spam, the model can have difficulty detecting the minority class, resulting in its being biased toward the majority class or possibly never predicting the minority class at all.</p>
<p>Class balancing refers to the process of changing the data by altering the number of samples in order to make the ratios of classes in the target variable less asymmetrical.There are two general strategies to balance a dataset, and the method that is better to use generally is decided by how much data you have in the first place:</p>
<p><strong>1. Downsampling</strong></p>
<p>Downsampling is the process of making the minority class represent a larger share of the whole dataset simply by removing observations from the majority class. It is mostly used with datasets that are large. But how large is large enough to consider downsampling? Tens of thousands is a good rule of thumb, but ultimately this needs to be validated by checking that model performance doesn’t deteriorate as you train with less data.</p>
<p>One way to downsample data is by selecting some observations randomly from the majority class and removing them from the dataset. There are some more technical, mathematically based methods, but random removal works very well in most cases.</p>
<p><strong>2. Upsampling</strong></p>
<p>Upsampling is basically the opposite of downsampling, and is done when the dataset doesn’t have a very large number of observations in the first place. Instead of removing observations from the majority class, you increase the number of observations in the minority class.</p>
<p>There are a couple of ways to go about this. The first and easiest method is to duplicate samples of the minority class. Depending on how many such observations you have compared to the majority class, you might have to duplicate each sample several times over.</p>
<p>Another way is to create synthetic, unique observations of the minority class. On the surface, there seems to be something wrong about editing the dataset like this, but if the goal is simply to train a better-performing model, it can be a valid and useful technique. You can generate these synthetic observations from the observations that currently exist. For example, you can average two points of the minority class and add the result to the dataset as a sample of the minority class. This can even be done algorithmically using publicly available Python packages.</p>
<p><strong>How to implement it</strong></p>
<p>In both cases, upsampling and downsampling, it is important to leave a partition of test data that is unaltered by the sampling adjustment. You do this because you need to understand how well your model predicts on the actual class distribution observed in the world that your data represents. In the case of the spam detector example, it’s great if your model can score well on resampled data that is 80% not spam and 20% spam, but you need to know how it will work when deployed in the real world, where spam emails are much less frequent. This is why the test holdout data is not rebalanced.</p>
<p><strong>Consequences</strong></p>
<p>Manipulating the class distribution of your data doesn’t come without consequences. The first consequence is the risk of your model predicting the minority class more than it should. By class rebalancing to get your model to recognize the minority class, you might build a model that over-recognizes that class. That happens because, in training, it learned a data distribution that is not what it will in practice in the real world.</p>
<p>Changing the class distribution affects the underlying class probabilities learned by the model. Consider, for example, how the <strong>Naive Bayes</strong> algorithm works. To calculate the probability of a class, given the features, it uses the background probability of a class in the data.</p>
</section>
<section id="naive-bayes-classifiers">
<h2>7.3. Naive Bayes classifiers<a class="headerlink" href="#naive-bayes-classifiers" title="Permalink to this heading"></a></h2>
<p>This is a supervised classification technique based on Baye’s theorem with an assumption of independence among predictor.When it comes to supervised machine learning techniques, Naive Bayes is a perfect example of that. Naive Bayes models remain relevant because they are simple, fast, and good predictors. In certain situations, Naive Bayes is also known to outperform much more advanced classification methods. Even if a more advanced model is required, producing a Naive Bayes model can also be a great starting point. Therefore, the Naive Bayes classifier is something that every data professional needs in their machine learning skill set.</p>
<p>A Naive Bayes model is a supervised learning technique used for classification problems. As with all supervised learning techniques, to create a Naive Bayes model you must have a response variable and a set of predictor variables to train the model. The Naive Bayes algorithm is based on Bayes’ Theorem, an equation that can be used to calculate the probability of an outcome or class, given the values of predictor variables. This value is known as the posterior probability. That probability is calculated using three values:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The probability of the outcome overall P(A)</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The probability of the value of the predictor variable P(B)</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The conditional probability P(B|A) (Note: P(B|A) is interpreted as the probability of B, given A.)</p>
<p>The probability of the outcome overall, P(A), is multiplied by the conditional probability, P(B|A). This result is then divided by the probability of the predictor variable, P(B), to obtain the posterior probability.</p>
<p><strong>Bayes’ Theorem:</strong></p>
<div class="math notranslate nohighlight" id="eq-213">
<span id="equation-eq-213"></span><span class="eqno">(107)<a class="headerlink" href="#eq-213" title="Permalink to this equation"></a></span>\[ P(B|A)= \frac{P(B|A).P(A)}{P(B)}\]</div>
<p>The goal of Bayes’ Theorem is to find the probability of an event, A, given that another event B is true. In the context of a predictive model, the class label would be A and the predictor variable would be B. P(A) is considered the prior probability of event A before any evidence (feature) is seen. Then, P(A|B) is the posterior probability, or the probability of the class label after the evidence (feature) has been seen.</p>
<p>In a predictive model, this calculation is carried out for each feature, for each class. Then the probabilities are multiplied together. The class with the highest resulting product is the model’s final prediction for that sample. These models make a number of assumptions about the data to work properly. One of the most important is the assumption that each predictor variable (different Bs in the formula) is independent from the others, conditional on the class. This is called conditional independence. Variables B and C are independent of one another on the condition that a third variable, A, exists such that:</p>
<div class="math notranslate nohighlight" id="eq-214">
<span id="equation-eq-214"></span><span class="eqno">(108)<a class="headerlink" href="#eq-214" title="Permalink to this equation"></a></span>\[ P(B|C,A)= P(B,A)\]</div>
<p>This equation can be interpreted as “the probability of B, given C and A, is equal to the probability of B, given A.” In other words, given A, introducing C does not change the probability of B. Note that two features can only be considered conditionally independent of each other when considered in relation to a third variable. Furthermore, it’s possible for variables B and C to be conditionally independent of one another with respect to A, but not with respect to another variable, say, Z.</p>
<p>In Naive Bayes, the predictor variables (B and C in the equation above) are assumed to be conditionally independent of each other, given the target variable (A). This is an assumption that very often is not actually true. However, Naive Bayes models still often perform well in spite of the data violating the assumption. The assumption is made to simplify the model. Otherwise, long probabilistic chains would have to be calculated to determine the probability of a feature’s value with respect to the values of every other variable.</p>
<p>Another assumption of the data is that no predictor variable has any more predictive power than any other predictor. In other words, the individual predictor variables are assumed to contribute equally to the model’s prediction. Like the assumption of class-conditional independence between the features, this assumption is also often violated by real-world data, but Naive Bayes still often proves a good model in spite of this.</p>
<p><strong>Pros</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Of all the classification algorithms that are still used today, Naive Bayes is one of the simplest. However, it is still able to produce valuable results. Its simplicity comes as an asset because it is one of the most straightforward algorithms to implement. In spite of their assumptions, Naive Bayes classifiers work quite well in many industry problems, most famously for document analysis/classification and spam filtering.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Additionally, the training time for a Naive Bayes model can sometimes be drastically lower than for other models because the calculations that are needed to make it work are relatively cheap in terms of computer resource consumption. This also means it is highly scalable and able to work with large increases in the amount of data it must handle.</p>
<p><strong>Cons</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> One of the biggest problems with Naive Bayes is the data assumptions that were mentioned earlier. Few datasets have truly conditionally independent features—it is something that is very rare in the world today. However, Naive Bayes models can still perform well even if the assumption of conditional independence is violated.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Another issue that could arise is what is known as the “zero frequency” problem. This occurs when the dataset you’re using has no occurrences of a class label and some value of a predictor variable together. This would mean that there is a probability of zero. Since the final posterior probability is found by multiplying all of the individual probabilities together, the probability of zero would automatically make the result zero. Library implementations of the algorithms account for this by adding a negligible value to each variable count (usually 1) to ensure a non-zero probability.</p>
<section id="building-a-bayes-model">
<h3>7.3.1 Building a Bayes’ model<a class="headerlink" href="#building-a-bayes-model" title="Permalink to this heading"></a></h3>
<p>We will build our own Naive Bayes model in this example. Naive Bayes models can be valuable to use any time you are doing work with predictions because they give you a way to account for new information. In today’s world, where data is constantly evolving, modeling with Naive Bayes can help you adapt quickly and make more accurate predictions about what could occur.</p>
<p>In this example we will work for a firm that provides insights for management and coaches in the National Basketball Association (NBA).The league is interested in retaining players who can last in the high-pressure environment of professional basketball and help the team be successful over time.</p>
<p>The data for this activity consists of performance statistics from each player’s rookie year. There are 1,341 observations, and each observation in the data represents a different player in the NBA. Your target variable is a Boolean value that indicates whether a given player will last in the league for five years.</p>
<p><strong>Feature engineering</strong></p>
<p>First we need to conduct feature engineering to determine which attributes in the data can best predict certain measures. We will analyze a subset of data that contains information about NBA players and their performance records. Next we will conduct feature engineering to determine which features will most effectively predict whether a player’s NBA career will last at least five years.</p>
<p>First we need to import the <strong>panda</strong> library:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
<p>The dataset is a .csv file named nba-players.csv. It consists of performance records for a subset of NBA players.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;nba-players.csv&quot;</span><span class="p">,</span> <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the description for each column for this dataset:</p>
<figure class="align-center">
<img alt="../_images/53.png" src="../_images/53.png" />
</figure>
<p>In the dataset, the name column is categorical, and the rest of the columns are numerical.</p>
<p><strong>Check for missing values</strong></p>
<p>Now, review the data to determine whether it contains any missing values. Begin by displaying the number of missing values in each column. After that, use isna() to check whether each value in the data is missing. Finally, use sum() to aggregate the number of missing values per column:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>We a see all columns in this dataset have 0 missing values.Checking for missing values is an important step in data exploration. Missing values are not particularly useful, so it’s important to handle them by cleaning the data.</p>
<p><strong>Statistical tests</strong></p>
<p>Next,use a statistical technique to check the class balance in the data. To understand how balanced the dataset is in terms of class, display the percentage of values that belong to each class in the target column. In this context, class 1 indicates an NBA career duration of at least five years, while class 0 indicates an NBA career duration of less than five years.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display percentage (%) of values for each class (1, 0) represented in the target column of this dataset.</span>

<span class="n">data</span><span class="p">[</span><span class="s2">&quot;target_5yrs&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span>
</pre></div>
</div>
<p>Here is the output of the above line:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span>    <span class="mf">62.014925</span>
<span class="mi">0</span>    <span class="mf">37.985075</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> About 62% of the values in the target columm belong to class 1, and about 38% of the values belong to class 0. In other words, about 62% of players represented by this data have an NBA career duration of at least five years, and about 38% do not.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The dataset is not perfectly balanced, but an exact 50-50 split is a rare occurance in datasets, and a 62-38 split is not too imbalanced. However, if the majority class made up 90% or more of the dataset, then that would be of concern, and it would be prudent to address that issue through techniques like upsampling and downsampling.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If there is a lot more representation of one class than another, then the model may be biased toward the majority class. When this happens, the predictions may be inaccurate.</p>
</div>
<p><strong>Results and evaluation</strong></p>
<p>Now, perform feature engineering, with the goal of identifying and creating features that will serve as useful predictors for the target variable, target_5yrs</p>
<p>Here are a couple notes:</p>
<p>You should avoid selecting the name column as a feature. A player’s name is not helpful in determining their career duration. Moreover, it may not be ethical or fair to predict a player’s career duration based on a name.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The number of games a player has played in may not be as important in determining their career duration as the number of points they have earned. While you could say that someone who has played in more games may have more practice and experience, the points they earn during the games they played in would speak more to their performance as a player. This, in turn, would influence their career duration. So, the gp column on its own may not be a helpful feature. However, gp and pts could be combined to get the total number of points earned across the games played, and that result could be a helpful feature. That approach can be implemented later in the feature engineering process—in feature extraction.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> If the number of points earned across games will be extracted as a feature, then that could be combined with the number of minutes played across games (min) to extract another feature. This could be a measure of players’ efficiency and could help in predicting players’ career duration. min on its own may not be useful as a feature for the same reason as gp, mentioned above.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> There are three different columns that give information about field goals. The percent of field goals a player makes (ft) says more about their performance than the number of field goals they make (ftm) or the number of field goals they attempt (fta). The percent gives more context, as it takes into account both how many field goals a player successfully made and how many field goals they attempted in total. This allows for a more meaningful comparison between players. The same logic applies to the percent of three-point field goals made, as well as the percent of free throws made.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> There are columns for the number offensive rebounds (oreb), the number of defensive rebounds (dreb), and the number of rebounds overall (reb). Because the overall number of rebounds should already incorporate both offensive and defensive rebounds, it would make sense to use the overall as a feature.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The number of assists (ast), steals (stl), blocks (blk), and turnovers (tov) also provide information about how well players are performing in games, and thus, could be helpful in predicting how long players last in the league.</p>
<p>Therefore, at this stage of the feature engineering process, it would be most effective to select the following columns:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">gp</span><span class="p">,</span> <span class="nb">min</span><span class="p">,</span> <span class="n">pts</span><span class="p">,</span> <span class="n">fg</span><span class="p">,</span> <span class="mi">3</span><span class="n">p</span><span class="p">,</span> <span class="n">ft</span><span class="p">,</span> <span class="n">reb</span><span class="p">,</span> <span class="n">ast</span><span class="p">,</span> <span class="n">stl</span><span class="p">,</span> <span class="n">blk</span><span class="p">,</span> <span class="n">tov</span><span class="o">.</span>
</pre></div>
</div>
<p>Next, select the columns you want to proceed with. Make sure to include the target column, target_5yrs. Display the first few rows to confirm they are as expected:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Select the columns to proceed with and save the DataFrame in new variable `selected_data`.</span>
<span class="c1"># Include the target column, `target_5yrs`.</span>


<span class="n">selected_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[[</span><span class="s2">&quot;gp&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;pts&quot;</span><span class="p">,</span> <span class="s2">&quot;fg&quot;</span><span class="p">,</span> <span class="s2">&quot;3p&quot;</span><span class="p">,</span> <span class="s2">&quot;ft&quot;</span><span class="p">,</span> <span class="s2">&quot;reb&quot;</span><span class="p">,</span> <span class="s2">&quot;ast&quot;</span><span class="p">,</span> <span class="s2">&quot;stl&quot;</span><span class="p">,</span> <span class="s2">&quot;blk&quot;</span><span class="p">,</span> <span class="s2">&quot;tov&quot;</span><span class="p">,</span> <span class="s2">&quot;target_5yrs&quot;</span><span class="p">]]</span>
</pre></div>
</div>
<p><strong>Feature transformation</strong></p>
<p>An important aspect of feature transformation is feature encoding. If there are categorical columns that you would want to use as features, those columns should be transformed to be numerical. This technique is also known as feature encoding.</p>
<p>Many types of models are designed in a way that requires the data coming in to be numerical. So, transforming categorical features into numerical features is an important step.
In this particular dataset, name is the only categorical column and the other columns are numerical (discussed in the exemplar response to Question 2). Given that <span class="math notranslate nohighlight">\(name\)</span> is not selected as a feature, all of the features that are selected at this point are already numerical and do not require transformation.</p>
<p><strong>Feature extraction</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The gp, pts, min columns lend themselves to feature extraction.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> gp represents the total number of games a player has played in, and pts represents the average number of points the player has earned per game. It might be helpful to combine these columns to get the total number of points the player has earned across the games and use the result as a new feature, which could be added into a new column named total_points. The total points earned by a player can reflect their performance and shape their career longevity.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The min column represents the number of minutes played across games. total_points could be combined with min to extract a new feature: points earned per minute. This can be considered a measure of player efficiency, which could shape career duration. This feature can be added into a column named efficiency.</p>
<p>Extract two features that you think would help predict target_5yrs. Then, create a new variable named ‘extracted_data’ that contains features from ‘selected_data’, as well as the features being extracted:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extract two features that would help predict target_5yrs.</span>
<span class="c1"># Create a new variable named `extracted_data`.</span>


<span class="c1"># Make a copy of `selected_data`</span>
<span class="n">extracted_data</span> <span class="o">=</span> <span class="n">selected_data</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Add a new column named `total_points`;</span>
<span class="c1"># Calculate total points earned by multiplying the number of games played by the average number of points earned per game</span>
<span class="n">extracted_data</span><span class="p">[</span><span class="s2">&quot;total_points&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">extracted_data</span><span class="p">[</span><span class="s2">&quot;gp&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">extracted_data</span><span class="p">[</span><span class="s2">&quot;pts&quot;</span><span class="p">]</span>

<span class="c1"># Add a new column named `efficiency`. Calculate efficiency by dividing the total points earned by the total number of minutes played, which yields points per minute</span>
<span class="n">extracted_data</span><span class="p">[</span><span class="s2">&quot;efficiency&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">extracted_data</span><span class="p">[</span><span class="s2">&quot;total_points&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">extracted_data</span><span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, to prepare for the Naive Bayes model.Now we need to clean the extracted data and ensure ensure it is concise. Naive Bayes involves an assumption that features are independent of each other given the class. In order to satisfy that criteria, if certain features are aggregated to yield new features, it may be necessary to remove those original features. Therefore, drop the columns that were used to extract new features:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove any columns from `extracted_data` that are no longer needed.</span>
<span class="c1"># Remove `gp`, `pts`, and `min` from `extracted_data`.</span>
<span class="n">extracted_data</span> <span class="o">=</span> <span class="n">extracted_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;gp&quot;</span><span class="p">,</span> <span class="s2">&quot;pts&quot;</span><span class="p">,</span> <span class="s2">&quot;min&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Next, export the extracted data as a new .csv file. You will use this in a later lab:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Export the extracted data.</span>
<span class="n">extracted_data</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="s2">&quot;extracted_nba_players_data.csv&quot;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Build a Naive Bayes model</strong></p>
<p>We conducted feature engineering to determine which features would most effectively predict a player’s career duration. We will now use those insights to build a model that predicts whether a player will have an NBA career lasting five years or more.</p>
<p>The data for this activity consists of performance statistics from each player’s rookie year. There are 1,341 observations, and each observation in the data represents a different player in the NBA. Your target variable is a Boolean value that indicates whether a given player will last in the league for five years. Since we previously performed feature engineering on this data, it is now ready for modeling.</p>
<p>First we need to import required packages. Begin with your import statements. Of particular note here are pandas and from sklearn, naive_bayes, model_selection, and metrics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">naive_bayes</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">model_selection</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
</pre></div>
</div>
<p>We will use the dataset we created in the previous step (feature engineering):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load extracted_nba_players.csv into a DataFrame called extracted_data.</span>
<span class="n">extracted_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;extracted_nba_players.csv&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Model preparation</strong></p>
<p>We should separately define the target variable (target_5yrs) and the features. In other words, we need to isolate the target and predictor variables:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the y (target) variable.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">extracted_data</span><span class="p">[</span><span class="s1">&#39;target_5yrs&#39;</span><span class="p">]</span>

<span class="c1"># Define the X (predictor) variables.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">extracted_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;target_5yrs&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Given that the target variable contains both 1 and 0 indicates that it is binary and requires a model suitable for binary classification.</p>
</div>
<p>Now we can divide your data into a training set (75% of data) and test set (25% of data). This is an important step in the process, as it allows you to reserve a part of the data that the model has not observed. This tests how well the model generalizes—or performs—on new data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Perform the split operation on your data.</span>
<span class="c1"># Assign the outputs as follows: X_train, X_test, y_train, y_test.</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Each training DataFrame contains 1,005 rows, while each test DataFrame contains 335 rows. Additionally, there are 10 columns in each X DataFrame, with only one column in each y DataFrame.</p>
<p>Using the assumption that your features are normally distributed and continuous, the <strong>Gaussian Naive Bayes</strong> algorithm is most appropriate for your data. While your data may not perfectly adhere to these assumptions, this model will still yield the most usable and accurate results.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are several implementations of Naive Bayes in scikit-learn, all of which are found in the sklearn.naive_bayes module:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> BernoulliNB:        Used for binary/Boolean features</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> CategoricalNB:       Used for categorical features</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> ComplementNB:        Used for imbalanced datasets, often for text classification tasks</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> GaussianNB:          Used for continuous features, normally distributed features</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> MultinomialNB:       Used for multinomial (discrete) features</p>
</div>
<p><strong>Fit the model</strong></p>
<p>By creating your model, you will be drawing on your feature engineering work by training the classifier on the X_train DataFrame. You will use this to predict target_5yrs from y_train.</p>
<p>Start by defining <strong>nb</strong> to be the relevant algorithm from <strong>sklearn.naive_bayes</strong>. Then fit your model to your training data. Use this fitted model to create predictions for your test data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assign `nb` to be the appropriate implementation of Naive Bayes.</span>
<span class="n">nb</span> <span class="o">=</span> <span class="n">naive_bayes</span><span class="o">.</span><span class="n">GaussianNB</span><span class="p">()</span>

<span class="c1"># Fit the model on your training data.</span>
<span class="n">nb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Apply your model to predict on your test data. Call this &quot;y_pred&quot;.</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">nb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Results and evaluation</strong></p>
<p>To evaluate the data yielded from your model, you can leverage a series of metrics and evaluation techniques from scikit-learn by examining the actual observed values in the test set relative to your model’s prediction. Specifically, print the accuracy score, precision score, recall score, and f1 score associated with your test data and predicted values.</p>
<p><span class="math notranslate nohighlight">\(F_{1}\)</span> Score</p>
<p><span class="math notranslate nohighlight">\(F_{1} Score\)</span> is a measurement that combines both precision and recall into a single expression, giving each equal importance. It is calculated as:</p>
<div class="math notranslate nohighlight" id="eq-215">
<span id="equation-eq-215"></span><span class="eqno">(109)<a class="headerlink" href="#eq-215" title="Permalink to this equation"></a></span>\[ F_{1}= 2. \frac{precision ⋅ recall}{precision + recall}\]</div>
<p>This combination is known as the harmonic mean. F1 score can range [0, 1], with zero being the worst and one being the best. The idea behind this metric is that it penalizes low values of either metric, which prevents one very strong factor—precision or recall—from “carrying” the other, when it is weaker.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The F1 score never exceeds the mean. In fact, it is only equal to the mean in a single case: when precision equals recall.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print your accuracy score.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;accuracy score:&#39;</span><span class="p">),</span> <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Print your precision score.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;precision score:&#39;</span><span class="p">),</span> <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Print your recall score.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;recall score:&#39;</span><span class="p">),</span> <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>

<span class="c1"># Print your f1 score.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;f1 score:&#39;</span><span class="p">),</span> <span class="nb">print</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="n">score</span><span class="p">:</span>
<span class="mf">0.6985074626865672</span>
<span class="n">precision</span> <span class="n">score</span><span class="p">:</span>
<span class="mf">0.8211920529801324</span>
<span class="n">recall</span> <span class="n">score</span><span class="p">:</span>
<span class="mf">0.6262626262626263</span>
<span class="n">f1</span> <span class="n">score</span><span class="p">:</span>
<span class="mf">0.7106017191977076</span>
</pre></div>
</div>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Based on the above results, The accuracy score for this model is 0.713, or 71.3% accurate.In classification problems, accuracy is useful to know but may not be the best metric by which to evaluate this model. While accuracy is often the most intuitive metric, it is a poor evaluation metric in some cases. In particular, if you have imbalanced classes, a model could appear accurate but be poor at balancing false positives and false negatives.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Precision and recall scores are both useful to evaluate the correct predictive capability of a model because they balance the false positives and false negatives inherent in prediction.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The model shows a precision score of 0.845, suggesting the model is quite good at predicting true positives—meaning the player will play longer than five years—while balancing false positives. The recall score of 0.6375 shows worse performance in predicting true negatives—where the player will not play for five years or more—while balancing false negatives.These two metrics combined can give a better assessment of model performance than accuracy does alone.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The F1 score balances the precision and recall performance to give a combined assessment of how well this model delivers predictions. In this case, the F1 score is 0.7268, which suggests reasonable predictive power in this model.</p>
<p><strong>Confusion matrix</strong></p>
<p>a confusion matrix is a graphic that shows your model’s true and false positives and negatives. It helps to create a visual representation of the components feeding into the metrics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Construct and display your confusion matrix.</span>
<span class="c1"># Construct the confusion matrix for your predicted and test values.</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Create the display for your confusion matrix.</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">display_labels</span><span class="o">=</span><span class="n">nb</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

<span class="c1"># Plot the visual in-line.</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/54.png" src="../_images/54.png" />
</figure>
<p>According to the above graph, there are a couple conclusions:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The top left to bottom right diagonal in the confusion matrix represents the correct predictions, and the ratio of these squares showcases the accuracy.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The concentration of true positives stands out relative to false positives. This ratio is why the precision score is so high (0.845).</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> True negatives and false negatives are closer in number, which explains the worse recall score</p>
<p><strong>Some notes</strong></p>
<p><strong>1.</strong> The evaluation of the model is important to inform if the model has delivered accurate predictions.
Splitting the data was important for ensuring that there was new data for the model to test its predictive performance.</p>
<p><strong>2.</strong> Each metric provided an evaluation from a different standpoint, and accuracy alone was not a strong way to evaluate the model.</p>
<p><strong>3.</strong> Effective assessments balance the true/false positives versus true/false negatives through the confusion matrix and F1 score.</p>
<p><strong>Conclusions</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The model created provides some value in predicting an NBA player’s chances of playing for five years or more.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Notably, the model performed better at predicting true positives than it did at predicting true negatives. In other words, it more accurately identified those players who will likely play for more than five years than it did those who likely will not.</p>
</section>
</section>
<section id="k-means-algorithm">
<h2>7.4. K-means algorithm<a class="headerlink" href="#k-means-algorithm" title="Permalink to this heading"></a></h2>
<p>K-means algorithm is an unsupervised learning technique used to cluster unlabeled data.</p>
<p>First we need to define <strong>Centroid</strong> :</p>
<p><strong>Centroid</strong></p>
<p>The center of a cluster is determined by mathematical mean of all the points in that cluster.</p>
<p>There 4 steps in implementation of the K-mean algorithm:</p>
<p><strong>1.</strong> Initiate K centroids by randomly placing the centroids in space</p>
<p><strong>2.</strong> Assign all points to their nearest centroid</p>
<p><strong>3.</strong> Recalculate the centroid of each cluster based on the points assigned to it</p>
<p><strong>4.</strong> Repeat the steps 2 and 3 until convergence</p>
<p>This is important K-means multiple times with different initial positions of the centroids to help avoid using a model that gets stuck in local minima. Fortunately, most machine learning packages have improved implementations of K-means that make it easier for you by removing this requirement.</p>
<p>In scikit-learn, this implementation is called K-means++. K-means++ still randomly initializes centroids in the data, but it does so based on a probability calibration. Basically, it randomly chooses one point within the data to be the first centroid, then it uses other data points as centroids, selecting them pseudo-randomly. The probability that a point will be selected as a centroid increases the farther it is from other centroids. This helps to ensure that centroids aren’t initially placed very close together, which is when convergence in local minima is most likely to occur.</p>
<section id="example-of-k-means-for-color-compression">
<h3>7.4.1 Example of K-means for color compression<a class="headerlink" href="#example-of-k-means-for-color-compression" title="Permalink to this heading"></a></h3>
<p>In this example, we will solve worked example of K-means on non-synthetic data. We will use K-means to cluster the pixels of a photograph of some tulips based on their encoded color values. We will explore how different values of k affect the clustering of the pixels, and thus the appearance of the photograph.</p>
<p>As usual, we need to import the required packages. We will be using numpy and pandas for operations, and Plotly for 3-D visualization. Of particular note is Kmeans, which is scikit-learn’s implementation of the K-means algorithm.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="nn">go</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
</pre></div>
</div>
<p>The “data” in this case is not a pandas dataframe. It’s a photograph, which we’ll convert into a numerical array:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">img</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;using_kmeans_for_color_compression_tulips_photo.jpg&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can see extract the dimensions of the photo and visualize the actual photo as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display the photo and its shape</span>
<span class="nb">print</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>Here is the photo:</p>
<figure class="align-center">
<img alt="../_images/55.png" src="../_images/55.png" />
</figure>
<p>Here we have a photograph of some tulips. The shape of the image is 320 x 240 x 3. This can be interpreted as pixel information. Each dot on your screen is a pixel. This photograph has 320 vertical pixels and 240 horizontal pixels.</p>
<p>But what is the third dimension of “3”? This dimension refers to the values that encode the color of each pixel. Each pixel has 3 parameters: red (R), green (G), and blue (B), also known as its RGB values. For each color—R, G, and B—the encoded value can range from 0-255. This means that there are 256³, or 16,777,216 different combinations of RGB, each resulting in a unique color on your screen.</p>
<p>To prepare this data for modeling, we’ll reshape it into an array, where each row represents a single pixel’s RGB color values:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the image so that each row represents a single pixel</span>
<span class="c1"># defined by three values: R, G, B</span>
<span class="n">img_flat</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>If we want to see information of the color for the first 5 pixels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">img_flat</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:]</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span><span class="mi">211</span><span class="p">,</span> <span class="mi">196</span><span class="p">,</span>  <span class="mi">41</span><span class="p">],</span>
<span class="p">[</span><span class="mi">199</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span>  <span class="mi">24</span><span class="p">],</span>
<span class="p">[</span><span class="mi">179</span><span class="p">,</span> <span class="mi">152</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>
<span class="p">[</span><span class="mi">186</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span>   <span class="mi">0</span><span class="p">],</span>
<span class="p">[</span><span class="mi">187</span><span class="p">,</span> <span class="mi">143</span><span class="p">,</span>   <span class="mi">0</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">uint8</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Plot the data in 3-D space</strong></p>
<p>Now we have an array that is 76,800 x 3. Each row is a single pixel’s color values. Because we have only 3 columns (R-G-B), we can visualize this data in 3-dimensional space. Let’s create a pandas dataframe to help us understand and visualize our data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a pandas df with r, g, and b as columns</span>
<span class="n">img_flat_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">img_flat</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Now we can create the 3d plot:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create 3D plot where each pixel in the `img` is displayed in its actual color</span>
<span class="n">trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">img_flat_df</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
             <span class="n">y</span> <span class="o">=</span> <span class="n">img_flat_df</span><span class="o">.</span><span class="n">g</span><span class="p">,</span>
             <span class="n">z</span> <span class="o">=</span> <span class="n">img_flat_df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
             <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
             <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                         <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rgb(</span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">b</span>
                                <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">img_flat_df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                       <span class="n">img_flat_df</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                       <span class="n">img_flat_df</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">)],</span>
                         <span class="n">opacity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">trace</span><span class="p">]</span>

<span class="n">layout</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Layout</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                       <span class="n">r</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                       <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                       <span class="n">t</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                       <span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;R&#39;</span><span class="p">,</span>
            <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;G&#39;</span><span class="p">,</span>
            <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">),</span>
          <span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/56.png" src="../_images/56.png" />
</figure>
<p>In this graph, each dot represents a color/pixel that is in our original image of tulips. The more intense the color, the more dots are concentrated in that area. The most-represented colors in the graph are the most abundant colors in the photograph: mostly reds, greens, and yellows.</p>
<p>We can train a K-means model on this data. The algorithm will create k clusters by minimizing the squared distances from each point to its nearest centroid. Let’s first do an experiment. What would you expect to happen if we built a K-means model with just a single centroid (k = 1) and replaced each pixel in the photograph with the RGB value of that centroid? What would the photograph look like?</p>
<p><strong>Cluster the data: k = 1</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate the model</span>
<span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">img_flat</span><span class="p">)</span>

<span class="c1"># Copy `img_flat` so we can modify it</span>
<span class="n">img_flat1</span> <span class="o">=</span> <span class="n">img_flat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Replace each row in the original image with its closest cluster center</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">):</span>
    <span class="n">img_flat1</span><span class="p">[</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="c1"># Reshape the data back to (640, 480, 3)</span>
<span class="n">img1</span> <span class="o">=</span> <span class="n">img_flat1</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>Here is the out put:</p>
<figure class="align-center">
<img alt="../_images/57.png" src="../_images/57.png" />
</figure>
<p>According to the steps mentioned in the procedure of K-mean implementation:</p>
<p><strong>1.</strong> We randomly placed our centroid in the colorspace.</p>
<p><strong>2.</strong> We assigned each point to its nearest centroid. Since there was only one centroid, all points were assigned to the same centroid, and thus to the same cluster.</p>
<p><strong>3.</strong> We updated the centroid’s location to the mean location of all of its points. Again, since there is only a single centroid, it updated to the mean location of every point in the image.</p>
<p><strong>4.</strong> Repeat until the model converges. In this case, it only took one iteration for the model to converge.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We then updated each pixel’s RGB values to be the same as the centroid’s. The result is the image of our tulips when every pixel is replaced with the average color. The average color of this photo was brown⁠—all the colors muddled together.</p>
</div>
<p>We can verify this for ourselves by manually calculating the average for each column in the flattened array. This will give us the average R value, G value, and B value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate mean of each column in the flattened array</span>
<span class="n">column_means</span> <span class="o">=</span> <span class="n">img_flat</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;column means: &#39;</span><span class="p">,</span> <span class="n">column_means</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">column</span> <span class="n">means</span><span class="p">:</span>  <span class="p">[</span><span class="mf">125.64397135</span>  <span class="mf">77.93165365</span>  <span class="mf">43.51584635</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, we can compare this to what the K-means model calculated as the final location of its one centroid.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;cluster centers: &#39;</span><span class="p">,</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">)</span>
</pre></div>
</div>
<p>The output of the above line is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">column</span> <span class="n">means</span><span class="p">:</span>  <span class="p">[</span><span class="mf">125.64397135</span>  <span class="mf">77.93165365</span>  <span class="mf">43.51584635</span><span class="p">]</span>
</pre></div>
</div>
<p>Which is the same as the average of the values calculated manually. Now return to the 3-D rendering of our data, only this time we’ll add the centroid:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="c1"># Create 3-D plot where each pixel in the `img` is displayed in its actual color</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">img_flat_df</span><span class="o">.</span><span class="n">r</span><span class="p">,</span>
                 <span class="n">y</span> <span class="o">=</span> <span class="n">img_flat_df</span><span class="o">.</span><span class="n">g</span><span class="p">,</span>
                 <span class="n">z</span> <span class="o">=</span> <span class="n">img_flat_df</span><span class="o">.</span><span class="n">b</span><span class="p">,</span>
                 <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
                 <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rgb(</span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">,</span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span>
                                    <span class="n">r</span><span class="p">,</span><span class="n">g</span><span class="p">,</span><span class="n">b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">img_flat_df</span><span class="o">.</span><span class="n">r</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                                 <span class="n">img_flat_df</span><span class="o">.</span><span class="n">g</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                                 <span class="n">img_flat_df</span><span class="o">.</span><span class="n">b</span><span class="o">.</span><span class="n">values</span><span class="p">)],</span>
                             <span class="n">opacity</span><span class="o">=</span><span class="mf">0.5</span><span class="p">))</span>

    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">trace</span><span class="p">]</span>

    <span class="n">layout</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Layout</span><span class="p">(</span><span class="n">margin</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">l</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">r</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">b</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">t</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="o">.</span><span class="n">Figure</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="n">layout</span><span class="p">)</span>


    <span class="c1"># Add centroid to chart</span>
    <span class="n">centroid</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">add_trace</span><span class="p">(</span>
<span class="n">go</span><span class="o">.</span><span class="n">Scatter3d</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="n">centroid</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
             <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="n">centroid</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
             <span class="n">z</span> <span class="o">=</span> <span class="p">[</span><span class="n">centroid</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span>
             <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;markers&#39;</span><span class="p">,</span>
             <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                         <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;rgb(125.79706706,77.8178776,42.58090169)&#39;</span><span class="p">],</span>
                         <span class="n">opacity</span><span class="o">=</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">scene</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
                <span class="n">xaxis_title</span><span class="o">=</span><span class="s1">&#39;R&#39;</span><span class="p">,</span>
                <span class="n">yaxis_title</span><span class="o">=</span><span class="s1">&#39;G&#39;</span><span class="p">,</span>
                <span class="n">zaxis_title</span><span class="o">=</span><span class="s1">&#39;B&#39;</span><span class="p">),</span>
              <span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/57.png" src="../_images/57.png" />
</figure>
<p>We can see the centroid as a large circle in the middle of the colorspace. (If you can’t, just click on the image and spin/zoom it.) Notice that this is the “center of gravity” of all the points in the graph.</p>
<p>Now let’s try something else. Let’s refit a K-means model to the data, this time using k = 3.</p>
<p><strong>Cluster the data: k = 3</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instantiate k-means model for 3 clusters</span>
<span class="n">kmeans3</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">img_flat</span><span class="p">)</span>
</pre></div>
</div>
<p>The .cluster_centers_ attribute returns an array where each element represents the coordinates of a centroid (i.e., their RGB values). We’ll use these coordinates as we did previously to generate the colors that are represented by our centroids:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">centers</span> <span class="o">=</span> <span class="n">kmeans3</span><span class="o">.</span><span class="n">cluster_centers_</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">centers</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="n">array</span><span class="p">([[</span> <span class="mf">40.67836777</span><span class="p">,</span>  <span class="mf">50.5497578</span> <span class="p">,</span>  <span class="mf">16.2448648</span> <span class="p">],</span>
<span class="p">[</span><span class="mf">202.22832829</span><span class="p">,</span> <span class="mf">173.55005895</span><span class="p">,</span> <span class="mf">109.60875583</span><span class="p">],</span>
<span class="p">[</span><span class="mf">177.41608916</span><span class="p">,</span>  <span class="mf">41.64855117</span><span class="p">,</span>  <span class="mf">27.31525836</span><span class="p">]])</span>
</pre></div>
</div>
<p>Let’s now replace each pixel in the original image with the RGB value of the centroid to which it was assigned:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Helper function to display our photograph when clustered into k clusters</span>
<span class="k">def</span> <span class="nf">cluster_image</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">img</span><span class="o">=</span><span class="n">img</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Fits a K-means model to a photograph.</span>
<span class="sd">    Replaces photo&#39;s pixels with RGB values of model&#39;s centroids.</span>
<span class="sd">    Displays the updated image.</span>

<span class="sd">    Args:</span>
<span class="sd">        k:    (int)          - Your selected K-value</span>
<span class="sd">        img:  (numpy array)  - Your original image converted to a numpy array</span>

<span class="sd">    Returns:</span>
<span class="sd">        The output of plt.imshow(new_img), where new_img is a new numpy array \</span>
<span class="sd">        where each row of the original array has been replaced with the \</span>
<span class="sd">        coordinates of its nearest centroid.</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">img_flat</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">k</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">img_flat</span><span class="p">)</span>
    <span class="n">new_img</span> <span class="o">=</span> <span class="n">img_flat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">):</span>
        <span class="n">new_img</span><span class="p">[</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span> <span class="o">==</span> <span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">new_img</span> <span class="o">=</span> <span class="n">new_img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">new_img</span><span class="p">),</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">);</span>

<span class="c1"># Generate image when k=3</span>
<span class="n">cluster_image</span><span class="p">(</span><span class="mi">3</span><span class="p">);</span>
</pre></div>
</div>
<p>This is the output:</p>
<figure class="align-center">
<img alt="../_images/59.png" src="../_images/59.png" />
</figure>
<p>We now have a photo with just three colors. Each pixel’s RGB values correspond to the values of its nearest centroid.</p>
<p>We can return once more to our 3-D colorspace. This time, we’ll re-color each dot in the colorspace to correspond with the color of its centroid. This will allow us to see how the K-means algorithm clustered our data spatially.</p>
<p>K-means works best when the clusters are more circular, because it tries to minimize distance from point to centroid. It may be worth trying a different clustering algorithm if you want to cluster a long, narrow, continuous band of data.</p>
<p>Nonetheless, K-means successfully compresses the colors of this photograph. This process can be applied for any value of k. Here’s the output of each photo for k = 2–10.</p>
<p><strong>Cluster the data: k = 2-10</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">cluster_image_grid</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">img</span><span class="o">=</span><span class="n">img</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Fits a K-means model to a photograph.</span>
<span class="sd">    Replaces photo&#39;s pixels with RGB values of model&#39;s centroids.</span>
<span class="sd">    Displays the updated image on an axis of a figure.</span>

<span class="sd">    Args:</span>
<span class="sd">        k:    (int)          - Your selected K-value</span>
<span class="sd">        ax:   (int)          - Index of the axis of the figure to plot to</span>
<span class="sd">        img:  (numpy array)  - Your original image converted to a numpy array</span>

<span class="sd">    Returns:</span>
<span class="sd">        A new image where each row of the ori array has been replaced with the \</span>
<span class="sd">        coordinates of its nearest centroid.</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">img_flat</span> <span class="o">=</span> <span class="n">img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">img_flat</span><span class="p">)</span>
    <span class="n">new_img</span> <span class="o">=</span> <span class="n">img_flat</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">):</span>
        <span class="n">new_img</span><span class="p">[</span><span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="o">==</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="n">new_img</span> <span class="o">=</span> <span class="n">new_img</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">img</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">new_img</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">pyplot</span><span class="o">.</span><span class="n">gcf</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">set_size_inches</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">axs</span> <span class="o">=</span> <span class="n">axs</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="n">k_values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">k_values</span><span class="p">):</span>
    <span class="n">cluster_image_grid</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">img</span><span class="o">=</span><span class="n">img</span><span class="p">)</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">title</span><span class="o">.</span><span class="n">set_text</span><span class="p">(</span><span class="s1">&#39;k=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/60.png" src="../_images/60.png" />
</figure>
<p>From the above figure, it could be observed that as we group the data into more and more clusters, additional clusters beyond a certain point contribute less and less to your understanding of your data.</p>
</section>
<section id="evaluation-of-the-k-mean">
<h3>7.4.2 Evaluation of the K-mean<a class="headerlink" href="#evaluation-of-the-k-mean" title="Permalink to this heading"></a></h3>
<p>You know that the evaluation metrics you used for supervised learning models don’t apply to unsupervised learning models. This is because unsupervised learning model results cannot be categorized as “correct” or “incorrect.” While supervised learning models use predictor variables to predict a defined target variable, unsupervised learning methods have metrics that seek an underlying structure within the data.</p>
<p>Clustering models are a type of unsupervised learning that do this by grouping observations together. Data professionals often use <strong>inertia</strong> and <strong>silhouette scores</strong> to evaluate their clustering models and help them determine which groupings make sense. This reading reviews these concepts and examines them in greater detail.</p>
<p><strong>1. Inertia</strong></p>
<p>Inertia is a measurement of intracluster distance. It indicates how compact the clusters are in a model. Specifically, inertia is the sum of the squared distance between each point and the centroid of the cluster that it’s assigned to. It can be represented by this formula, where:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <span class="math notranslate nohighlight">\(n\)</span> = the number of observations in the data,</p>
<p>​
<span class="math notranslate nohighlight">\(\bullet\)</span> <span class="math notranslate nohighlight">\(x_{i}\)</span> = the location of a particular observation,</p>
<p>​
<span class="math notranslate nohighlight">\(\bullet\)</span> <span class="math notranslate nohighlight">\(C_{k}\)</span> = the location of the centroid of cluster k, which is the cluster to which point  is assigned.</p>
<p>The inertia is defined as:</p>
<div class="math notranslate nohighlight" id="eq-117">
<span id="equation-eq-117"></span><span class="eqno">(110)<a class="headerlink" href="#eq-117" title="Permalink to this equation"></a></span>\[ Inertia = \sum_{i=0}^n (x_{i}-C_{k})^2\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The greater the inertia, the greater the distances between points and their centroids, which means the points within each cluster are farther apart from each other. Note, however, that inertia only measures intracluster distance. For the same dataset and the same number of clusters, lower inertia values are typically better than higher values, because low values indicate that points are closer together within their clusters.</p>
</div>
<p><strong>Evaluating inertia</strong></p>
<p>Inertia is a useful metric to determine how well your clustering model identifies meaningful patterns in the data. But it’s generally not very useful by itself. If your model has an inertia of 53.25, is that good? It depends. The measurement becomes meaningful when it’s compared to the inertia values and k values of other models on the same data. As you increase the number of clusters (k), the inertia value will drop, but there comes a point where adding more clusters will have only small changes in inertia. And it’s this transition that we need to detect.</p>
<p><strong>The elbow method</strong></p>
<p>The elbow method is a great way to this point of transition. It’s a way to help decide which clustering gives the most meaningful model of your data. It uses a line plot to visually compare the inertias of different models. With K-means models, this is done as a comparison between different values of k. Here’s an example:</p>
<figure class="align-center">
<img alt="../_images/61.png" src="../_images/61.png" />
</figure>
<p>Based on the above graph, it compares the inertias of nine different K-means models—one for each value of k from two through 10. It’s clear that inertia begins very high when the data is grouped into two clusters. The three-cluster model, however, has much lower inertia, creating a steep negative slope between two and three clusters. After that, the rate of inertial decline slows down dramatically, as indicated by the much flatter line in the plot.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please note that, you want inertia to be low, but if you add more and more clusters with only minimal improvement to inertia, you’re only adding complexity without capturing real structure in the data.</p>
</div>
<p><strong>2. Silhouette analysis</strong></p>
<p>A silhouette analysis is the comparison of different models’ silhouette scores. The silhouette score is the mean silhouette coefficient over all the observations in a model.To calculate a model’s silhouette score, first, a silhouette coefficient is calculated for each instance in the data. An instance’s silhouette coefficient is defined by the following formula, where:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <span class="math notranslate nohighlight">\(a\)</span> = the mean distance between the instance and each other instance in the same cluster</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <span class="math notranslate nohighlight">\(b\)</span> = the mean distance from the instance to each instance in the nearest other cluster (i.e., excluding the cluster</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <span class="math notranslate nohighlight">\(max(a,b)\)</span> = whichever value is greater between <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span></p>
<p>The silhouette score is defined as:</p>
<div class="math notranslate nohighlight" id="eq-118">
<span id="equation-eq-118"></span><span class="eqno">(111)<a class="headerlink" href="#eq-118" title="Permalink to this equation"></a></span>\[ Silhouette\: Scores = \frac{(b-a)}{max(a,b)}\]</div>
<p>A silhouette coefficient can range between -1 and +1.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> A value closer to +1 means that a point is close to other points in its own cluster and well separated from points in other clusters.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> A value closer to zero means that a point is between clusters.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> A value closer to -1 means that a point is probably assigned to the wrong cluster,</p>
<p>The greater the silhouette score, the better defined the model clusters, because the points in a given cluster are closer to each other, and the clusters themselves are more separated from each other.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that, unlike inertia, silhouette coefficients contain information about both intracluster distance (captured by the variable a) and intercluster distance (captured by the variable b).</p>
</div>
<p>As with inertia values, you can plot silhouette scores for different models to compare them against each other:</p>
<figure class="align-center">
<img alt="../_images/62.png" src="../_images/62.png" />
</figure>
<p>In this example, it’s evident that a three-cluster model has a higher silhouette score than any other model. Based on this diagram, the data is probably best grouped into three clusters.</p>
<p>We can summarize the key points regarding the inertia and Silhouette score:</p>
<p><strong>Inertia:</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Measures intracluster distance</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Equal to the sum of the squared distance between each point and the centroid of the cluster that it’s assigned to</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Used in elbow plots</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> All else equal, lower values are generally better</p>
<p><strong>Silhouette score:</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Measures both intercluster distance and intracluster distance</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Equal to the average of all points’ silhouette coefficients</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Can be between -1 and +1 (greater values are better)</p>
</section>
<section id="build-a-k-means-model">
<h3>7.4.3 Build a K-means model<a class="headerlink" href="#build-a-k-means-model" title="Permalink to this heading"></a></h3>
<p>In this example, the dataset is a spreadsheet that includes datapoints across a sample size of 345 penguins, such as species, island, and sex. We will use a K-means clustering model to group this data and identify patterns that provide important insights about penguins</p>
<p>We should import the required packages at the first step:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import standard operational packages.</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Important tools for modeling and evaluation.</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">silhouette_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Import visualization packages.</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
<p>Then we should load the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Save the `pandas` DataFrame in variable `penguins`.</span>
<span class="n">penguins</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;penguins.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>After loading the dataset, the next step is to prepare the data to be suitable for clustering. This includes:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Exploring data</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Checking for missing values</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Encoding data</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Dropping a column</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Scaling the features using StandardScaler</p>
<p>To cluster penguins of multiple different species, determine how many different types of penguin species are in the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Find out how many penguin types there are.</span>
<span class="n">penguins</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>


<span class="c1"># Find the count of each species type.</span>
<span class="n">penguins</span><span class="p">[</span><span class="s1">&#39;species&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">dropna</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>There are three types of species. Note the Chinstrap species is less common than the other species. This has a chance to affect K-means clustering as K-means performs best with similar sized groupings.</p>
<p>For purposes of clustering, pretend you don’t know that there are three different types of species. Then, you can explore whether the algorithm can discover the different species. You might even find other relationships in the data.</p>
<p><strong>Check for missing values</strong></p>
<p>An assumption of K-means is that there are no missing values. Check for missing values in the rows of the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for missing values.</span>
<span class="n">penguins</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>Now, drop the rows with missing values and save the resulting pandas DataFrame in a variable named penguins_subset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop rows with missing values.</span>
<span class="c1"># Save DataFrame in variable `penguins_subset`.</span>
<span class="n">penguins_subset</span> <span class="o">=</span> <span class="n">penguins</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, check to make sure that <cite>penguins_subset</cite> does not contain any missing values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check for missing values.</span>
<span class="n">penguins_subset</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>Now we can see that there is no row remained with missing values in the data.</p>
<p>Some versions of the penguins dataset have values encoded in the sex column as ‘Male’ and ‘Female’ instead of ‘MALE’ and ‘FEMALE’. The code below will make sure all values are ALL CAPS:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_subset</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">penguins_subset</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
</pre></div>
</div>
<p>K-means needs numeric columns for clustering. Convert the categorical column ‘sex’ into numeric. There is no need to convert the ‘species’ column because it isn’t being used as a feature in the clustering algorithm.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert `sex` column from categorical to numeric.</span>
<span class="n">penguins_subset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">penguins_subset</span><span class="p">,</span> <span class="n">drop_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;sex&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Drop the categorical column island from the dataset. While it has value, we assume if penguins of the same species exhibit different physical characteristics based on sex. This doesn’t include location.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop the island column.</span>
<span class="n">penguins_subset</span> <span class="o">=</span> <span class="n">penguins_subset</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;island&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Scale the features</strong></p>
<p>Because K-means uses distance between observations as its measure of similarity, it’s important to scale the data before modeling. Use a third-party tool, such as scikit-learn’s <strong>StandardScaler</strong> function. StandardScaler scales each point xᵢ by subtracting the mean observed value for that feature and dividing by the standard deviation: x-scaled = (xᵢ – mean(X)) / σ</p>
<p>This ensures that all variables have a mean of 0 and variance/standard deviation of 1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because the species column isn’t a feature, it doesn’t need to be scaled.</p>
</div>
<p>First, copy all the features except the ‘species’ column to a DataFrame X:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Exclude `species` variable from X</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">penguins_subset</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;species&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Scale the features in X using StandardScaler, and assign the scaled data to a new variable X_scaled.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Scale the features.</span>
<span class="c1">#Assign the scaled data to variable `X_scaled`.</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, fit K-means and evaluate inertia for different values of k. Because you may not know how many clusters exist in the data, start by fitting K-means and examining the inertia values for different values of k. To do this, write a function called kmeans_inertia that takes in num_clusters and x_vals (X_scaled) and returns a list of each k-value’s inertia.</p>
<p>When using K-means inside the function, set the random_state to 42. This way, others can reproduce your results.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit K-means and evaluate inertia for different values of k.</span>
<span class="n">num_clusters</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">11</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">kmeans_inertia</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Accepts as arguments list of ints and data array.</span>
<span class="sd">    Fits a KMeans model where k = each value in the list of ints.</span>
<span class="sd">    Returns each k-value&#39;s inertia appended to a list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
<span class="n">inertia</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">num_clusters</span><span class="p">:</span>
    <span class="n">kms</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">kms</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span>
    <span class="n">inertia</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">kms</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="k">return</span> <span class="n">inertia</span>
</pre></div>
</div>
<p>Use the kmeans_inertia function to return a list of inertia for k=2 to 10:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Return a list of inertia for k=2 to 10.</span>
<span class="n">inertia</span> <span class="o">=</span> <span class="n">kmeans_inertia</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">,</span> <span class="n">X_scaled</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">inertia</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output of the above code which is the 10 inertia corresponding to the k=2 to k=10:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">885.6224143652249</span><span class="p">,</span>
<span class="mf">577.8284278107235</span><span class="p">,</span>
<span class="mf">386.1453442477329</span><span class="p">,</span>
<span class="mf">284.5464837898288</span><span class="p">,</span>
<span class="mf">217.92858573807678</span><span class="p">,</span>
<span class="mf">201.39287843423264</span><span class="p">,</span>
<span class="mf">185.461310432323</span><span class="p">,</span>
<span class="mf">173.4545211497985</span><span class="p">,</span>
<span class="mf">164.12001520260708</span><span class="p">]</span>
</pre></div>
</div>
<p>Next, create a line plot that shows the relationship between num_clusters and inertia. Use either seaborn or matplotlib to visualize this relationship:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a line plot.</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">num_clusters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">inertia</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Number of clusters&quot;</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Inertia&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/63.png" src="../_images/63.png" />
</figure>
<p>The plot seems to depict an elbow at six clusters, but there isn’t a clear method for confirming that a six-cluster model is optimal. Therefore, the silhouette scores should be checked.</p>
<p>Now, evaluate the silhouette score using the <strong>silhouette_score()</strong> function. Silhouette scores are used to study the distance between clusters. Then, compare the silhouette score of each value of k, from 2 through 10. To do this, write a function called kmeans_sil that takes in num_clusters and x_vals (X_scaled) and returns a list of each k-value’s silhouette score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Evaluate silhouette score.</span>
<span class="c1"># Write a function to return a list of each k-value&#39;s score.</span>

<span class="k">def</span> <span class="nf">kmeans_sil</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">,</span> <span class="n">x_vals</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Accepts as arguments list of ints and data array.</span>
<span class="sd">    Fits a KMeans model where k = each value in the list of ints.</span>
<span class="sd">    Calculates a silhouette score for each k value.</span>
<span class="sd">    Returns each k-value&#39;s silhouette score appended to a list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sil_score</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">num_clusters</span><span class="p">:</span>
        <span class="n">kms</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">num</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
        <span class="n">kms</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_vals</span><span class="p">)</span>
        <span class="n">sil_score</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">silhouette_score</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">kms</span><span class="o">.</span><span class="n">labels_</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">sil_score</span>


<span class="n">sil_score</span> <span class="o">=</span> <span class="n">kmeans_sil</span><span class="p">(</span><span class="n">num_clusters</span><span class="p">,</span> <span class="n">X_scaled</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">sil_score</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output of the above code which is the 10 silhouette score corresponding to the k=2 to k=10:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mf">0.44398088353055243</span><span class="p">,</span>
<span class="mf">0.45101024097188375</span><span class="p">,</span>
<span class="mf">0.5080140996630784</span><span class="p">,</span>
<span class="mf">0.519998574860868</span><span class="p">,</span>
<span class="mf">0.5263224884981607</span><span class="p">,</span>
<span class="mf">0.47774022332151733</span><span class="p">,</span>
<span class="mf">0.42219207326432245</span><span class="p">,</span>
<span class="mf">0.36062890821417276</span><span class="p">,</span>
<span class="mf">0.36172505634200175</span><span class="p">]</span>
</pre></div>
</div>
<p>Next, create a line plot that shows the relationship between num_clusters and sil_score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a line plot.</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">num_clusters</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">sil_score</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;# of clusters&quot;</span><span class="p">);</span>
<span class="n">plot</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Silhouette Score&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/64.png" src="../_images/64.png" />
</figure>
<p>Silhouette scores near 1 indicate that samples are far away from neighboring clusters. Scores close to 0 indicate that samples are on or very close to the decision boundary between two neighboring clusters.</p>
<p>The plot indicates that the silhouette score is closest to 1 when the data is partitioned into six clusters, although five clusters also yield a relatively good silhouette score.</p>
<p>To decide on an optimal k-value, fit a six-cluster model to the dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit a 6-cluster model.</span>
<span class="n">kmeans6</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">kmeans6</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>
</pre></div>
</div>
<p>We can print the unique labels (clusters) of the fit model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print unique labels.</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Unique labels:&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">kmeans6</span><span class="o">.</span><span class="n">labels_</span><span class="p">))</span>
</pre></div>
</div>
<p>The output is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Unique</span> <span class="n">labels</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span> <span class="mi">1</span> <span class="mi">2</span> <span class="mi">3</span> <span class="mi">4</span> <span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
<p>Now, create a new column cluster that indicates cluster assignment in the DataFrame penguins_subset. It’s important to understand the meaning of each cluster’s labels, then decide whether the clustering makes sense.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Create a new column `cluster`.</span>
<span class="n">penguins_subset</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">kmeans6</span><span class="o">.</span><span class="n">labels_</span>
<span class="n">penguins_subset</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<p>Use groupby to verify if any ‘cluster’ can be differentiated by ‘species’:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify if any `cluster` can be differentiated by `species`.</span>
<span class="n">penguins_subset</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<p>This is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cluster</span>  <span class="n">species</span>
<span class="mi">0</span>        <span class="n">Gentoo</span>       <span class="mi">58</span>
<span class="mi">1</span>        <span class="n">Adelie</span>       <span class="mi">73</span>
 <span class="n">Chinstrap</span>     <span class="mi">5</span>
<span class="mi">2</span>        <span class="n">Adelie</span>       <span class="mi">71</span>
<span class="mi">3</span>        <span class="n">Adelie</span>        <span class="mi">2</span>
 <span class="n">Chinstrap</span>    <span class="mi">34</span>
<span class="mi">4</span>        <span class="n">Gentoo</span>       <span class="mi">61</span>
<span class="mi">5</span>        <span class="n">Chinstrap</span>    <span class="mi">29</span>
</pre></div>
</div>
<p>Next, interpret the groupby outputs. Although the results of the groupby show that each ‘cluster’ can be differentiated by ‘species’, it is useful to visualize these results. The graph shows that each ‘cluster’ can be differentiated by ‘species’:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_subset</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span> <span class="s1">&#39;species&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Clusters differentiated by species&#39;</span><span class="p">,</span>
                                                           <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                                                           <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Size&#39;</span><span class="p">,</span>
                                                           <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;(Cluster, Species)&#39;</span><span class="p">,</span><span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;turquoise&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p>This is the graph:</p>
<figure class="align-center">
<img alt="../_images/65.png" src="../_images/65.png" />
</figure>
<p>Use groupby to verify if each ‘cluster’ can be differentiated by ‘species’ AND ‘sex_MALE’:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Verify if each `cluster` can be differentiated by `species` AND `sex_MALE`.</span>
<span class="n">penguins_subset</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="s1">&#39;species&#39;</span><span class="p">,</span> <span class="s1">&#39;sex_MALE&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>This is what we get as the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cluster</span>  <span class="n">species</span>    <span class="n">sex_MALE</span>
<span class="mi">1</span>        <span class="n">Adelie</span>     <span class="mi">0</span>           <span class="mi">73</span>
<span class="mi">2</span>        <span class="n">Adelie</span>     <span class="mi">1</span>           <span class="mi">71</span>
<span class="mi">4</span>        <span class="n">Gentoo</span>     <span class="mi">1</span>           <span class="mi">61</span>
<span class="mi">0</span>        <span class="n">Gentoo</span>     <span class="mi">0</span>           <span class="mi">58</span>
<span class="mi">3</span>        <span class="n">Chinstrap</span>  <span class="mi">1</span>           <span class="mi">34</span>
<span class="mi">5</span>        <span class="n">Chinstrap</span>  <span class="mi">0</span>           <span class="mi">29</span>
<span class="mi">1</span>        <span class="n">Chinstrap</span>  <span class="mi">0</span>            <span class="mi">5</span>
<span class="mi">3</span>        <span class="n">Adelie</span>     <span class="mi">1</span>            <span class="mi">2</span>
</pre></div>
</div>
<p>Even though clusters 1 and 3 weren’t all one species or sex, the groupby indicates that the algorithm produced clusters mostly differentiated by species and sex.</p>
<p>Finally, interpret the groupby outputs and visualize these results. The graph shows that each ‘cluster’ can be differentiated by ‘species’ and ‘sex_MALE’. Furthermore, each cluster is mostly comprised of one sex and one species:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">penguins_subset</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;cluster&#39;</span><span class="p">,</span><span class="s1">&#39;species&#39;</span><span class="p">,</span><span class="s1">&#39;sex_MALE&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">level</span> <span class="o">=</span> <span class="s1">&#39;species&#39;</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Clusters differentiated by species and sex&#39;</span><span class="p">,</span>
                                                                                                                <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
                                                                                                                <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Size&#39;</span><span class="p">,</span>
                                                                                                                <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;(Cluster, Sex)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">bbox_to_anchor</span><span class="o">=</span><span class="p">(</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/66.png" src="../_images/66.png" />
</figure>
<p>In summary we can state that:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Inertia and silhouette score can be used to find the optimal value of clusters.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Clusters can find natural groupings in data.</p>
</section>
</section>
<section id="tree-based-learning">
<h2>7.5. Tree-based learning<a class="headerlink" href="#tree-based-learning" title="Permalink to this heading"></a></h2>
<p>This is a type a supervised ML performing classification and regression tasks. Decision trees are a flowchart-like structure that uses branching paths to predict the outcomes of events, the probability of certain outcomes, or to reach a decision.They can be used for classification problems, where a specific class or outcome is predicted—like whether or not a sports team will win a game. They can also be used for regression problems, where a continuous variable is predicted—like the price of a car.</p>
<section id="structure-of-a-classification-tree">
<h3>7.5.1 Structure of a classification tree<a class="headerlink" href="#structure-of-a-classification-tree" title="Permalink to this heading"></a></h3>
<p>Decision trees only resemble actual trees if you flip them upside down, because they start with the root at the top and grow downward so the “leaves” are at the bottom. Decision trees are made of nodes. Nodes are groups of samples. There are different types of nodes, depending on how they function in the tree. The first node in a decision tree is called the <strong>root node</strong>. The first split always comes off of the root node, which divides the samples into two new nodes based on the values they contain for a particular feature.</p>
<p>These two new nodes are referred to as <strong>child nodes</strong> of the root. A child node is any node that results from a split. The node that the child splits from is known as the <strong>parent node</strong>. Each of these two new child nodes in turn splits the data again, based on a new criterion. This process continues until the nodes stop splitting. The bottom-level nodes that do not split are called <strong>leaf nodes</strong>. All the nodes above the leaf nodes are called <strong>decision nodes</strong>, because they all make a decision that sorts the data either to the left or to the right:</p>
<figure class="align-center">
<img alt="../_images/67.png" src="../_images/67.png" />
</figure>
<p>In a decision tree, the data is split and passed down through decision nodes until reaching a leaf node. A decision node is split on the criterion that minimizes the <strong>impurity</strong> of the classes in their resulting children. Impurity refers to the degree of mixture with respect to class.
Nodes with low impurity have many more of one class than any other. A perfect split would have no impurity in the resulting child nodes; it would partition the data with each child containing only a single class. The worst possible split would have high impurity in the resulting child nodes; both of the child nodes would have equal numbers of each class.</p>
<p>When building a tree and growing a new node, a set of potential split points is generated for every predictor variable in the dataset.The feature and split point that generate the purest child nodes are selected to partition the data.To determine the set of potential split points that will be considered for a variable, the algorithm first identifies what type of variable it is—such as categorical or continuous—and the range of values that exist for that variable.</p>
<p><strong>Categorical variables</strong></p>
<p>If the predictor variable is categorical, the decision tree algorithm will consider splitting based on category.</p>
<p><strong>Continuous variables</strong></p>
<p>If the predictor variable is continuous, splits can be made anywhere along the range of numbers that exist in the data. Often the potential split points are determined by sorting the values for the feature and taking the mean of each consecutive pair of values.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There can be any number of split points, and fewer split points can be considered to save computational resources and time.</p>
</div>
<p><strong>Choosing splits: Gini impurity</strong></p>
<p>Generally, splits are better when each resulting child node contains many more samples of one class than any other because this means the split is effectively separating the classes—the primary job of the decision tree!
In such cases, the child nodes are said to have low impurity. The decision tree algorithm determines the split that will result in the lowest impurity among the child nodes by performing a calculation.</p>
<div class="math notranslate nohighlight" id="eq-200">
<span id="equation-eq-200"></span><span class="eqno">(112)<a class="headerlink" href="#eq-200" title="Permalink to this equation"></a></span>\[ Gini\: impurity = \sum_{i=0}^n P_{i}^2\]</div>
<p>Where <span class="math notranslate nohighlight">\(i\)</span> refers to the classes and <span class="math notranslate nohighlight">\(P_{i}\)</span> is the probability of samples belonging to class i in a given node.</p>
<p>In an example where we want to split a node including fruits based on the color:</p>
<figure class="align-center">
<img alt="../_images/68.png" src="../_images/68.png" />
</figure>
<p>We can calculate the <strong>Gini impurity</strong> of each child node:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> For the “red=yes” child node: Gini impurity = <span class="math notranslate nohighlight">\(1-(\frac{1}{3})^2-(\frac{2}{3})^2=0.445\)</span></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> For the “red=no” child node: Gini impurity = <span class="math notranslate nohighlight">\(1-(\frac{3}{4})^2-(\frac{1}{4})^2=0.375\)</span></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Splitting would continue until all the leaves are pure or some imposed condition stops the splitting.</p>
</div>
</section>
<section id="hyperparameter-tuning">
<h3>7.5.2 Hyperparameter tuning<a class="headerlink" href="#hyperparameter-tuning" title="Permalink to this heading"></a></h3>
<p>Depending on the characteristics of both the data and the algorithm used to model it, a model might overfit or underfit the data. Remember, the aim of a predictive model is to identify underlying, intrinsic patterns and characteristics in data that are representative of all such distributions, and use these characteristics to make predictions on new data.</p>
<p><strong>Overfitting:</strong></p>
<p>Overfitting is when the model learns the training data so closely that it captures more than the intrinsic patterns of all such data distributions. This results in a model that scores very well on the training data but considerably worse on unseen data because it cannot generalize well.</p>
<p><strong>Underfitting:</strong></p>
<p>Underfitting is when the model does not learn the patterns and characteristics of the training data well, and consequently fails to make accurate predictions on new data. It’s typically easier to identify underfitting, because the model performs poorly on both training and test data. The best models neither underfit nor overfit the data. They identify intrinsic patterns within it, but do not capture randomness or noise.</p>
<p><strong>Hyperparameter tuning</strong></p>
<p>One way of helping to achieve this balance is through the use of hyperparameters. Hyperparameters are aspects of a model that you set before the model is trained, andthat affect how the model fits the data. They are not derived from the data itself. Hyperparameter tuning is the process of adjusting the hyperparameters to build a model that best fits the data.</p>
<p>There are many different hyperparameters available to control how a decision tree grows. Each hyperparameter affects something very specific related to the growth conditions.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> One might affect what causes a node to split</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> One might another might limit how deep the tree is allowed to grow,</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> One might change the way node purity is calculated.</p>
<p><strong>max_depth</strong></p>
<p>max_depth defines how deep the tree is allowed to grow. The depth of the tree is the distance, measured in number of levels, from the root node to the furthest leaf node.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>An unrestricted decision tree will continue splitting until every leaf node contains only a single class. It’s possible for a tree to grow so deep that leaves contain just a single sample. However, this overfits the model to the training data, and the performance on the testing data would probably be much worse. A tree that is not allowed to grow deeply enough will have high bias and fail to make accurate predictions. The best decision tree models are neither too shallow nor too deep, but just right.</p>
</div>
<p><strong>min_samples_split</strong></p>
<p>min_samples_split is the minimum number of samples that a node must have for it to split into more nodes.The greater the value you use for min_samples_split, the sooner the tree will stop growing. The minimum possible value is two, because two is the smallest number that can be divided into two separate child nodes.</p>
<p><strong>min_samples_leaf</strong></p>
<p>min_samples_leaf is similar to min_samples_split, but with an important difference. Instead of defining how many samples the parent node must have before splitting, min_samples_leaf defines the minimum number of samples that must be in each child node after the parent splits.</p>
<p><strong>Grid search</strong></p>
<p>A grid search is a technique that will train a model for every combination of preset ranges of hyperparameter values. The aim is to find the combination of values that results in a model that both fits the training data well and generalizes well enough to predict accurately on unseen data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>With more hyperparameters and a more expansive array of values to search over, grid searches can quickly become computationally expensive. One helpful search strategy is to try a wider array of values for each hyperparameter</p>
</div>
</section>
<section id="model-validation">
<h3>7.5.3 Model validation<a class="headerlink" href="#model-validation" title="Permalink to this heading"></a></h3>
<p>Fitting a model to training data and evaluating it on test data might be an adequate way of evaluating how well a single model generalizes to new data, but it’s not a recommended way to compare multiple models to determine which one is best. That’s because, by selecting the model that performs best on the test data, you never get a truly objective measure of future performance. The measure would be optimistic.Usually the test data is used to select a final model. However, there are better, more rigorous ways of evaluating models and selecting the best!</p>
<p>One such way is through a process called validation. Model validation is the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to better understand its strengths and limitations.</p>
<p><strong>Validation sets</strong></p>
<p>The simplest way to maintain the objectivity of the test data is to create another partition in the data—a validation set—and save the test data for after you select the final model. The validation set is then used, instead of the test set, to compare different models.</p>
<p>Here is one common way of splitting data, but note that these proportions are not required. You can split to whichever ratios make the most sense for your use case.</p>
<figure class="align-center">
<img alt="../_images/69.png" src="../_images/69.png" />
</figure>
<p>This method—using a separate validation set to compare models—is most commonly used when you have a very large dataset. The reason for this is that the more data you use for validation, the less you have for training and testing. However, if you don’t have enough validation data, then your models’ scores cannot be expected to give a reliable measure that you can use to select a model, because there’s a greater chance that the distributions in the validation data are not representative of those in the entire dataset.</p>
<p><strong>Cross validation</strong></p>
<p>There is another approach to model validation that avoids having to split the data into three partitions (train / validate / test) in advance. Cross-validation makes more efficient use of the training data by splitting the training data into k number of “folds” (partitions), training a model on (k – 1) folds, and using the fold that was held out to get a validation score. The training process occurs k times, each time using a different fold as the validation set. At the end, the final validation score is the average of all k scores. This process is also commonly referred to as <strong>k-fold cross validation</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The cross-validation process maximizes the usefulness of your data with the goal of getting a more accurate measure of model performance. The more folds you use, the more thorough the validation. However, adding folds increases the time needed to train, and may not be useful beyond a certain point.</p>
</div>
<p><strong>How to select the best model</strong></p>
<p>Once you’ve trained and validated your candidate models, it’s time to select a champion. Of course, your models’ validation scores factor heavily into this decision, but score is seldom the only criterion.Often you’ll need to consider other factors too.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It’s not uncommon for a model with a slightly lower validation score to be selected over the highest-scoring model due to it being simpler, less computationally expensive, or more stable.</p>
</div>
<p>Once you have selected a champion model, it’s time to evaluate it using the test data. The test data is used only for this final model. Your model’s score on this data is how you can expect the model to perform on completely new data.</p>
<p><strong>Conclusion</strong></p>
<p>A rigorous approach to model development might use both cross-validation and validation. The cross-validation can be used to tune hyperparameters, while the separate validation set lets you compare the scores of different algorithms (e.g., logistic regression vs. Naive Bayes vs. decision tree) to select a champion model. Finally, the test set gives you a benchmark score for performance on new data.</p>
</section>
<section id="build-a-decision-tree-in-python">
<h3>7.5.4 Build a decision tree in Python<a class="headerlink" href="#build-a-decision-tree-in-python" title="Permalink to this heading"></a></h3>
<p>We want to build a decision tree model that makes predictions for a target based on multiple features. We have data from an airline which is interested in predicting whether a future customer would be satisfied with their services given customer feedback given previous customer feedback about their flight experience. In addition, we are interested in knowing which features are most important to customer satisfaction.</p>
<p>The data for this activity includes survey responses from 129,880 customers. It includes data points such as class, flight distance, and in-flight entertainment, among others.</p>
<p>Because this activity uses a dataset from the industry, you will need to conduct basic EDA, data cleaning, and other manipulations to prepare the data for modeling.</p>
<p>As usual, we need to import the required packages:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standard operational package imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="c1"># Important imports for modeling and evaluation</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">plot_tree</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>

<span class="c1"># Visualization package imports</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
<p>Then we need to load the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># RUN THIS CELL TO IMPORT YOUR DATA.</span>
<span class="n">df_original</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;Invistico_Airline.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>After loading the dataset, prepare the data to be suitable for decision tree classifiers. This includes:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Exploring the data</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Checking for missing values</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Encoding the data</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Renaming a column</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Creating the training and testing data</p>
<p>We can see the types of the classes listed under <strong>Class</strong> column:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_original</span><span class="p">[</span><span class="s2">&quot;Class&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</pre></div>
</div>
<p>In order to predict customer satisfaction, verify if the dataset is imbalanced. To do this, check the counts of each of the predicted labels:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_original</span><span class="p">[</span><span class="s1">&#39;satisfaction&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">dropna</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">satisfied</span>       <span class="mi">71087</span>
<span class="n">dissatisfied</span>    <span class="mi">58793</span>
</pre></div>
</div>
<p>There are 71087 satisfied customers and 58793 dissatisfied customers.</p>
<p>The sklearn decision tree implementation does not support missing values. Check for missing values in the rows of the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_original</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>This is important to check because if there are only a small number of missing values in the dataset, they can more safely be removed.</p>
<p>Now, we should drop the rows with missing values and save the resulting pandas DataFrame in a variable named df_subset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_subset</span> <span class="o">=</span> <span class="n">df_original</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Four columns (satisfaction, Customer Type, Type of Travel, Class) are the pandas dtype object. Decision trees need numeric columns. Start by converting the ordinal Class column into numeric:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_subset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_subset</span><span class="p">[</span><span class="s1">&#39;Class&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s2">&quot;Business&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">&quot;Eco Plus&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;Eco&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
</pre></div>
</div>
<p>To represent the data in the <strong>target variable</strong> numerically, assign “satisfied” to the label 1 and “unsatisfied” to the label 0 in the satisfaction column:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_subset</span><span class="p">[</span><span class="s1">&#39;satisfaction&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_subset</span><span class="p">[</span><span class="s1">&#39;satisfaction&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s2">&quot;satisfied&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;dissatisfied&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
</pre></div>
</div>
<p>There are other columns in the dataset that are still categorical. Be sure to convert categorical columns in the dataset into numeric:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">df_subset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df_subset</span><span class="p">,</span> <span class="n">drop_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Create the training and testing data</strong></p>
<p>Put 75% of the data into a training set and the remaining 25% into a testing set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">df_subset</span><span class="p">[</span><span class="s2">&quot;satisfaction&quot;</span><span class="p">]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_subset</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;satisfaction&quot;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Fit a decision tree classifier model to the data</strong></p>
<p>Make a decision tree instance called decision_tree and pass in 0 to the random_state parameter. This is only so that if other data professionals run this code, they get the same results. Fit the model on the training set, use the predict() function on the testing set, and assign those predictions to the variable dt_pred:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">decision_tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">decision_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">dt_pred</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Decision trees require no assumptions regarding the distribution of underlying data and don’t require scaling of features.</p>
</div>
<p><strong>Results and evaluation</strong></p>
<p>Print out the decision tree model’s accuracy, precision, recall, and F1 score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decision Tree&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy:&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Precision:&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recall:&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;F1 Score:&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_pred</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Decision</span> <span class="n">Tree</span>
<span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.935438</span>
<span class="n">Precision</span><span class="p">:</span> <span class="mf">0.942859</span>
<span class="n">Recall</span><span class="p">:</span> <span class="mf">0.939030</span>
<span class="n">F1</span> <span class="n">Score</span><span class="p">:</span> <span class="mf">0.940940</span>
</pre></div>
</div>
<p>Decision trees can be particularly susceptible to overfitting. Combining hyperparameter tuning and grid search can help ensure this doesn’t happen. For instance, setting an appropriate value for max depth could potentially help reduce a decision tree’s overfitting problem by limiting how deep a tree can grow.</p>
<p><strong>Confusion matrix</strong></p>
<p>We like to know the types of errors made by an algorithm. To obtain this information, produce a confusion matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">cm</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">dt_pred</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">cm</span><span class="p">,</span><span class="n">display_labels</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/70.png" src="../_images/70.png" />
</figure>
<p>Based on the above confusion matrix, there are a high proportion of true positives and true negatives (where the matrix accurately predicted that the customer would be satisfied or dissatisfied, respectively).</p>
<p>The matrix also had a relatively low number of false positives and false negatives (where the matrix inaccurately predicted that the customer would be satisfied or dissatisfied, respectively.)</p>
<p><strong>Plot the decision tree</strong></p>
<p>Now we can examine the decision tree. Use plot_tree function to produce a visual representation of the tree to pinpoint where the splits in the data are occurring:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">decision_tree</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span><span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">);</span>
</pre></div>
</div>
<p>Here is the output:</p>
<figure class="align-center">
<img alt="../_images/71.png" src="../_images/71.png" />
</figure>
<p><strong>Build a feature importance graph</strong></p>
<p>We can uncover which features might be most important to your decision tree model by building a feature importance graph:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="o">.</span><span class="n">feature_importances_</span>

<span class="n">forest_importances</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">importances</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">forest_importances</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span><span class="n">color</span><span class="o">=</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the graph:</p>
<figure class="align-center">
<img alt="../_images/72.png" src="../_images/72.png" />
</figure>
<p>The feature importance graph seems to confirm that ‘Inflight entertainment’, ‘Seat comfort’, and ‘Ease of Online booking’ are the most important features for this model.</p>
<p><strong>Hyperparameter tuning</strong></p>
<p>We will find the best values for the hyperparameters max_depth and min_samples_leaf using grid search and cross validation. Below are some values for the hyperparameters max_depth and min_samples_leaf:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tree_para</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">14</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">18</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">50</span><span class="p">],</span>
     <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">]}</span>

<span class="n">scoring</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">,</span> <span class="s1">&#39;f1&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p><strong>Check combinations of values</strong></p>
<p>Check every combination of values to examine which pair has the best evaluation metrics.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Make a decision tree instance called tuned_decision_tree with random_state=0</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> make a GridSearchCV instance called clf</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> make sure to refit the estimator using “f1”, and fit the model on the training set</p>
<p>The above 3 steps could be coded as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tuned_decision_tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">tuned_decision_tree</span><span class="p">,</span>
                   <span class="n">tree_para</span><span class="p">,</span>
                   <span class="n">scoring</span> <span class="o">=</span> <span class="n">scoring</span><span class="p">,</span>
                   <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                   <span class="n">refit</span><span class="o">=</span><span class="s2">&quot;f1&quot;</span><span class="p">)</span>

<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can use the best estimator tool to help uncover the best pair combination:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span><span class="o">.</span><span class="n">best_estimator_</span>
</pre></div>
</div>
<p>This is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>After running the DecisionTreeClassifier, the maximum depth is 18 and the minimum number of samples is two, meaning this is the best combination of values.</p>
<p><strong>Determine the “best” decision tree scores</strong></p>
<p>We can print out the decision tree model’s accuracy, precision, recall, and F1 score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Model&#39;</span><span class="p">,</span> <span class="s1">&#39;F1&#39;</span><span class="p">,</span> <span class="s1">&#39;Recall&#39;</span><span class="p">,</span> <span class="s1">&#39;Precision&#39;</span><span class="p">,</span> <span class="s1">&#39;Accuracy&#39;</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">make_results</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_object</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Accepts as arguments a model name (your choice - string) and</span>
<span class="sd">a fit GridSearchCV model object.</span>

<span class="sd">Returns a pandas df with the F1, recall, precision, and accuracy scores</span>
<span class="sd">for the model with the best mean F1 score across all validation folds.</span>
<span class="sd">&quot;&quot;&quot;</span>

    <span class="c1"># Get all the results from the CV and put them in a df.</span>
    <span class="n">cv_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">model_object</span><span class="o">.</span><span class="n">cv_results_</span><span class="p">)</span>

    <span class="c1"># Isolate the row of the df with the max(mean f1 score).</span>
    <span class="n">best_estimator_results</span> <span class="o">=</span> <span class="n">cv_results</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;mean_test_f1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">idxmax</span><span class="p">(),</span> <span class="p">:]</span>

    <span class="c1"># Extract accuracy, precision, recall, and f1 score from that row.</span>
    <span class="n">f1</span> <span class="o">=</span> <span class="n">best_estimator_results</span><span class="o">.</span><span class="n">mean_test_f1</span>
    <span class="n">recall</span> <span class="o">=</span> <span class="n">best_estimator_results</span><span class="o">.</span><span class="n">mean_test_recall</span>
    <span class="n">precision</span> <span class="o">=</span> <span class="n">best_estimator_results</span><span class="o">.</span><span class="n">mean_test_precision</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="n">best_estimator_results</span><span class="o">.</span><span class="n">mean_test_accuracy</span>

    <span class="c1"># Create a table of results.</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">table</span> <span class="o">=</span> <span class="n">table</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;Model&#39;</span><span class="p">:</span> <span class="n">model_name</span><span class="p">,</span>
                          <span class="s1">&#39;F1&#39;</span><span class="p">:</span> <span class="n">f1</span><span class="p">,</span>
                          <span class="s1">&#39;Recall&#39;</span><span class="p">:</span> <span class="n">recall</span><span class="p">,</span>
                          <span class="s1">&#39;Precision&#39;</span><span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
                          <span class="s1">&#39;Accuracy&#39;</span><span class="p">:</span> <span class="n">accuracy</span><span class="p">},</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">table</span>

<span class="n">result_table</span> <span class="o">=</span> <span class="n">make_results</span><span class="p">(</span><span class="s2">&quot;Tuned Decision Tree&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="p">)</span>

<span class="n">result_table</span>
</pre></div>
</div>
<p>This is the output:</p>
<figure class="align-center">
<img alt="../_images/73.png" src="../_images/73.png" />
</figure>
<p><strong>Conclusions</strong></p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Precision measures what proportion of predicted positives is truly positive. For example, if you wanted to not falsely claiming a customer is satisfied, precision would be a good metric. Assuming a customer is happy when they are really not might lead to customer churn.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Recall measures the percentage of actual positives a model correctly identified (true positive). For this dataset, the airline might want to limit false negatives (actually satisfied people who are predicted to be unsatisfied). Assuming a customer is unhappy when the customer is happy can lead to the airline wasting resources trying to improve the customer experience of an already happy customer.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> F1 balances precision and recall. It is the harmonic mean of precision and recall, or their product divided by their sum.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Decision trees accurately predicted satisfaction over 94 percent of the time.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The confusion matrix is useful as it shows a similar number of true positives and true negatives.</p>
</section>
<section id="bagging">
<h3>7.5.5 Bagging<a class="headerlink" href="#bagging" title="Permalink to this heading"></a></h3>
<p>First we need to define two concepts including the <strong>ensemble learning</strong> and <strong>base learner</strong>:</p>
<p><strong>Ensemble learning</strong></p>
<p>Building multiple models and aggregating their predictions</p>
<p><strong>Base learner</strong></p>
<p>Each individual model that comprises an ensemble</p>
<p>The ensembles of base learners can combine to become powerful predictors. You learned about bagging,  and that it’s one of the more commonly used modeling strategies.</p>
<p><strong>Bagging</strong></p>
<p>Bagging includes bootstrapping and aggregating,</p>
<p><strong>Bootstrapping</strong></p>
<p>The bootstrapping refers to sampling with replacement. In ensemble modeling architectures, this means that for each base learner, the same observation can and will be sampled multiple times.Suppose you have a dataset of 1,000 observations, and you bootstrap sample it to generate a new dataset of 1,000 observations, on average, you should find about 632 of those observations in your sampled dataset (~63.2%).</p>
<p><strong>Aggregating</strong></p>
<p>Building a single model with bootstrapped data probably wouldn’t be very useful. To use the example above, if you start with 1,000 unique observations and use bootstrapping to create a sampled dataset of 1,000 observations, you’d only expect to get an average of 632 unique observations in that new dataset. This means that you’d lose whatever information was contained in the 368 observations that didn’t make it into the new sampled dataset.</p>
<p>This is when ensemble learning—or ensembling is helpful. In this example, those 368 observations might not make it into that particular sampled dataset, but if you keep repeating the bootstrapping process —once for each base learner—eventually your overall ensemble of base learners will see all of the observations.</p>
<p>There are 3 main advantages of bagging:</p>
<p><strong>1.Reduces variance:</strong> Standalone models can result in high variance. Aggregating base models’ predictions in an ensemble help reduce it.</p>
<p><strong>2.Fast:</strong> Training can happen in parallel across CPU cores and even across different servers.</p>
<p><strong>3.Good for big data:</strong> Bagging doesn’t require an entire training dataset to be stored in memory during model training. You can set the sample size for each bootstrap to a fraction of the overall data, train a base learner, and string these base learners together without ever reading in the entire dataset all at once.</p>
</section>
</section>
<section id="random-forest">
<h2>7.6. Random Forest<a class="headerlink" href="#random-forest" title="Permalink to this heading"></a></h2>
<p>Random forest is domination of <strong>Bagging</strong> and <strong>Random Feature Sampling</strong></p>
<p>You know that bootstrap aggregating—or bagging—can be an effective way to make predictions by building many base learners that are each trained on bootstrapped data and then combining their results. If you build a bagging ensemble of decision trees but take it one step further by randomizing the features used to train each base learner, the result is called a <strong>Random Forest</strong>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Random forest models uses randomness to reduce the likelihood that a given base learner will make the same mistakes as other base learners.</p>
</div>
<p>To illustrate this, consider a dataset with five observations: 1, 2, 3, 4, and 5. If you were to create a new, bootstrapped dataset of five observations from this original data, it might look like 1, 1, 3, 5, 5. It’s still five observations long, but some observations are missing and some are counted twice. The result is that the base learners are trained on data that is randomized by observation.</p>
<p>Random forest goes further. It randomizes the data by features too. This means that if there are five available features: A, B, C, D, and E, you can set the model to only sample from a subset of them. In other words, each base learner will only have a limited number of features available to it, but what those features are will vary between learners.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is possible for a model to perform better even with less bootstrap sample size and less features samples.</p>
</div>
<section id="build-a-random-forest-model-in-python">
<h3>7.6.1 Build a random forest model in Python<a class="headerlink" href="#build-a-random-forest-model-in-python" title="Permalink to this heading"></a></h3>
<p>Here we want to build a random forest model using the data of the airline. We will train, tune, and evaluate a random forest model using data from spreadsheet of survey responses from 129,880 customers. It includes data points such as class, flight distance, and inflight entertainment. The random forest model will be used to predict whether a customer will be satisfied with their flight experience.</p>
<p>First, we need to import the relevant python libraries:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import `numpy`, `pandas`, and `sklearn`.</span>
<span class="c1"># Import the relevant functions from `sklearn.ensemble`, `sklearn.model_selection`, and `sklearn.metrics`.</span>


<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">PredefinedSplit</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">accuracy_score</span>
</pre></div>
</div>
<p>Next we should import the data:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># RUN THIS CELL TO IMPORT YOUR DATA.</span>
<span class="n">air_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;Invistico_Airline.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Data cleaning</strong></p>
<p>Now, we should start cleaning the data.We should check for missing values in the rows of the data. Start with .isna() to get Booleans indicating whether each value in the data is missing. Then, use .any(axis=1) to get Booleans indicating whether there are any missing values along the columns in each row. Finally, use .sum() to get the number of rows that contain missing values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get Booleans to find missing values in data.</span>
<span class="c1"># Get Booleans to find missing values along columns.</span>
<span class="c1"># Get the number of rows that contain missing values.</span>

<span class="n">air_data</span><span class="o">.</span><span class="n">isna</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
</pre></div>
</div>
<p>Now we can see that there are 393 rows with missing values. Drop the rows with missing values. This is an important step in data cleaning, as it makes the data more useful for analysis and regression. Then, save the resulting pandas DataFrame in a variable named air_data_subset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Drop missing values.</span>
<span class="c1"># Save the DataFrame in variable `air_data_subset`.</span>

<span class="n">air_data_subset</span> <span class="o">=</span> <span class="n">air_data</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we are sure the new dataset does not contain any missing values.</p>
<p>Next, convert the categorical features to indicator (one-hot encoded) features (the target variable, satisfaction, does not need to be encoded and will be extracted in a later step):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert categorical features to one-hot encoded features.</span>

<span class="n">air_data_subset_dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">air_data_subset</span><span class="p">,</span>
                                 <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Customer Type&#39;</span><span class="p">,</span><span class="s1">&#39;Type of Travel&#39;</span><span class="p">,</span><span class="s1">&#39;Class&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><strong>Model building</strong></p>
<p>The first step to building your model is separating the labels (y) from the features (X):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Separate the dataset into labels (y) and features (X).</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">air_data_subset_dummies</span><span class="p">[</span><span class="s2">&quot;satisfaction&quot;</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">air_data_subset_dummies</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;satisfaction&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Once separated, split the data into train, validate, and test sets:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Separate into train, validate, test sets.</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">X_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_tr</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, fit and tune a random forest model with separate validation set. Begin by determining a set of hyperparameters for tuning the model using GridSearchCV:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Determine set of hyperparameters.</span>

<span class="n">cv_params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_estimators&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">],</span>
      <span class="s1">&#39;max_depth&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">],</span>
      <span class="s1">&#39;min_samples_leaf&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
      <span class="s1">&#39;min_samples_split&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
      <span class="s1">&#39;max_features&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="s2">&quot;sqrt&quot;</span><span class="p">],</span>
      <span class="s1">&#39;max_samples&#39;</span> <span class="p">:</span> <span class="p">[</span><span class="mf">.5</span><span class="p">,</span><span class="mf">.9</span><span class="p">]}</span>
</pre></div>
</div>
<p>Next, create a list of split indices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create list of split indices.</span>

<span class="n">split_index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">if</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_val</span><span class="o">.</span><span class="n">index</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_train</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
<span class="n">custom_split</span> <span class="o">=</span> <span class="n">PredefinedSplit</span><span class="p">(</span><span class="n">split_index</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, instantiate your model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, use GridSearchCV to search over the specified parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Search over specified parameters.</span>

<span class="n">rf_val</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">cv_params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">custom_split</span><span class="p">,</span> <span class="n">refit</span><span class="o">=</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, fit the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the model.</span>

<span class="n">rf_val</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output of the above lines:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Fitting</span> <span class="mi">1</span> <span class="n">folds</span> <span class="k">for</span> <span class="n">each</span> <span class="n">of</span> <span class="mi">32</span> <span class="n">candidates</span><span class="p">,</span> <span class="n">totalling</span> <span class="mi">32</span> <span class="n">fits</span>
<span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mf">3.39</span> <span class="n">s</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mi">234</span> <span class="n">ms</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mf">3.62</span> <span class="n">s</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mf">17.7</span> <span class="n">s</span>
<span class="n">GridSearchCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="n">PredefinedSplit</span><span class="p">(</span><span class="n">test_fold</span><span class="o">=</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])),</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;sqrt&#39;</span><span class="p">],</span>
                        <span class="s1">&#39;max_samples&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">],</span>
                        <span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                        <span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">],</span>
                        <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]},</span>
        <span class="n">refit</span><span class="o">=</span><span class="s1">&#39;f1&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, obtain the optimal parameters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Obtain optimal parameters.</span>

<span class="n">rf_val</span><span class="o">.</span><span class="n">best_params_</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;max_depth&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">,</span>
<span class="s1">&#39;max_features&#39;</span><span class="p">:</span> <span class="s1">&#39;sqrt&#39;</span><span class="p">,</span>
<span class="s1">&#39;max_samples&#39;</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">,</span>
<span class="s1">&#39;min_samples_leaf&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="s1">&#39;min_samples_split&#39;</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
<span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span> <span class="mi">50</span><span class="p">}</span>
</pre></div>
</div>
<p><strong>Results and evaluation</strong></p>
<p>We should use the selected model to predict on your test data. Use the optimal parameters found via GridSearchCV:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use optimal parameters on GridSearchCV.</span>

<span class="n">rf_opt</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                                <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">min_samples_split</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span>
                                <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;sqrt&quot;</span><span class="p">,</span> <span class="n">max_samples</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Once again, fit the optimal model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fit the optimal model.</span>

<span class="n">rf_opt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>And predict on the test set using the optimal model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict on test set.</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf_opt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Obtain performance scores</strong></p>
<p><strong>Evaluate the model</strong></p>
<p>Now that we have results, evaluate the model.There are the four basic parameters for evaluating the performance of a classification model:</p>
<blockquote>
<div><p><strong>1.</strong> True positives (TP): These are correctly predicted positive values, which means the value of actual and predicted classes are positive.</p>
<p><strong>2.</strong> True negatives (TN): These are correctly predicted negative values, which means the value of the actual and predicted classes are negative.</p>
<p><strong>3.</strong> False positives (FP): This occurs when the value of the actual class is negative and the value of the predicted class is positive.</p>
<p><strong>4.</strong> False negatives (FN): This occurs when the value of the actual class is positive and the value of the predicted class in negative.</p>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The goal is to minimize false positives and false negatives.</p>
</div>
<p>We review different types of scores again here:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Accuracy:</strong> (TP+TN/TP+FP+FN+TN): The ratio of correctly predicted observations to total observations.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Precision:</strong> (TP/TP+FP): The ratio of correctly predicted positive observations to total predicted positive observations.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Recall:</strong> (Sensitivity, TP/TP+FN): The ratio of correctly predicted positive observations to all observations in actual class.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>F1 score:</strong> The harmonic average of precision and recall, which takes into account both false positives and false negatives.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get precision score.</span>

<span class="n">pc_test</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span> <span class="o">=</span> <span class="s2">&quot;satisfied&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The precision score is </span><span class="si">{pc:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pc</span> <span class="o">=</span> <span class="n">pc_test</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">precision</span> <span class="n">score</span> <span class="ow">is</span> <span class="mf">0.950</span>
</pre></div>
</div>
<p>Then, collect the recall score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get recall score.</span>

<span class="n">rc_test</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span> <span class="o">=</span> <span class="s2">&quot;satisfied&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The recall score is </span><span class="si">{rc:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rc</span> <span class="o">=</span> <span class="n">rc_test</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">recall</span> <span class="n">score</span> <span class="ow">is</span> <span class="mf">0.945</span>
</pre></div>
</div>
<p>Next, obtain the accuracy score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get accuracy score.</span>

<span class="n">ac_test</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The accuracy score is </span><span class="si">{ac:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ac</span> <span class="o">=</span> <span class="n">ac_test</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">accuracy</span> <span class="n">score</span> <span class="ow">is</span> <span class="mf">0.942</span>
</pre></div>
</div>
<p>Finally, collect your F1-score:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get F1 score.</span>

<span class="n">f1_test</span> <span class="o">=</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span> <span class="o">=</span> <span class="s2">&quot;satisfied&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The F1 score is </span><span class="si">{f1:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">f1</span> <span class="o">=</span> <span class="n">f1_test</span><span class="p">))</span>
</pre></div>
</div>
<p>Here is the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">F1</span> <span class="n">score</span> <span class="ow">is</span> <span class="mf">0.947</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The tuned random forest has higher scores overall, so it is the better model. Particularly, it shows a better F1 score than the decision tree model, which indicates that the random forest model may do better at classification when taking into account false positives and false negatives</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> A separate validation set is typically used for tuning a model, rather than using the test set. This also helps avoid the evaluation becoming biased.</p>
</div>
<p>As a conclusion, the random forest model yields a more effective performance than a decision tree model.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="Linear%20Regression.html" class="btn btn-neutral float-left" title="6. Linear Regression" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="LDA.html" class="btn btn-neutral float-right" title="8. Machine Learning in Diagnosis" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Jafar Arash Mehr.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>