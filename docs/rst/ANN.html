<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>7. Deep Learning (Neural Networks) &mdash; Jafar Arash Mehr 2022 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="About the Author" href="contact.html" />
    <link rel="prev" title="6. Machine Learning in Diagnosis" href="LDA.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../index.html" class="icon icon-home"> Jafar Arash Mehr
            <img src="../_static/profile.jpg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Intro.html"><strong>What is this document about?</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="resource.html"><strong>Useful Resources</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="package.html"><strong>Required Packages</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperelasticity.html"><strong>1. Hyperelasticity</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="multi-physics.html"><strong>2. Multi-Physics Problem</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="drug-delivery.html"><strong>3. Drug Delivery Simulation</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html"><strong>4. Optimization (Python vs C++)</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="Linear_Regression.html"><strong>5. Linear Regression</strong></a></li>
<li class="toctree-l1"><a class="reference internal" href="LDA.html"><strong>6. Machine Learning in Diagnosis</strong></a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#"><strong>7. Deep Learning (Neural Networks)</strong></a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-is-neural-network">7.1. What is neural network?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#neural-networks-architecture">7.2.  Neural networks architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#feedforward-procedure">7.3. Feedforward procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#backpropagation-procedure">7.4. Backpropagation procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="#update-weights-and-biases">7.5. Update weights and biases</a></li>
<li class="toctree-l2"><a class="reference internal" href="#c-code-from-scratch">7.6. C++ code from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="#classification-of-an-eye-disease">7.7. Classification of an eye disease</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contact.html"><strong>About the Author</strong></a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/JafarArashMehr">GitHub</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Jafar Arash Mehr</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
      <li><strong>7. Deep Learning (Neural Networks)</strong></li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/rst/ANN.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="deep-learning-neural-networks">
<h1><strong>7. Deep Learning (Neural Networks)</strong><a class="headerlink" href="#deep-learning-neural-networks" title="Permalink to this heading"></a></h1>
<section id="what-is-neural-network">
<h2>7.1. What is neural network?<a class="headerlink" href="#what-is-neural-network" title="Permalink to this heading"></a></h2>
<p>A neural network, within the realm of artificial intelligence (AI), imparts to computers the ability to process
data in a manner inspired by the human brain. Operating as a form of deep learning, this method employs interconnected
nodes or neurons arranged in layered structures reminiscent of the human brain. This establishes an adaptive
system wherein computers learn from errors and consistently enhance their performance. Consequently, artificial
neural networks strive to address intricate problems, enabling computers to make intelligent decisions with minimal
human intervention. This capability arises from their capacity to grasp and model the intricate, nonlinear
relationships between input and output data.</p>
<p>Neural networks find application in various industries, including but not limited to:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\bullet\)</span> Medical diagnosis through the classification of medical images.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Targeted marketing via social network filtering and analysis of behavioral data.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Financial predictions through the analysis of historical data related to financial instruments.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Forecasting electrical load and energy demand.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Process and quality control.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Identification of chemical compounds.</p>
</div></blockquote>
<p>The design of neural network architecture draws inspiration from the human brain. In the human brain,
neurons, the fundamental units, create an intricate, highly interconnected network, transmitting electrical
signals to facilitate information processing. Similarly, artificial neural networks are comprised of artificial
neurons, represented as software modules or nodes, working collaboratively to tackle problem-solving tasks.
These artificial neurons operate within software programs or algorithms that leverage computing systems for
mathematical calculations.</p>
</section>
<section id="neural-networks-architecture">
<h2>7.2.  Neural networks architecture<a class="headerlink" href="#neural-networks-architecture" title="Permalink to this heading"></a></h2>
<p>The structure of a basic neural network involves three interconnected layers:</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Input Layer</strong>:</p>
<p>External information enters the artificial neural network through the input layer.
Input nodes within this layer process, analyze, or categorize the data before passing it on to the next layer.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Hidden Layer</strong>:</p>
<p>Hidden layers receive input from the input layer or other hidden layers.
Neural networks can feature multiple hidden layers, with each layer analyzing output from the preceding layer,
further processing it, and transmitting it to the subsequent layer.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> <strong>Output Layer</strong>:</p>
<p>The output layer presents the final result of the artificial neural network’s data processing.
It may consist of a single node or multiple nodes, depending on the task. For instance, in a binary
classification scenario (yes/no), the output layer may have one node yielding a result of 1 or 0.
In contrast, a multi-class classification problem might involve an output layer with more than one node.</p>
<figure class="align-center" id="id1">
<img alt="../_images/NN1.png" src="../_images/NN1.png" />
<figcaption>
<p><span class="caption-number">Figure.  20 </span><span class="caption-text">General structure of a neural network</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Artificial neural networks can be classified based on the flow of data from the input node to the output node. Here are some examples:</p>
<p><strong>1.</strong> <strong>Feedforward Neural Networks:</strong></p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\bullet\)</span> Data is processed in a unidirectional manner, moving from the input node to the output node.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Each node in one layer is connected to every node in the subsequent layer.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> Feedforward networks utilize a feedback process to iteratively enhance predictions over time.</p>
</div></blockquote>
<p><strong>2.</strong> <strong>Backpropagation Algorithm:</strong></p>
<p>Neural networks achieve continuous learning through corrective feedback loops, refining their predictive analytics.
Conceptually, data traverses multiple paths from the input node to the output node within the neural network.
A feedback loop is employed in the backpropagation algorithm, following these steps:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\bullet\)</span> Each node generates a prediction for the next node in the path.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The correctness of the prediction is assessed, with higher weight values assigned to paths
associated with correct
predictions and lower weight values to paths leading to incorrect predictions.</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> For the next data point, nodes make new predictions using the weighted paths and repeat
the process outlined in Step 1.</p>
</div></blockquote>
<p>In the next section, the mathematical framework of a simple neural network will be explained in detail. We consider a neural
network with 4 input data. Each data has 2 dimensions. We also assume, there is 1 hidden layer including 3 neurons. For
every single input data, we have 1 output which is either 0 or 1 (These are actual values).</p>
<p>First we need to go through the <strong>Feedforward</strong> process and then implement <strong>Backpropagation</strong>.</p>
</section>
<section id="feedforward-procedure">
<h2>7.3. Feedforward procedure<a class="headerlink" href="#feedforward-procedure" title="Permalink to this heading"></a></h2>
<p>The structure of our neural network and several steps involved in Feedforward process are shown in this figure:</p>
<figure class="align-center" id="id2">
<img alt="../_images/NN2.png" src="../_images/NN2.png" />
<figcaption>
<p><span class="caption-number">Figure.  21 </span><span class="caption-text">Feedforward steps in the neural network</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(\textbf {Step.1}\)</span></p>
<p>First we need to make a matrix including the input data. In general, this is a <span class="math notranslate nohighlight">\([d \times n]\)</span> matrix where
<span class="math notranslate nohighlight">\(d\)</span> and
<span class="math notranslate nohighlight">\(n\)</span> correspond to the dimensions (i.e., 2) and number of input data (i.e., 4) respectively. This is the very
first step of Feedforward implementation in our neural network. According to the fact that we have 4 input data,
the <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> matrix is formed as following:</p>
<div class="math notranslate nohighlight" id="eq-700">
<span id="equation-eq-700"></span><span class="eqno">(119)<a class="headerlink" href="#eq-700" title="Permalink to this equation"></a></span>\[\begin{split} \boldsymbol{X} =  \begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14}   \\ x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24}
  \\ \end{bmatrix}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\textbf {Step.2}\)</span></p>
<p>The values of the <span class="math notranslate nohighlight">\(\boldsymbol{W^{[1]}}\)</span> matrix are initially determined by generating random numbers
between 0 and 1 for each element.
In the second step, we need to make a matrix including the weights values(<span class="math notranslate nohighlight">\(\boldsymbol{W^{[1]}}\)</span>). This is
a <span class="math notranslate nohighlight">\([r \times d]\)</span> matrix where <span class="math notranslate nohighlight">\(b\)</span> is the number of the neurons in the hidden layer (i.e., 3):</p>
<div class="math notranslate nohighlight" id="eq-701">
<span id="equation-eq-701"></span><span class="eqno">(120)<a class="headerlink" href="#eq-701" title="Permalink to this equation"></a></span>\[\begin{split} \boldsymbol{W^{[1]}} =  \begin{bmatrix} {W^{[1]}}_{11} &amp; {W^{[1]}}_{12} \\ {W^{[1]}}_{21} &amp; {W^{[1]}}_{22}
   \\ {W^{[1]}}_{31} &amp; {W^{[1]}}_{32} \\  \end{bmatrix}\end{split}\]</div>
<p>Next we should transfer the data to the hidden layer by multiplication of <span class="math notranslate nohighlight">\(\boldsymbol{W^{[1]}}\)</span> to the
<span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> to form the <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> matrix with the dimensions of <span class="math notranslate nohighlight">\([r \times n]\)</span>
(In this example we will have a <span class="math notranslate nohighlight">\([3 \times 4]\)</span> matrix)</p>
<div class="math notranslate nohighlight" id="eq-702">
<span id="equation-eq-702"></span><span class="eqno">(121)<a class="headerlink" href="#eq-702" title="Permalink to this equation"></a></span>\[ \boldsymbol{h} =  \boldsymbol{W^{[1]}} \times \boldsymbol{X}\]</div>
<p>The values in the neurons inside the hidden layer are calculated as:</p>
<div class="math notranslate nohighlight" id="eq-703">
<span id="equation-eq-703"></span><span class="eqno">(122)<a class="headerlink" href="#eq-703" title="Permalink to this equation"></a></span>\[ \begin{align}\begin{aligned} \vec{\boldsymbol{h_1}}= \begin{gathered} \begin{bmatrix} h_{11} = {W^{[1]}}_{11} x_{11} +{W^{[1]}}_{12} x_{21} \quad &amp;
 h_{12} = {W^{[1]}}_{11} x_{12} +{W^{[1]}}_{12} x_{22}   \quad &amp;
 h_{13} = {W^{[1]}}_{11} x_{13} +{W^{[1]}}_{12} x_{23}  \quad &amp;
 h_{14} = {W^{[1]}}_{11} x_{14} +{W^{[1]}}_{12} x_{24} \end{bmatrix} \end{gathered}\\ \vec{\boldsymbol{h_2}} = \begin{gathered} \begin{bmatrix} h_{21} = {W^{[1]}}_{21} x_{11} +{W^{[1]}}_{22} x_{21} \quad  &amp;
 h_{22} = {W^{[1]}}_{21} x_{12} +{W^{[1]}}_{22} x_{22}  \quad&amp;
 h_{23} = {W^{[1]}}_{21} x_{13} +{W^{[1]}}_{22} x_{23}   \quad &amp;
 h_{24} = {W^{[1]}}_{21} x_{14} +{W^{[1]}}_{22} x_{24} \end{bmatrix} \end{gathered}\\ \vec{\boldsymbol{h_3}} = \begin{gathered} \begin{bmatrix} h_{31} = {W^{[1]}}_{31} x_{11} +{W^{[1]}}_{32} x_{21} \quad &amp;
 h_{32} = {W^{[1]}}_{31} x_{12} +{W^{[1]}}_{32} x_{22}   \quad &amp;
 h_{33} = {W^{[1]}}_{31} x_{13} +{W^{[1]}}_{32} x_{23}  \quad &amp;
 h_{34} = {W^{[1]}}_{31} x_{14} +{W^{[1]}}_{32} x_{24}  \end{bmatrix} \end{gathered}\end{aligned}\end{align} \]</div>
<p>In the above, the <span class="math notranslate nohighlight">\(\vec{\boldsymbol{h_1}}\)</span>, <span class="math notranslate nohighlight">\(\vec{\boldsymbol{h_2}}\)</span> and <span class="math notranslate nohighlight">\(\vec{\boldsymbol{h_3}}\)</span>, are
the first, second and third row in the <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> matrix.</p>
<p><span class="math notranslate nohighlight">\(\textbf {Step.3}\)</span></p>
<p>This step is called <span class="math notranslate nohighlight">\(\textbf{Activation of the Neurons}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The activation function plays a crucial role in determining whether a neuron should be activated. This decision is
made by calculating the weighted sum of inputs and adding bias to it.</p>
</div>
<p>Before moving to the activation mode, we should define a <span class="math notranslate nohighlight">\(r\)</span>-dimensional vector for the bias values:</p>
<div class="math notranslate nohighlight" id="eq-704">
<span id="equation-eq-704"></span><span class="eqno">(123)<a class="headerlink" href="#eq-704" title="Permalink to this equation"></a></span>\[\begin{split}\vec{\boldsymbol{bias}}^{[1]} = \begin{bmatrix} {bias^{[1]}}_1 \\ {bias^{[1]}}_2  \\ {bias^{[1]}}_3
\end{bmatrix}\end{split}\]</div>
<p>The values in the <span class="math notranslate nohighlight">\(\boldsymbol{bias}^{[1]}\)</span> vector should be added in a vector-wise fashion to the values
in the <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> to form the <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b]}}\)</span> matrix:</p>
<div class="math notranslate nohighlight" id="eq-705">
<span id="equation-eq-705"></span><span class="eqno">(124)<a class="headerlink" href="#eq-705" title="Permalink to this equation"></a></span>\[\begin{split}\boldsymbol{h^{[b]}} = \begin{gathered} \begin{bmatrix} h_{11} + {bias^{[1]}}_1 \quad &amp; h_{12} +  {bias^{[1]}}_1 \quad &amp;
 h_{13} + {bias^{[1]}}_1 \quad &amp;  h_{14} + {bias^{[1]}}_1 \\
 h_{21} + {bias^{[1]}}_2 \quad  &amp; h_{22} + {bias^{[1]}}_2 \quad &amp; h_{23} + {bias^{[1]}}_2 \quad &amp;
 h_{24} + {bias^{[1]}}_2 \\
 h_{31} + {bias^{[1]}}_3 \quad &amp; h_{32} +  {bias^{[1]}}_3 \quad &amp; h_{33} + {bias^{[1]}}_3 \quad &amp;
 h_{34} + {bias^{[1]}}_3 \end{bmatrix} \end{gathered}\end{split}\]</div>
<p>Now, this is the time to perform the activation mode using applying the <span class="math notranslate nohighlight">\(\textbf{Sigmoid Function}\)</span> to the members
of <span class="math notranslate nohighlight">\(\boldsymbol{h}\)</span> matrix. The <span class="math notranslate nohighlight">\(\textbf{Sigmoid Function}\)</span> is defined as:</p>
<div class="math notranslate nohighlight" id="eq-706">
<span id="equation-eq-706"></span><span class="eqno">(125)<a class="headerlink" href="#eq-706" title="Permalink to this equation"></a></span>\[f(x) = \frac {1}{1+e^{-x}}\]</div>
<p>After, applying the <span class="math notranslate nohighlight">\(\textbf{Sigmoid Function}\)</span> to the <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b]}}\)</span> matrix, we can form the
<span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-707">
<span id="equation-eq-707"></span><span class="eqno">(126)<a class="headerlink" href="#eq-707" title="Permalink to this equation"></a></span>\[\begin{split}\boldsymbol{h^{[b \Rightarrow a]}} = \begin{gathered} \begin{bmatrix} f({h^{[b]}}_{11})  \quad &amp; f({h^{[b]}}_{12})
\quad &amp; f({h^{[b]}}_{13})  \quad &amp;  f({h^{[b]}}_{14})  \\
 f({h^{[b]}}_{21})  \quad  &amp; f({h^{[b]}}_{22})  \quad &amp; f({h^{[b]}}_{23})  \quad &amp;
 f({h^{[b]}}_{24})  \\
 f({h^{[b]}}_{31})  \quad &amp; f({h^{[b]}}_{32})  \quad &amp; f({h^{[b]}}_{33})  \quad &amp;
 f({h^{[b]}}_{34})  \end{bmatrix} \end{gathered}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\textbf {Step.4}\)</span></p>
<p>Here we need to define the second set of the weights (<span class="math notranslate nohighlight">\(\boldsymbol{W^{[2]}}\)</span>) which is a <span class="math notranslate nohighlight">\(r\)</span>
dimensional vector:</p>
<div class="math notranslate nohighlight" id="eq-708">
<span id="equation-eq-708"></span><span class="eqno">(127)<a class="headerlink" href="#eq-708" title="Permalink to this equation"></a></span>\[{\vec{\boldsymbol{W}}}^{[2]} = \begin{bmatrix} {W^{[2]}}_1 \quad &amp; {W^{[2]}}_2 \quad &amp; {W^{[2]}}_3 \end{bmatrix}\]</div>
<p>The values of the <span class="math notranslate nohighlight">\(\boldsymbol{W^{[2]}}\)</span> vector are initially determined by generating random numbers
between 0 and 1 for each element.</p>
<p>In this step, the <span class="math notranslate nohighlight">\(\boldsymbol{W^{[2]}}\)</span> vector should be multiplied into the
<span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span> matrix. The output is a <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector:</p>
<div class="math notranslate nohighlight" id="eq-709">
<span id="equation-eq-709"></span><span class="eqno">(128)<a class="headerlink" href="#eq-709" title="Permalink to this equation"></a></span>\[\vec{\boldsymbol{u}} = {\vec{\boldsymbol{W}}}^{[2]} \times \boldsymbol{h^{[b \Rightarrow a]}}\]</div>
<p>It will result into obtaining the <span class="math notranslate nohighlight">\(\vec{\boldsymbol{u}}\)</span> as following:</p>
<div class="math notranslate nohighlight" id="eq-710">
<span id="equation-eq-710"></span><span class="eqno">(129)<a class="headerlink" href="#eq-710" title="Permalink to this equation"></a></span>\[ \begin{align}\begin{aligned}u_{11} = {h^{[b \Rightarrow a]}}_{11} {W^{[2]}}_1 + {h^{[b \Rightarrow a]}}_{21} {W^{[2]}}_2 +
{h^{[b \Rightarrow a]}}_{31} {W^{[2]}}_3\\  u_{12} = {h^{[b \Rightarrow a]}}_{12} {W^{[2]}}_1 + {h^{[b \Rightarrow a]}}_{22} {W^{[2]}}_2 +
{h^{[b \Rightarrow a]}}_{32} {W^{[2]}}_3\\  u_{13} = {h^{[b \Rightarrow a]}}_{13} {W^{[2]}}_1 + {h^{[b \Rightarrow a]}}_{23} {W^{[2]}}_2 +
{h^{[b \Rightarrow a]}}_{33} {W^{[2]}}_3\\  u_{14} = {h^{[b \Rightarrow a]}}_{14} {W^{[2]}}_1 + {h^{[b \Rightarrow a]}}_{24} {W^{[2]}}_2 +
{h^{[b \Rightarrow a]}}_{34} {W^{[2]}}_3\end{aligned}\end{align} \]</div>
<p><span class="math notranslate nohighlight">\(\textbf {Step.5}\)</span></p>
<p>This is the last step in the Feedforward process where we should active the <span class="math notranslate nohighlight">\(\vec{\boldsymbol{u}}\)</span>.
Like what was done for the hidden layer, first, we need to add <span class="math notranslate nohighlight">\(bias^{[2]}\)</span> which is a scalar value
to all members of the <span class="math notranslate nohighlight">\(\vec{\boldsymbol{u}}\)</span>. With that being said, we will form
a new vector which is <span class="math notranslate nohighlight">\(\vec{\boldsymbol{u^{[b]}}}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-711">
<span id="equation-eq-711"></span><span class="eqno">(130)<a class="headerlink" href="#eq-711" title="Permalink to this equation"></a></span>\[{\vec{\boldsymbol{u}}}^{[b]} = \begin{gathered} \begin{bmatrix} u_{11}+bias^{[2]} \quad &amp; u_{12}+bias^{[2]} \quad &amp;
u_{13}+bias^{[2]} \quad &amp; u_{14}+bias^{[2]}
  \end{bmatrix} \end{gathered}\]</div>
<p>At the end, the <span class="math notranslate nohighlight">\(\textbf{Sigmoid Function}\)</span> should be applied on the members of the
<span class="math notranslate nohighlight">\(\vec{\boldsymbol{u^{[b]}}}\)</span> to produce the output values which is in a vector:</p>
<div class="math notranslate nohighlight" id="eq-712">
<span id="equation-eq-712"></span><span class="eqno">(131)<a class="headerlink" href="#eq-712" title="Permalink to this equation"></a></span>\[{\vec{\boldsymbol{u}}}^{[b \Rightarrow a]} = \begin{gathered} \begin{bmatrix} f({u^{[b]}}_{11}) \quad &amp;
f({u^{[b]}}_{12}) \quad &amp;
f({u^{[b]}}_{13}) \quad &amp;
f({u^{[b]}}_{14})
  \end{bmatrix} \end{gathered}\]</div>
<p>The members of the above vector are the values that our neural network predicts associated with each input data.</p>
</section>
<section id="backpropagation-procedure">
<h2>7.4. Backpropagation procedure<a class="headerlink" href="#backpropagation-procedure" title="Permalink to this heading"></a></h2>
<p>Now, the neural net should go through a Backpropagation with the purpose of correction and updating the weights values.
First off, the error must be calculated based on the predicted values at the end of the Feedforward process. The error
is calculated based on the <span class="math notranslate nohighlight">\(\textbf{Mean Squared Error}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-713">
<span id="equation-eq-713"></span><span class="eqno">(132)<a class="headerlink" href="#eq-713" title="Permalink to this equation"></a></span>\[Error =  \frac {1}{2n}\sum_{i=0}^n [{(y_{real}})_i - ({y_{predicted}})_i]^2\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It should be recalled that <span class="math notranslate nohighlight">\({\vec{\boldsymbol{y}}}_{predicted}\)</span> =
<span class="math notranslate nohighlight">\(\vec{\boldsymbol{u}}^{[b \Rightarrow a]}\)</span>. In addition, the superscript  <span class="math notranslate nohighlight">\(\boldsymbol{[b \Rightarrow a]}\)</span>
corresponds to the stage where:</p>
<blockquote>
<div><p><span class="math notranslate nohighlight">\(\bullet\)</span> The <span class="math notranslate nohighlight">\(\boldsymbol{bias}\)</span> value has been added</p>
<p><span class="math notranslate nohighlight">\(\bullet\)</span> The <span class="math notranslate nohighlight">\(\boldsymbol{Activation Function}\)</span> has been applied.</p>
</div></blockquote>
</div>
<p>There are 5 steps in the Backpropagation procedure that will be discussed one by one. The diagram of the
Backpropagation is shown in this figure:</p>
<figure class="align-center" id="id3">
<img alt="../_images/NN3.png" src="../_images/NN3.png" />
<figcaption>
<p><span class="caption-number">Figure.  22 </span><span class="caption-text">Backpropagation steps in neural network</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p><span class="math notranslate nohighlight">\(\textbf {Step.1}\)</span></p>
<p>In the first step, we should calculate the derivative of the error with respect to the
<span class="math notranslate nohighlight">\({\vec{\boldsymbol{y}}}_{predicted}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-714">
<span id="equation-eq-714"></span><span class="eqno">(133)<a class="headerlink" href="#eq-714" title="Permalink to this equation"></a></span>\[{\vec {\boldsymbol{V}}}_{step.1} = \frac {\partial {Error}} {\partial {\boldsymbol{\vec{y}}}_{predicted}} =
- \frac {1}{n}  [({\boldsymbol{\vec {y}}}_{real}) - ({\boldsymbol{\vec {y}}}_{predicted})]\]</div>
<p>In the above equation, the members of 2 vectros including <span class="math notranslate nohighlight">\({\vec{\boldsymbol{y}}}_{real}\)</span> and
<span class="math notranslate nohighlight">\({\vec{\boldsymbol{y}}}_{predicted}\)</span> should be deducted in an <strong>element-wise</strong> manner. Note that the
<span class="math notranslate nohighlight">\({\vec {\boldsymbol{V}}}_{step.1}\)</span> is a <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector.</p>
<p><span class="math notranslate nohighlight">\(\textbf {Step.2}\)</span></p>
<p>In this step, we should take derivative of the <span class="math notranslate nohighlight">\({\vec{\boldsymbol{y}}}_{predicted}\)</span> with respect to the
<span class="math notranslate nohighlight">\({\vec{u}}^{[b]}\)</span>.
To this end, we should calculate the derivative of the <span class="math notranslate nohighlight">\(\textbf{Sigmoid Function}\)</span>. The derivative of the
<span class="math notranslate nohighlight">\(\textbf{Sigmoid Function}\)</span> is calculated as:</p>
<div class="math notranslate nohighlight" id="eq-715">
<span id="equation-eq-715"></span><span class="eqno">(134)<a class="headerlink" href="#eq-715" title="Permalink to this equation"></a></span>\[\frac {\partial f(x)}{\partial x} = f(x) \times [1-f(x)]\]</div>
<p>Now we can write:</p>
<div class="math notranslate nohighlight" id="eq-716">
<span id="equation-eq-716"></span><span class="eqno">(135)<a class="headerlink" href="#eq-716" title="Permalink to this equation"></a></span>\[\frac {\partial {\vec{\boldsymbol{y}}}_{predicted}}
{\partial {\vec{\boldsymbol{u}}}^{[b]}}=
 \begin{gathered} \begin{bmatrix} f({u^{[b]}}_{11})  &amp; f({u^{[b]}}_{12})  &amp;
 f({u^{[b]}}_{13})  &amp; f({u^{[b]}}_{14}) \end{bmatrix} \end{gathered} \odot
 \begin{gathered} \begin{bmatrix} (1-f({u^{[b]}}_{11})) &amp; (1-f({u^{[b]}}_{12}))  &amp;
 (1-f({u^{[b]}}_{13}))  &amp; (1-f({u^{[b]}}_{14})) \end{bmatrix} \end{gathered}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <span class="math notranslate nohighlight">\(\odot\)</span> defines the <strong>element-wise</strong> multiplication.</p>
</div>
<p>So far, we have reached to the <span class="math notranslate nohighlight">\({\vec{\boldsymbol{u}}}^{[b]}\)</span> where we have:</p>
<div class="math notranslate nohighlight" id="eq-717">
<span id="equation-eq-717"></span><span class="eqno">(136)<a class="headerlink" href="#eq-717" title="Permalink to this equation"></a></span>\[{\vec {\boldsymbol{V}}}_{step.2} = \frac {\partial Error}{\partial {\vec{\boldsymbol{u}}}^{[b]}} =
\frac {\partial Error}{\partial {\vec{\boldsymbol{y}}}_{predicted}} \odot
\frac {\partial {\vec{\boldsymbol{y}}}_{predicted}}{\partial {\vec{\boldsymbol{u}}}^{[b]}} =
{\vec {\boldsymbol{V}}}_{step.1} \odot \frac {\partial {\vec{\boldsymbol{y}}}_{predicted}}{\partial {\vec{\boldsymbol{u}}}^{[b]}}\]</div>
<p>In the above equation, the first term on the right hand-side was obtained in the <a class="reference internal" href="#eq-714"><span class="std std-ref">Equation.133</span></a> and the second term
was found in <a class="reference internal" href="#eq-716"><span class="std std-ref">Equation.135</span></a>.</p>
<p>Note that the <span class="math notranslate nohighlight">\({\partial Error}/{\partial {\vec{\boldsymbol{u}}}^{[b]}}\)</span> is a <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector.</p>
<p><span class="math notranslate nohighlight">\(\textbf {Step.3}\)</span></p>
<p>At this step, we move one more step backward to get to the <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span>.
The purpose is to
calculate the <span class="math notranslate nohighlight">\({\partial Error} / {\partial \boldsymbol{h^{[b \Rightarrow a]}}}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-718">
<span id="equation-eq-718"></span><span class="eqno">(137)<a class="headerlink" href="#eq-718" title="Permalink to this equation"></a></span>\[{{\boldsymbol{V}}}_{step.3} = \frac{{\partial Error}} {\partial \boldsymbol{h^{[b \Rightarrow a]}}} =
\frac {\partial{\vec{\boldsymbol{u}}}^{[b]}}{\partial \boldsymbol{h^{[b \Rightarrow a]}}}
 \frac{\partial Error}{\partial {\vec{\boldsymbol{u}}}^{[b]}}\]</div>
<p>According to the <a class="reference internal" href="#eq-709"><span class="std std-ref">Equation.128</span></a>, the first term in the right hand-side of the above equation, is equal
to <span class="math notranslate nohighlight">\({\vec{\boldsymbol{W}}}^{[2]}\)</span>. In addition, the second term in the right hand-side of the above equation was
found in  <a class="reference internal" href="#eq-717"><span class="std std-ref">Equation.136</span></a></p>
<p>Note that the <span class="math notranslate nohighlight">\({\vec{\boldsymbol{W}}}^{[2]}\)</span> is a <span class="math notranslate nohighlight">\([r \times 1]\)</span> vector and the
<span class="math notranslate nohighlight">\({\vec{\boldsymbol{V}}}_{step.2}\)</span> is a <span class="math notranslate nohighlight">\([1 \times n]\)</span> vector. Thus, the
<span class="math notranslate nohighlight">\({\vec{\boldsymbol{V}}}_{step.3}\)</span> is a <span class="math notranslate nohighlight">\([r \times n]\)</span> matrix.</p>
<p>There are two more local gradient that we need to calculate in this step:</p>
<p><span class="math notranslate nohighlight">\(\textbf {1. }\)</span> <span class="math notranslate nohighlight">\({\partial Error} / {\partial \vec{\boldsymbol{W}}^{[2]}}\)</span></p>
<p>In order to calculate the above derivative, using chain rule we can write:</p>
<div class="math notranslate nohighlight" id="eq-719">
<span id="equation-eq-719"></span><span class="eqno">(138)<a class="headerlink" href="#eq-719" title="Permalink to this equation"></a></span>\[\frac{{\partial Error}} {{\partial \vec{\boldsymbol{W}}^{[2]}}} =
\frac{{\partial Error}} {\partial {\vec{\boldsymbol{u}}}^{[b]}}
\frac{\partial {\vec{\boldsymbol{u}}}^{[b]}} {\partial \vec{\boldsymbol{W}}^{[2]}}\]</div>
<p>The first term in the right hand-side of the above equation was obtained in <a class="reference internal" href="#eq-717"><span class="std std-ref">Equation.136</span></a>. In addition,
according to the <a class="reference internal" href="#eq-709"><span class="std std-ref">Equation.128</span></a>, the second term in the right hand-side of the above equation, is equal
to <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span>. Please note that the
<span class="math notranslate nohighlight">\({\partial Error} / {\partial \vec{\boldsymbol{u}^{[b]}}}\)</span> is a <span class="math notranslate nohighlight">\([1 \times n]\)</span> vector. This vector should be
multiplied by <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span> and as this is a <span class="math notranslate nohighlight">\([r \times n]\)</span> matrix, for calculation of
<a class="reference internal" href="#eq-719"><span class="std std-ref">Equation.138</span></a>, we should reshape the <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span> by transposing it.
To be more specific, for the <span class="math notranslate nohighlight">\(\boldsymbol{h^{[b \Rightarrow a]}}\)</span>, we should consider a <span class="math notranslate nohighlight">\([n \times r]\)</span>
matrix instead.</p>
<p>Finally, the output of the <span class="math notranslate nohighlight">\({\partial Error} / {\partial \vec{\boldsymbol{W}}^{[2]}}\)</span> is a
<span class="math notranslate nohighlight">\([1 \times r]\)</span> vector.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, as the <span class="math notranslate nohighlight">\(\vec{\boldsymbol{W}}^{[2]}\)</span> is a <span class="math notranslate nohighlight">\(r\)</span>-dimensional vector, the
<span class="math notranslate nohighlight">\({{\partial Error}} / {{\partial \vec{\boldsymbol{W}}^{[2]}}}\)</span> is also a <span class="math notranslate nohighlight">\(r\)</span>-dimensional vector</p>
</div>
<p><span class="math notranslate nohighlight">\(\textbf {2. }\)</span> <span class="math notranslate nohighlight">\({\partial Error} / {\partial {bias^{[2]}}}\)</span></p>
<p>In order to calculate the above derivative, using chain rule we can write:</p>
<div class="math notranslate nohighlight" id="eq-720">
<span id="equation-eq-720"></span><span class="eqno">(139)<a class="headerlink" href="#eq-720" title="Permalink to this equation"></a></span>\[\frac{{\partial Error}} {{\partial {bias^{[2]}}}} =
\frac{{\partial Error}} {\partial {\vec{\boldsymbol{u}}}^{[b]}}
\frac{\partial {\vec{\boldsymbol{u}}}^{[b]}} {\partial {bias}^{[2]}}\]</div>
<p>Again, the first term in the right hand-side of the above equation was obtained in <a class="reference internal" href="#eq-717"><span class="std std-ref">Equation.136</span></a>.
Furthermore, according to the <a class="reference internal" href="#eq-711"><span class="std std-ref">Equation.130</span></a>, the second term in the right hand-side of the above
equation is a <span class="math notranslate nohighlight">\(n\)</span>-dimensional vector where all <span class="math notranslate nohighlight">\(n\)</span> elements are equal to 1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, as the <span class="math notranslate nohighlight">\(bias^{[2]}\)</span> is a scalar, the
<span class="math notranslate nohighlight">\({{\partial Error}}/ {{\partial {bias}^{[2]}}}\)</span> is also a scalar</p>
</div>
<p>According to the above note, we need to calculate the inner product of 2 vectors on the right-hand side of the
<a class="reference internal" href="#eq-720"><span class="std std-ref">Equation.139</span></a>. Now we can re-write the <a class="reference internal" href="#eq-720"><span class="std std-ref">Equation.139</span></a>:</p>
<div class="math notranslate nohighlight" id="eq-721">
<span id="equation-eq-721"></span><span class="eqno">(140)<a class="headerlink" href="#eq-721" title="Permalink to this equation"></a></span>\[\frac{{\partial Error}} {{\partial {bias^{[2]}}}} = \sum_{i=0}^n [
\frac{{\partial Error}} {\partial {\vec{\boldsymbol{u}}}^{[b]}}]_{i}\]</div>
<p><span class="math notranslate nohighlight">\(\textbf {Step.4}\)</span></p>
<p>Here we move another step backward to calculate the <span class="math notranslate nohighlight">\({{\partial Error}} / {{\partial {h^{[b]}}}}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-722">
<span id="equation-eq-722"></span><span class="eqno">(141)<a class="headerlink" href="#eq-722" title="Permalink to this equation"></a></span>\[{{\boldsymbol{V}}}_{step.4} = \frac{{\partial Error}} {{\partial {h^{[b]}}}} =
\frac{{\partial Error}} {\partial {\boldsymbol{h}}^{[b \Rightarrow a]}} \odot
\frac{{\partial {\boldsymbol{h}}^{[b \Rightarrow a]}}} {\partial {\boldsymbol{h}}^{[b]}}\]</div>
<p>The first term in the right hand-side of the above equation was obtained in <a class="reference internal" href="#eq-718"><span class="std std-ref">Equation.137</span></a>.
In addition, based on <a class="reference internal" href="#eq-707"><span class="std std-ref">Equation.126</span></a> we can write the second term on the right-hand side as following:</p>
<div class="math notranslate nohighlight" id="eq-723">
<span id="equation-eq-723"></span><span class="eqno">(142)<a class="headerlink" href="#eq-723" title="Permalink to this equation"></a></span>\[\begin{split}\frac{{\partial {\boldsymbol{h}}^{[b \Rightarrow a]}}} {\partial {\boldsymbol{h}}^{[b]}} =
\begin{gathered} \begin{bmatrix} f({h^{[b]}}_{11})  \quad &amp; f({h^{[b]}}_{12})
\quad &amp; f({h^{[b]}}_{13})  \quad &amp;  f({h^{[b]}}_{14})  \\
 f({h^{[b]}}_{21})  \quad  &amp; f({h^{[b]}}_{22})  \quad &amp; f({h^{[b]}}_{23})  \quad &amp;
 f({h^{[b]}}_{24})  \\
 f({h^{[b]}}_{31})  \quad &amp; f({h^{[b]}}_{32})  \quad &amp; f({h^{[b]}}_{33})  \quad &amp;
 f({h^{[b]}}_{34})  \end{bmatrix} \end{gathered} \odot
 \begin{gathered} \begin{bmatrix} (1-f({h^{[b]}}_{11}))  \quad &amp; (1-f({h^{[b]}}_{12}))
\quad &amp; (1-f({h^{[b]}}_{13}))  \quad &amp;  (1-f({h^{[b]}}_{14}))  \\
 (1-({h^{[b]}}_{21}))  \quad  &amp; (1-f({h^{[b]}}_{22}))  \quad &amp; (1-f({h^{[b]}}_{23}))  \quad &amp;
 (1-f({h^{[b]}}_{24}))  \\
 (1-f({h^{[b]}}_{31}))  \quad &amp; (1-f({h^{[b]}}_{32}))  \quad &amp; (1-f({h^{[b]}}_{33}))  \quad &amp;
 (1-f({h^{[b]}}_{34}))  \end{bmatrix} \end{gathered}\end{split}\]</div>
<p>Finally, the <span class="math notranslate nohighlight">\({{\boldsymbol{V}}}_{step.4}\)</span> is a <span class="math notranslate nohighlight">\([r \times n]\)</span> matrix.</p>
<p><span class="math notranslate nohighlight">\(\textbf {Step.5}\)</span></p>
<p>In the last step of the Backpropagation we should calculate
<span class="math notranslate nohighlight">\({{\partial Error}} / {\partial \boldsymbol{W^{[1]}}}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-724">
<span id="equation-eq-724"></span><span class="eqno">(143)<a class="headerlink" href="#eq-724" title="Permalink to this equation"></a></span>\[ \frac {{\partial Error}}{\partial \boldsymbol{W^{[1]}}} = \frac {{\partial Error}} {\partial {\boldsymbol{h}}^{[b]}}
 \frac {{\partial {\boldsymbol{h}}}^{[b]}} {\partial \boldsymbol{W^{[1]}}}\]</div>
<p>The first term in the right hand-side of the above equation was obtained in <a class="reference internal" href="#eq-722"><span class="std std-ref">Equation.141</span></a> which is
the same thing found in the <strong>Step.4</strong>.</p>
<p>In addition, the second term on the right-hand side of the above equation is found based on <a class="reference internal" href="#eq-702"><span class="std std-ref">Equation.121</span></a>
which is equal to <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>. It should be noted that <span class="math notranslate nohighlight">\(\boldsymbol{V_{step.4}}\)</span> is a
<span class="math notranslate nohighlight">\([r \times n]\)</span> matrix tht should be multiplied by <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> which is a <span class="math notranslate nohighlight">\([d \times n]\)</span> matrix.
In order to perform this matrix multiplication, we require to reshape the <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> by transposing it.
to have a <span class="math notranslate nohighlight">\([n \times d]\)</span> matrix instead.</p>
<p>Finally, the <span class="math notranslate nohighlight">\({{\partial Error}} / {\partial \boldsymbol{W^{[1]}}}\)</span> is a <span class="math notranslate nohighlight">\([r \times d]\)</span> matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In general, as the <span class="math notranslate nohighlight">\(\boldsymbol{W^{[1]}}\)</span> is a matrix, the
<span class="math notranslate nohighlight">\({{\partial Error}}/ {\partial \boldsymbol{W^{[1]}}}\)</span> is also a matrix with the same dimensions</p>
</div>
<p>Next, we should calculate <span class="math notranslate nohighlight">\({{\partial Error}} / {\partial {\vec{\boldsymbol{bias}}}^{[1]}}\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-725">
<span id="equation-eq-725"></span><span class="eqno">(144)<a class="headerlink" href="#eq-725" title="Permalink to this equation"></a></span>\[ \frac {{\partial Error}}{\partial {\vec{\boldsymbol{bias}}}^{[1]}} =
 \frac {{\partial Error}} {\partial {\boldsymbol{h}}^{[b]}} \odot
 \frac {{\partial {\boldsymbol{h}}}^{[b]}} {\partial {\vec{\boldsymbol{bias}}}^{[1]}}\]</div>
<p>Again, the first term in the right hand-side of the above equation was obtained in <a class="reference internal" href="#eq-722"><span class="std std-ref">Equation.141</span></a> which is
the same thing found in the <strong>Step.4</strong>. In addition, the second term on the right-hand side of the above equation is found based on <a class="reference internal" href="#eq-702"><span class="std std-ref">Equation.121</span></a>
which is equal to:</p>
<div class="math notranslate nohighlight" id="eq-726">
<span id="equation-eq-726"></span><span class="eqno">(145)<a class="headerlink" href="#eq-726" title="Permalink to this equation"></a></span>\[\begin{split} \frac {{\partial {\boldsymbol{h}}}^{[b]}} {\partial {\vec{\boldsymbol{bias}}}^{[1]}}=
 \begin{gathered} \begin{bmatrix} 1  \quad &amp; 1 \quad &amp; 1  \quad &amp;  1  \\
 1  \quad  &amp; 1 \quad &amp;  1 \quad &amp; 1 \\
  1 \quad &amp;  1 \quad &amp;  1 \quad &amp; 1 \\
   \end{bmatrix} \end{gathered}\end{split}\]</div>
<p>According to the above equation, the <span class="math notranslate nohighlight">\({{\partial Error}} / {\partial {\vec{\boldsymbol{bias}}}^{[1]}}\)</span> is
obtained by <span class="math notranslate nohighlight">\(\textbf {row-wise inner product }\)</span> of the <span class="math notranslate nohighlight">\({{\partial Error}} / {\partial {\boldsymbol{h}}^{[b]}}\)</span> and
<span class="math notranslate nohighlight">\({{\partial {\boldsymbol{h}}}^{[b]}} / {\partial {\vec{\boldsymbol{bias}}}^{[1]}}\)</span> that yields to:</p>
<div class="math notranslate nohighlight" id="eq-727">
<span id="equation-eq-727"></span><span class="eqno">(146)<a class="headerlink" href="#eq-727" title="Permalink to this equation"></a></span>\[\begin{split} \frac {{\partial Error}}{\partial {\vec{\boldsymbol{bias}}}^{[1]}}=
 \begin{gathered} \begin{bmatrix} \sum\limits_{i=0}^n (\frac {{\partial Error}} {\partial {\boldsymbol{h}}^{[b]}})_{1i}\\
 \sum\limits_{i=0}^n (\frac {{\partial Error}} {\partial {\boldsymbol{h}}^{[b]}})_{2i} \\
 \sum\limits_{i=0}^n (\frac {{\partial Error}} {\partial {\boldsymbol{h}}^{[b]}})_{3i}
 \end{bmatrix} \end{gathered}\end{split}\]</div>
</section>
<section id="update-weights-and-biases">
<h2>7.5. Update weights and biases<a class="headerlink" href="#update-weights-and-biases" title="Permalink to this heading"></a></h2>
<p>As you remember, we initialized  the values of the <span class="math notranslate nohighlight">\(\boldsymbol{W^{[1]}}\)</span>,
<span class="math notranslate nohighlight">\({\vec{\boldsymbol{bias}}}^{[1]}\)</span>, <span class="math notranslate nohighlight">\({\vec{\boldsymbol{W}}}^{[2]}\)</span> and <span class="math notranslate nohighlight">\({bias}^{[2]}\)</span> by generating
random numbers. Now we should used <span class="math notranslate nohighlight">\(\textbf {Gradient Descent}\)</span> method to update these values to be used in the
next round of training our neural network in the Feedforward process.</p>
<p><span class="math notranslate nohighlight">\(\textbf {Gradient Descent}\)</span></p>
<div class="math notranslate nohighlight" id="eq-728">
<span id="equation-eq-728"></span><span class="eqno">(147)<a class="headerlink" href="#eq-728" title="Permalink to this equation"></a></span>\[ \begin{align}\begin{aligned} {\boldsymbol{W^{[1]}}}_{current} = {\boldsymbol{W^{[1]}}}_{previous}  - \lambda
 \frac {{\partial Error}}{\partial {\boldsymbol{W^{[1]}}}_{previous}}\\
 {\vec{\boldsymbol{bias}}}^{[1]}_{current} = {{\vec{\boldsymbol{bias}}}^{[1]}}_{previous}  - \lambda
 \frac {\partial Error}{\partial {{\vec{\boldsymbol{bias}}}^{[1]}}_{previous}}\\ {\vec{\boldsymbol{W}}}^{[2]}_{current} = {{\vec{\boldsymbol{W}}}^{[2]}}_{previous}  - \lambda
 \frac {\partial Error}{\partial {{\vec{\boldsymbol{W}}}^{[2]}}_{previous}}\\ {{bias}^{[2]}}_{current} = {{bias}^{[2]}}_{previous}  - \lambda
 \frac {\partial Error}{\partial {{bias}^{[2]}}_{previous}}\end{aligned}\end{align} \]</div>
<p>In the above equations, the <span class="math notranslate nohighlight">\(\lambda\)</span> is the <span class="math notranslate nohighlight">\(\textbf {Learning Rate}\)</span>. Meanwhile, the deduction of the
vectors and matrices should be done in an element-wise fashion.</p>
</section>
<section id="c-code-from-scratch">
<h2>7.6. C++ code from scratch<a class="headerlink" href="#c-code-from-scratch" title="Permalink to this heading"></a></h2>
<p>Now it is the time to implement the above algorithm in a computer code. The <strong>C++</strong> is chosen here due to its speed
which is an important parameter when it comes to dealing with a computational problem like training a neural network.</p>
<p>For the sake of simplicity, as described in the previous sections, we consider 4 input data each of which includes 2
dimensions. There is also one value associated to every single input data which are actual values that we want
to predict by training our neural network. Our neural network consists of 1 hidden layer with 3 neurons.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are many different libraries having the neural network implemented and ready to use in a bunch of lines
of code. Although, they all are advantageous to use in terms of saving time but implementation of the algorithm
from the scratch, gives much better and deeper understanding regarding how a neural network actually works.</p>
</div>
<p>The different blocks of the code will be explained as following:</p>
<p>The first step is including standard required headers:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;random&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cmath&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;fstream&gt;</span><span class="cp"></span>

<span class="n">using</span><span class="w"> </span><span class="n">namespace</span><span class="w"> </span><span class="n">std</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Then we should define our input data and assigned target values (That we want to finally predict):</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{{</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="w"> </span><span class="p">},{</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="p">},{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="w"> </span><span class="p">},{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="p">}};</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">target_values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>Next, we should define the architecture of the neural network including the number of data, number of neurons in the
hidden layer, number of dimensions that each input data has and the learning rate:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"></span>
<span class="kt">double</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>We should set up the engine for producing random numbers between 0 and 1 to fill the weights matrix/vector:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">random_device</span><span class="w"> </span><span class="n">rd</span><span class="p">;</span><span class="w"></span>
<span class="n">default_random_engine</span><span class="w"> </span><span class="nf">eng</span><span class="p">(</span><span class="n">rd</span><span class="p">());</span><span class="w"></span>
<span class="n">uniform_real_distribution</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">distr</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">};</span><span class="w"></span>
</pre></div>
</div>
<p>The number of the training iteration should be defined here:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">training_iteration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5000</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Now, we should define the required vectors and matrices in the Feedforward process:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">weights_1</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">weights_2</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">matrix_x</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias_2</span><span class="w"> </span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hidden_values</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hidden_values_activated</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hidden_values_activated_derivative</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_values</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_values_activated</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>There are also some vectors and matrices which are required in the Backpropagation that need to be defined:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">front_output_activated</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">behind_output_activated</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">Gradient_wrt_W_2</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Calculate_Grad_W2_Vector</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">BP_behind_hidden_activated</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">Gradient_wrt_W_1</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">BP_front_hidden_activated</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>We need to initialize the above vectors and matrices with values using <code class="code docutils literal notranslate"><span class="pre">Vectors_Matrices_Builder</span></code> function.
It should be noted that the weights vector/matrix should be filled with random numbers and the rest of the defined
items should be filled with 0 elements:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">Vectors_Matrices_Builder</span><span class="p">(){</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">     </span><span class="n">weights_2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">distr</span><span class="p">(</span><span class="n">eng</span><span class="p">));</span><span class="w"></span>
<span class="w">     </span><span class="n">weights_1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">       </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">           </span><span class="n">weights_1</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">distr</span><span class="p">(</span><span class="n">eng</span><span class="p">));</span><span class="w"></span>
<span class="w">       </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">     </span><span class="n">bias_1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">     </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">.</span><span class="w"></span>
<span class="w">   </span><span class="p">.</span><span class="w"></span>
<span class="w">   </span><span class="p">.</span><span class="w"></span>
<span class="w">   </span><span class="p">.</span><span class="w"></span>
</pre></div>
</div>
<p>We define a class <code class="code docutils literal notranslate"><span class="pre">Required_Functions</span></code> containing some required function including the Sigmoid function,
derivative of the Sigmoid function and the Prediction function so we can have access to these function in different
sections of out implementation:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="n">class</span><span class="w"> </span><span class="n">Required_Functions</span><span class="p">{</span><span class="w"></span>
<span class="w">  </span><span class="n">public</span><span class="o">:</span><span class="w"></span>

<span class="w">  </span><span class="kt">double</span><span class="w"> </span><span class="n">sigmoid</span><span class="w"> </span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="p">){</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w">  </span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="w">  </span><span class="kt">double</span><span class="w"> </span><span class="n">derivative_sigmoid</span><span class="w"> </span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="p">){</span><span class="w"></span>
<span class="w">        </span><span class="k">return</span><span class="w">  </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">));</span><span class="w"></span>
<span class="w">  </span><span class="p">}</span><span class="w"></span>

<span class="kt">double</span><span class="w"> </span><span class="n">predict</span><span class="w"> </span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Input_to_Predict</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">W1</span><span class="p">,</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">W2</span><span class="p">,</span><span class="w"> </span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">bias2</span><span class="p">){</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">W1X</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">W1X_activated</span><span class="p">;</span><span class="w"></span>

<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="p">.</span><span class="w"></span>
<span class="w"> </span><span class="p">.</span><span class="w"></span>
<span class="w"> </span><span class="p">.</span><span class="w"></span>
<span class="w"> </span><span class="p">.</span><span class="w"></span>
</pre></div>
</div>
<p>We need to define two more vectors to store the values of the errors and iteration of the training:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Training_Number</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Error_Values</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Here is the main body of the code where we initialize the vectors and matrices and form the X matrix from the input data:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(){</span><span class="w"></span>

<span class="n">Vectors_Matrices_Builder</span><span class="p">();</span><span class="w"></span>

<span class="w">   </span><span class="c1">// Forming the training set</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">matrix_x</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">         </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">            </span><span class="n">matrix_x</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span><span class="w">  </span><span class="c1">// dimension x number_of_data (2 x 4)</span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Here is the outermost loop where the first training starts:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">training_iteration</span><span class="w"> </span><span class="p">;</span><span class="w">  </span><span class="n">epoch</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
</pre></div>
</div>
<p>The Feedforward process starts by implementation of the 5 steps as mentioned before. The final step is to calculate
the predicted values:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="kt">double</span><span class="w"> </span><span class="n">result_output_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">weights_2</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">     </span><span class="n">result_output_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">weights_2</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_values_activated</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="n">r</span><span class="p">];</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">     </span><span class="n">result_output_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span><span class="w"></span>

<span class="w">     </span><span class="n">output_values</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result_output_layer</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">     </span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">result_output_layer</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"></span>

<span class="w">     </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>After predicting the target values, we should calculate the error:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">error</span><span class="w"> </span><span class="o">+=</span><span class="w">  </span><span class="n">pow</span><span class="p">((</span><span class="n">target_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="n">error</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mf">2.</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">number_of_data</span><span class="p">);</span><span class="w"></span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;In Training Number: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; -&gt;&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; The Error is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>

<span class="n">Error_Values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">error</span><span class="p">);</span><span class="w"></span>
<span class="n">Training_Number</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">epoch</span><span class="p">);</span><span class="w"></span>
</pre></div>
</div>
<p>Having the Feedforward process done, we should start the Backpropagation by implementing the 5 steps as
mentioned before:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">t</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">    </span><span class="n">front_output_activated</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mf">-1.</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">number_of_data</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">target_values</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">t</span><span class="p">]);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">w</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">    </span><span class="n">behind_output_activated</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">front_output_activated</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">derivative_sigmoid</span><span class="p">(</span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">w</span><span class="p">]);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
</pre></div>
</div>
<p>We need to update the weights and biases using <strong>Gradient Descent</strong> approach at the end of the Backpropagation:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="c1">////////////////////////// UPDATE W2 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="n">weights_2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights_2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Calculate_Grad_W2_Vector</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">////////////////////////// UPDATE Bias2 ////////////////////////////////////</span>

<span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">new_bias2</span><span class="p">;</span><span class="w"></span>

<span class="c1">////////////////////////// UPDATE W1 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="n">weights_1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights_1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">];</span><span class="w"></span>

<span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">////////////////////////// UPDATE Bias1 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="n">bias_1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>Here we save the results in a text file including 2 columns corresponding to the number of the iteration in training of the
neural network and the calculated error at that iteration respectively:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">string</span><span class="w"> </span><span class="nf">filename</span><span class="p">(</span><span class="s">&quot;Results-CPP.txt&quot;</span><span class="p">);</span><span class="w"></span>
<span class="n">fstream</span><span class="w"> </span><span class="n">file_out</span><span class="p">;</span><span class="w"></span>
<span class="n">file_out</span><span class="p">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">out</span><span class="p">);</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">training_iteration</span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">    </span><span class="n">file_out</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">Training_Number</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;       &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">Error_Values</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="n">file_out</span><span class="p">.</span><span class="n">close</span><span class="p">();</span><span class="w"></span>
</pre></div>
</div>
<p>After finishing the training procedure, at the end, we can print out the predicted value for an input data to assess
the performance of out neural network:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Predict_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="kt">double</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">predict</span><span class="p">(</span><span class="n">Predict_value</span><span class="p">,</span><span class="w"> </span><span class="n">weights_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">weights_2</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The predicted Value is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>The output of the above code will be:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.132078</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.130562</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.129356</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.128402</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.127652</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4997</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.000122601</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4998</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.000122501</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4999</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.000122401</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">5000</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.000122301</span><span class="w"></span>

<span class="n">The</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.984482</span><span class="w"></span>
</pre></div>
</div>
<p>According to the above output, it is clear that the error starting at around 0.13 in the first iteration, gradually
decreases to nearly 0 at the last training iteration. The neural network does a good job in predicting the input value
where we get a number fairly close to 1 which is what we expect for this input number.</p>
<p>If we change the input value to predict as following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Predict_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="kt">double</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">predict</span><span class="p">(</span><span class="n">Predict_value</span><span class="p">,</span><span class="w"> </span><span class="n">weights_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">weights_2</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The predicted Value is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>The output of the code will be like this:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">,</span><span class="w"></span>
<span class="p">,</span><span class="w"></span>
<span class="p">,</span><span class="w"></span>
<span class="p">,</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4997</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">8.65583e-05</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4998</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">8.64981e-05</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4999</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">8.64379e-05</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">5000</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">8.63779e-05</span><span class="w"></span>

<span class="n">The</span><span class="w"> </span><span class="n">predicted</span><span class="w"> </span><span class="n">Value</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.00975805</span><span class="w"></span>
</pre></div>
</div>
<p>Again, we get a value small enough and close to 0 which is what we expected for this input value.</p>
<p>We can, investigate the performance of the neural network based on the number of the neurons in the hidden layer:</p>
<figure class="align-center" id="id4">
<img alt="../_images/NN4.png" src="../_images/NN4.png" />
<figcaption>
<p><span class="caption-number">Figure.  23 </span><span class="caption-text">Performance of the neural network based on the number of neurons</span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>We can clearly see that increasing the neurons inside the hidden layer boosts the accuracy of the model by reducing
the error. However, it should be noted that it will come at the cost of increasing the computational cost.</p>
<p>The complete C++ code is presented as following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;random&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;iostream&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cmath&gt;</span><span class="cp"></span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;fstream&gt;</span><span class="cp"></span>

<span class="n">using</span><span class="w"> </span><span class="n">namespace</span><span class="w"> </span><span class="n">std</span><span class="p">;</span><span class="w"></span>


<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{{</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="w"> </span><span class="p">},{</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="p">},{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="w"> </span><span class="p">},{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="p">}};</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">target_values</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">};</span><span class="w"></span>

<span class="kt">int</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">4</span><span class="p">;</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">;</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">3</span><span class="p">;</span><span class="w"></span>
<span class="c1">// Defining learning rate</span>
<span class="kt">double</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"></span>

<span class="n">random_device</span><span class="w"> </span><span class="n">rd</span><span class="p">;</span><span class="w"></span>
<span class="n">default_random_engine</span><span class="w"> </span><span class="nf">eng</span><span class="p">(</span><span class="n">rd</span><span class="p">());</span><span class="w"></span>
<span class="n">uniform_real_distribution</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">distr</span><span class="p">{</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">};</span><span class="w"></span>


<span class="kt">int</span><span class="w"> </span><span class="n">training_iteration</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5000</span><span class="p">;</span><span class="w"></span>

<span class="c1">///////////////// Required Vectors/Matrices for Feed Forward /////////////////</span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">weights_1</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">weights_2</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">matrix_x</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias_2</span><span class="w"> </span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hidden_values</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hidden_values_activated</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">hidden_values_activated_derivative</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_values</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_values_activated</span><span class="p">;</span><span class="w"></span>

<span class="c1">///////////////// Required Vectors/Matrices for Back Propagation /////////////////</span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">front_output_activated</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">behind_output_activated</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">Gradient_wrt_W_2</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Calculate_Grad_W2_Vector</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">BP_behind_hidden_activated</span><span class="p">;</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">Gradient_wrt_W_1</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">BP_front_hidden_activated</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">;</span><span class="w"></span>

<span class="c1">////////////////////////////////////////////////////////////////////</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">Vectors_Matrices_Builder</span><span class="p">(){</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">      </span><span class="n">weights_2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">distr</span><span class="p">(</span><span class="n">eng</span><span class="p">));</span><span class="w"></span>
<span class="w">      </span><span class="n">weights_1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">            </span><span class="n">weights_1</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">distr</span><span class="p">(</span><span class="n">eng</span><span class="p">));</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">bias_1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="n">bias_2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">output_values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="n">output_values_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>

<span class="w">      </span><span class="n">front_output_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="n">behind_output_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">hidden_values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">      </span><span class="n">hidden_values_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">      </span><span class="n">hidden_values_activated_derivative</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">      </span><span class="n">BP_front_hidden_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">      </span><span class="n">BP_behind_hidden_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">            </span><span class="n">hidden_values</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">hidden_values_activated</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">BP_front_hidden_activated</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">hidden_values_activated_derivative</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">            </span><span class="n">BP_behind_hidden_activated</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">Gradient_wrt_W_2</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">            </span><span class="n">Gradient_wrt_W_2</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">            </span><span class="n">Calculate_Grad_W2_Vector</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>


<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">Gradient_wrt_W_1</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">            </span><span class="n">Gradient_wrt_W_1</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">            </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">/////////////////////// Defines required function including sigmoid, derivative of the sigmoid and prediction ///////////////////////////////////////////////</span>

<span class="n">class</span><span class="w"> </span><span class="n">Required_Functions</span><span class="p">{</span><span class="w"></span>
<span class="w">   </span><span class="n">public</span><span class="o">:</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">sigmoid</span><span class="w"> </span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="p">){</span><span class="w"></span>
<span class="w">         </span><span class="k">return</span><span class="w">  </span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">));</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">derivative_sigmoid</span><span class="w"> </span><span class="p">(</span><span class="kt">double</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="p">){</span><span class="w"></span>
<span class="w">         </span><span class="k">return</span><span class="w">  </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">));</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="kt">double</span><span class="w"> </span><span class="n">predict</span><span class="w"> </span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Input_to_Predict</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">W1</span><span class="p">,</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">W2</span><span class="p">,</span><span class="w"> </span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">bias1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="kt">double</span><span class="w"> </span><span class="n">bias2</span><span class="p">){</span><span class="w"></span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">W1X</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">W1X_activated</span><span class="p">;</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">W1</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Input_to_Predict</span><span class="p">[</span><span class="n">k</span><span class="p">];</span><span class="w">  </span><span class="c1">// 3x1 Output</span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="c1">//derivatives_matrix_y_predict[m] = derivative_sigmoid(result);</span>

<span class="w">      </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">bias1</span><span class="p">[</span><span class="n">m</span><span class="p">];</span><span class="w"></span>

<span class="w">      </span><span class="n">W1X</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">result_hidden_layer</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="n">W1X_activated</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">result_hidden_layer</span><span class="p">))</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">      </span><span class="n">result_output</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">W2</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">W1X_activated</span><span class="p">[</span><span class="n">m</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">      </span><span class="n">result_output</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">bias2</span><span class="p">;</span><span class="w"></span>

<span class="w">   </span><span class="k">return</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">result_output</span><span class="p">);</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="p">};</span><span class="w"></span>

<span class="n">Required_Functions</span><span class="w"> </span><span class="n">Start</span><span class="p">;</span><span class="w"></span>
<span class="n">Required_Functions</span><span class="w"> </span><span class="o">*</span><span class="n">Function</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">&amp;</span><span class="n">Start</span><span class="p">;</span><span class="w"></span>

<span class="c1">///////////////////////////////////////////////////</span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Training_Number</span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Error_Values</span><span class="p">;</span><span class="w"></span>
<span class="c1">////////////////////////////////////////////////////</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(){</span><span class="w"></span>

<span class="n">Vectors_Matrices_Builder</span><span class="p">();</span><span class="w"></span>

<span class="w">   </span><span class="c1">// Forming the training set</span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">matrix_x</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>

<span class="w">         </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">            </span><span class="n">matrix_x</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">input_data</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]);</span><span class="w">  </span><span class="c1">// dimension x number_of_data (2 x 4)</span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// Training Begins! ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">training_iteration</span><span class="w"> </span><span class="p">;</span><span class="w">  </span><span class="n">epoch</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="c1">/////////////////////////// Feed Forwards //////////////////////////</span>

<span class="c1">/////////////////////////// W_1 * X //////////////////////////////</span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">weights_1</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">matrix_x</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">n</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="w">      </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">bias_1</span><span class="p">[</span><span class="n">m</span><span class="p">];</span><span class="w"></span>

<span class="w">      </span><span class="n">hidden_values</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">]</span><span class="o">=</span><span class="w"> </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="n">hidden_values_activated</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">]</span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">result_hidden_layer</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="n">result_hidden_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// W_2 * SIGMOID (W_1 * X) ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_output_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">weights_2</span><span class="p">.</span><span class="n">size</span><span class="p">()</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">result_output_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">weights_2</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_values_activated</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="n">r</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="w">      </span><span class="n">result_output_layer</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">];</span><span class="w"></span>

<span class="w">      </span><span class="n">output_values</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result_output_layer</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">result_output_layer</span><span class="p">)</span><span class="w"> </span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// Error Calculation //////////////////////////</span>

<span class="kt">double</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">error</span><span class="w"> </span><span class="o">+=</span><span class="w">  </span><span class="n">pow</span><span class="p">((</span><span class="n">target_values</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">k</span><span class="p">]),</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="n">error</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mf">2.</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">number_of_data</span><span class="p">);</span><span class="w"></span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;In Training Number: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; -&gt;&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; The Error is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>

<span class="n">Error_Values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">error</span><span class="p">);</span><span class="w"></span>
<span class="n">Training_Number</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">);</span><span class="w"></span>

<span class="c1">/////////////////////////// Back Propagation //////////////////////////</span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">t</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">      </span><span class="n">front_output_activated</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mf">-1.</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">number_of_data</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">target_values</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">t</span><span class="p">]);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">w</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">w</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">      </span><span class="n">behind_output_activated</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">front_output_activated</span><span class="p">[</span><span class="n">w</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">derivative_sigmoid</span><span class="p">(</span><span class="n">output_values_activated</span><span class="p">[</span><span class="n">w</span><span class="p">]);</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// Form Grad W2 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">Gradient_wrt_W_2</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hidden_values_activated</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="n">r</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">/////////////////////////// Calculation Grad(W2)) ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">result_1</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">behind_output_activated</span><span class="p">[</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Gradient_wrt_W_2</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="n">r</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="w">      </span><span class="n">Calculate_Grad_W2_Vector</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">result_1</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="c1">/////////////////////////// Calculation Grad(bias2)) ////////////////////////////////////</span>

<span class="kt">double</span><span class="w"> </span><span class="n">new_bias2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>
<span class="w">      </span><span class="n">new_bias2</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">behind_output_activated</span><span class="p">[</span><span class="n">r</span><span class="p">];</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// Moving in front of hidden_activated ///////////////////////////</span>

<span class="c1">/////////////////////////// W2 * behind_output_activated ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">BP_front_hidden_activated</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights_2</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">behind_output_activated</span><span class="p">[</span><span class="n">p</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// Derivative of hidden values activated ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">hidden_values_activated_derivative</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">derivative_sigmoid</span><span class="p">(</span><span class="n">hidden_values</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">]);</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">/////////////////////////// Moving behind hidden_activated ///////////////////////////</span>

<span class="c1">///////////////// (W2 * behind_output_activated) * (Derivative of hidden values activated) ////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">BP_behind_hidden_activated</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="n">BP_front_hidden_activated</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">hidden_values_activated_derivative</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="p">}</span><span class="w"></span>

<span class="c1">////////////////////////// Form Grad W1 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">p</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">Gradient_wrt_W_1</span><span class="p">[</span><span class="n">p</span><span class="p">][</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">matrix_x</span><span class="p">[</span><span class="n">r</span><span class="p">][</span><span class="n">p</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">/////////////////////////// Calculation Grad(W1)) ////////////////////////////////////</span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">m</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="kt">double</span><span class="w"> </span><span class="n">result_4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">n</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">k</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">result_4</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BP_behind_hidden_activated</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">k</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Gradient_wrt_W_1</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">n</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="w">      </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">n</span><span class="p">]</span><span class="o">=</span><span class="w"> </span><span class="n">result_4</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">      </span><span class="n">result_4</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="p">}</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
<span class="c1">/////////////////////////// Calculation Grad(bias1)) ////////////////////////////////////</span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="kt">double</span><span class="w"> </span><span class="n">new_bias1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">         </span><span class="n">new_bias1</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">BP_behind_hidden_activated</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">];</span><span class="w"></span>
<span class="w">      </span><span class="p">}</span><span class="w"></span>

<span class="w">      </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_bias1</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>

<span class="c1">////////////////////////// Gradient Descent ////////////////////////////////////</span>
<span class="c1">////////////////////////// UPDATE W2 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="n">weights_2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights_2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Calculate_Grad_W2_Vector</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">////////////////////////// UPDATE Bias2 ////////////////////////////////////</span>

<span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">new_bias2</span><span class="p">;</span><span class="w"></span>

<span class="c1">////////////////////////// UPDATE W1 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="n">weights_1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights_1</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Calculate_Grad_W1_Matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">];</span><span class="w"></span>

<span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="c1">////////////////////////// UPDATE Bias1 ////////////////////////////////////</span>

<span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">hidden_layer_nodes</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="n">bias_1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">lr</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">Calculate_Grad_Bias1_Vector</span><span class="p">[</span><span class="n">i</span><span class="p">];</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>
<span class="p">}</span><span class="w"></span>

<span class="c1">////////////////////////// Saving in a text file ////////////////////////////////////</span>

<span class="w">   </span><span class="n">string</span><span class="w"> </span><span class="n">filename</span><span class="p">(</span><span class="s">&quot;Results-CPP.txt&quot;</span><span class="p">);</span><span class="w"></span>
<span class="w">   </span><span class="n">fstream</span><span class="w"> </span><span class="n">file_out</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="n">file_out</span><span class="p">.</span><span class="n">open</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">ios</span><span class="o">::</span><span class="n">out</span><span class="p">);</span><span class="w"></span>

<span class="w">   </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">training_iteration</span><span class="p">;</span><span class="w"> </span><span class="n">r</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">      </span><span class="n">file_out</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">Training_Number</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;       &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">Error_Values</span><span class="p">[</span><span class="n">r</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="p">};</span><span class="w"></span>

<span class="w">   </span><span class="n">file_out</span><span class="p">.</span><span class="n">close</span><span class="p">();</span><span class="w"></span>
<span class="c1">////////////////////////// Prediction ////////////////////////////////////</span>

<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Predict_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="w"> </span><span class="p">};</span><span class="w"></span>
<span class="kt">double</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">predict</span><span class="p">(</span><span class="n">Predict_value</span><span class="p">,</span><span class="w"> </span><span class="n">weights_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">weights_2</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The predicted Value is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="w">   </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="classification-of-an-eye-disease">
<h2>7.7. Classification of an eye disease<a class="headerlink" href="#classification-of-an-eye-disease" title="Permalink to this heading"></a></h2>
<p>Now we can test our neural network for prediction of an eye disease where the posterior of the eye globe becomes
flattened. This eye disorder is called <strong>IIH</strong> and more details are available in Chapter 6
(Please check out <strong>Section 6.3</strong>). We have information of 661 eye globes in which 301 of them have been
identified as flattened eye globes. In other words, those are the eyes having IIH syndrome. The rest of the data, are
healthy eyes where the curvature of the eye globe is normal. We have saved the data in a text file (<strong>EYE-DATA.txt</strong>)
as following :</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.33325</span><span class="p">,</span><span class="mi">1</span><span class="w"></span>
<span class="mf">0.05</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mf">0.6665</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="w"></span>
<span class="mf">0.05</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.6665</span><span class="p">,</span><span class="mi">1</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="mi">1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.33325</span><span class="p">,</span><span class="mi">0</span><span class="w"></span>
<span class="mi">1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.33325</span><span class="p">,</span><span class="mi">0</span><span class="w"></span>
<span class="mi">1</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.04</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">0</span><span class="w"></span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">EYE-DATA.txt</span></code> file is available inside the <strong>Neural-Network</strong> repository.</p>
</div>
<p>In the above data, each line corresponds to the information of a single eye. There are 10 numbers in each line
separated with <code class="code docutils literal notranslate"><span class="pre">,</span></code>. The first 7 numbers are the normalized stiffness of the 7 tissues within the
optic nerve head, the <span class="math notranslate nohighlight">\(8^{th}\)</span> and <span class="math notranslate nohighlight">\(9^{th}\)</span> are the normalized <strong>Intraocular Pressure (IOP)</strong> and
<strong>Intracranial Pressure (ICP)</strong> respectively. With that being said, every single input data has 9 dimensions.
Finally, the <span class="math notranslate nohighlight">\(10^{th}\)</span> is a number which is either 0 or 1.
The 0 means that the eye does not have IIH while if it is 1, it indicates that this is a flattened eye globe.</p>
<p>We use, these data to train our neural network. Finally, the purpose is giving a new input to our neural network
and predict if this eye globe with these specific properties is flattened or not.</p>
<p>The main body of the code for training the neural network code is almost the same as presented
before in <strong>Section 7.6</strong>. However there are some modifications. First of all, we need to update
the <code class="code docutils literal notranslate"><span class="pre">number_of_data</span></code> and <code class="code docutils literal notranslate"><span class="pre">dimensions</span></code>:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">661</span><span class="p">;</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">9</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>In addition, we require to read the data from the text file that could be done as following:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">input_data</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">target_values</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="kt">int</span><span class="w"> </span><span class="n">main</span><span class="p">(){</span><span class="w"></span>

<span class="c1">//////////////////////////////////////////////////////</span>
<span class="n">vector</span><span class="w"> </span><span class="o">&lt;</span><span class="n">vector</span><span class="w"> </span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">data</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="n">ifstream</span><span class="w"> </span><span class="nf">infile</span><span class="p">(</span><span class="w"> </span><span class="s">&quot;EYE-DATA.txt&quot;</span><span class="w"> </span><span class="p">);</span><span class="w"></span>
<span class="w"> </span><span class="n">string</span><span class="w"> </span><span class="n">line</span><span class="p">;</span><span class="w"></span>
<span class="w"> </span><span class="n">string</span><span class="w"> </span><span class="n">str</span><span class="p">;</span><span class="w"></span>

<span class="w"> </span><span class="c1">//  Read the file</span>
<span class="w"> </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">getline</span><span class="p">(</span><span class="n">infile</span><span class="p">,</span><span class="w"> </span><span class="n">line</span><span class="p">))</span><span class="w"></span>
<span class="w"> </span><span class="p">{</span><span class="w">   </span><span class="n">istringstream</span><span class="w"> </span><span class="nf">ss</span><span class="w"> </span><span class="p">(</span><span class="n">line</span><span class="p">);</span><span class="w"></span>
<span class="w">     </span><span class="n">vector</span><span class="w"> </span><span class="o">&lt;</span><span class="n">string</span><span class="o">&gt;</span><span class="w"> </span><span class="n">record</span><span class="p">;</span><span class="w"></span>

<span class="w">     </span><span class="k">while</span><span class="w"> </span><span class="p">(</span><span class="n">getline</span><span class="p">(</span><span class="n">ss</span><span class="p">,</span><span class="w"> </span><span class="n">str</span><span class="p">,</span><span class="w"> </span><span class="sc">&#39;,&#39;</span><span class="p">))</span><span class="w"></span>
<span class="w">         </span><span class="n">record</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">str</span><span class="p">);</span><span class="w"></span>
<span class="w">     </span><span class="n">data</span><span class="p">.</span><span class="n">push_back</span><span class="w"> </span><span class="p">(</span><span class="n">record</span><span class="p">);</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>

<span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">number_of_data</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">   </span><span class="n">input_data</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">());</span><span class="w"></span>
<span class="w">   </span><span class="n">target_values</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">stod</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">9</span><span class="p">]));</span><span class="w"></span>

<span class="w">     </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w">  </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">dimensions</span><span class="w"> </span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">){</span><span class="w"></span>

<span class="w">         </span><span class="n">input_data</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">push_back</span><span class="p">(</span><span class="n">stod</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]));</span><span class="w"></span>

<span class="w">     </span><span class="p">}</span><span class="w"></span>
<span class="w"> </span><span class="p">}</span><span class="w"></span>
</pre></div>
</div>
<p>This prediction is performed at the end of training using this block of code:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Predict_value1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mf">0.05</span><span class="p">,</span><span class="mf">0.25</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.33325</span><span class="p">};</span><span class="w"></span>
<span class="n">vector</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="w"> </span><span class="n">Predict_value2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.11</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.03</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mf">0.22</span><span class="p">,</span><span class="mf">0.3333333333</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.11</span><span class="p">};</span><span class="w"></span>

<span class="kt">double</span><span class="w"> </span><span class="n">prediction1</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">predict</span><span class="p">(</span><span class="n">Predict_value1</span><span class="p">,</span><span class="w"> </span><span class="n">weights_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">weights_2</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>
<span class="kt">double</span><span class="w"> </span><span class="n">prediction2</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Function</span><span class="o">-&gt;</span><span class="n">predict</span><span class="p">(</span><span class="n">Predict_value2</span><span class="p">,</span><span class="w"> </span><span class="n">weights_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">weights_2</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_1</span><span class="w"> </span><span class="p">,</span><span class="w"> </span><span class="n">bias_2</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span><span class="w"></span>

<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="w"> </span><span class="p">;</span><span class="w"></span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The prediction for the first eye is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">prediction1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;The prediction for the second eye is: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">prediction2</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span><span class="w"></span>
</pre></div>
</div>
<p>Please note that, the first two lines, are the information of two new eyes that <strong>did NOT exist in the
original data set</strong>.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="p">.</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4997</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">6.3865e-06</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4998</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">6.38371e-06</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">4999</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">6.38093e-06</span><span class="w"></span>
<span class="n">In</span><span class="w"> </span><span class="n">Training</span><span class="w"> </span><span class="n">Number</span><span class="o">:</span><span class="w"> </span><span class="mi">5000</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">Error</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">6.37814e-06</span><span class="w"></span>

<span class="n">The</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">first</span><span class="w"> </span><span class="n">eye</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.997147</span><span class="w"></span>
<span class="n">The</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">second</span><span class="w"> </span><span class="n">eye</span><span class="w"> </span><span class="n">is</span><span class="o">:</span><span class="w"> </span><span class="mf">0.00195323</span><span class="w"></span>
</pre></div>
</div>
<p>The above results indicate that the first eye was identified as a flattened globe and the second eye, was classified
as an healthy eye. These predictions are correct as after performing a FEM analysis, it turned out that the first eye
based the given properties is deformed in a way that the globe becomes flattened while the second eye keeps its normal
shape indicating an healthy eye.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The full C++ code is available inside the <strong>Neural-Network</strong> repository (<strong>IIH_Prediction_Neural_Network.cpp</strong>)</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="LDA.html" class="btn btn-neutral float-left" title="6. Machine Learning in Diagnosis" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="contact.html" class="btn btn-neutral float-right" title="About the Author" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Jafar Arash Mehr.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>