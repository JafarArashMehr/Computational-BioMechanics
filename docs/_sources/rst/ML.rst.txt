

**7. Fundamentals of Machine Learning**
================================================

.. note:: 

    All of required CSV files are available in the folder: **Machine-Learning** 

Machine learning (ML) is a computational tool to discover patterns in data and make informed predictions.
There are two main types of ML including: 


:math:`\bullet` **1. Supervised ML:** This technique uses labeled dataset to train algorithms to classify or predict something

:math:`\bullet` **2. Unsupervised ML:** This technique uses algorithms to analyze and cluster unlabeled dataset


Two types of machine learning are linear regression and decision tree regression. The linear regression is used for continuous variable while the decision tree is used for both continuous and categorical variables. 

7.1. Feature Engineering
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


The process of using practical, statistical and data science knowledge to select, transform or extract properties from raw data. 


**1. Feature Selection**

Feature selection is the process of picking variables from a dataset that will be used as predictor variables for your model. With very large datasets, there are dozens if not hundreds of features for each observation in the data. Using all of the features in a dataset often doesn’t give any performance boost. In fact, it may actually hurt performance by adding complexity and noise to the model. Therefore, choosing the features to use for the model is an important part of the model development process. 

Generally, there are three types of features:

1. Predictive: Features that by themselves contain information useful to predict the target                       

2. Interactive: Features that are not useful by themselves to predict the target variable, but become predictive in conjunction with other features

3. Irrelevant: Features that don’t contain any useful information to predict the target


You want predictive features, but a predictive feature can also be a redundant feature. Redundant features are highly correlated with other features and therefore do not provide the model with any new information—for example, the steps you took in a day, may be highly correlated with the calories you burned. The goal of feature selection is to find the predictive and interactive features and exclude redundant and irrelevant features.



**2. Feature Transformation**


Feature transformation is a process where you take features that already exist in the dataset, and alter them so that they’re better suited to be used for training the model. Some of the transformation techniques include: 


:math:`\bullet` **Normalization**

Normalization (e.g., MinMaxScaler in scikit-learn) transforms data to reassign each value to fall within the range [0, 1]. When applied to a feature, the feature’s minimum value becomes zero and its maximum value becomes one. All other values scale to somewhere between them. The formula for this transformation is:


.. math:: 
  :name: eq.120

   x_{i,normalized}= \frac {x_i-x_{min}}{x_{max}-x_{min}}


Features with higher magnitudes of scale will be more influential in some machine learning algorithms, like K-means, where  Euclidean distances between data points are calculated with the absolute value of the features (so large feature values have major effects, compared to small feature values). By min-max scaling (normalizing) each feature, they are both reduced to the same range

:math:`\bullet` **Standardization**


Another type of scaling is called standardization (e.g., StandardScaler in scikit-learn). Standardization transforms each value within a feature so they collectively have a mean of zero and a standard deviation of one. To do this, for each value, subtract the mean of the feature and divide by the feature’s standard deviation:



.. math:: 
  :name: eq.121

   x_{i,standardized}= \frac {x_{i}-x_{mean}}{x_{standard.dev}}


This method is useful because it centers the feature’s values on zero, which is useful for some machine learning algorithms. It also preserves outliers, since it does not place a hard cap on the range of possible values. 


:math:`\bullet` **Encoding**

Another form of feature transformation is known as encoding. Variable encoding is the process of converting categorical data to numerical data. Consider the bank churn dataset. The original data has a feature called “Geography”, whose values represent each customer’s country of residence—France, Germany, or Spain. Most machine learning methodologies cannot extract meaning from strings. Encoding transforms the strings to numbers that can be interpreted mathematically. the feature would typically be encoded into binary. This process requires that a column be added to represent each possible class contained within the feature. 

In this example, if the geography is France, we put 1 under France column and zero under Germany and Spain columns. Tools commonly used to do this include **pandas.get_dummies()** and **OneHotEncoder()**.  Often methods drop one of the columns to avoid having redundant information in the dataset . 



.. note::

   Keep in mind that some features may be inferred to be numerical by Python of other frameworks but still represent a category. For example, suppose you had a dataset with people assigned to different arbitrary groups: 1, 2, and 3:

   A different kind of encoding can be used for features that contain discrete or ordinal values. This is called ordinal encoding. It is used when the values do contain inherent order or ranking. For instance, consider a “Temperature” column that has values of cold, warm, and hot. In this case, **ordinal encoding** could reassign these classes to 0, 1, and 2. 



**3. Feature Extraction**


Feature extraction involves producing new features from existing ones, with the goal of having features that deliver more predictive power to your model. While there is some overlap between extraction and transformation colloquially, the main difference is that a new feature is created from one or more other features rather than simply changing one that already exists.

Feature extraction involves combining existing columns meaningfully to construct new features that would help improve prediction.


.. note::

   It is important to check for class balance in a dataset, particularly in the context of feature engineering and predictive modeling. If the target column in a dataset has more than 90% of its values belonging to one class, it is recommended to redistribute the data; otherwise, once a model is trained on the imbalanced data and predictions are made, the predictions may be biased.



We need to understand more about what class imbalance is, when it becomes problematic, and some issues that can arise if it isn’t addressed. We need to learn two of the general categories for balancing datasets, upsampling, and downsampling. 


7.2. Imbalanced Datasets
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


We need to understand more about what class imbalance is, when it becomes problematic, and some issues that can arise if it isn’t addressed. We need to learn two of the general categories for balancing datasets, upsampling, and downsampling. 


For categorical variables, the different possible values that each can take are known as classes. This is true for both predictor variables and target variables. If you were trying to classify the weather on a given day as rainy or sunny, these would be considered two classes. The number of classes is equal to the number of unique values in the variable.

The number of occurrences of each class in the target variable is known as the **class distribution**. When predicting a categorical target, problems can arise when the class distribution is highly imbalanced. If there are not enough instances of certain outcomes, the resulting model might not be very good at predicting that class.

This is where the process of class balancing comes in, a process that allows you to manipulate the dataset, or the model fitting process, in a way that the class imbalance that exists doesn’t affect the performance of the resulting model. In general, in an unbalanced dataset,  at least one of the classes in the target variable occurs much less frequently than another. 

For example if we want to create a model that will classify emails that are sent to the company either as “spam” or “not spam.” The company receives thousands and thousands of emails daily, not to mention all the emails they’ve received in the past. However, the number of examples of spam is very small. For the sake of this example, say that 10 emails per day are identified as spam manually. This doesn't have the makings of a very good model. With so few examples of spam relative to examples of not spam, the model can have difficulty detecting the minority class, resulting in its being biased toward the majority class or possibly never predicting the minority class at all. 


Class balancing refers to the process of changing the data by altering the number of samples in order to make the ratios of classes in the target variable less asymmetrical.There are two general strategies to balance a dataset, and the method that is better to use generally is decided by how much data you have in the first place:


**1. Downsampling**


Downsampling is the process of making the minority class represent a larger share of the whole dataset simply by removing observations from the majority class. It is mostly used with datasets that are large. But how large is large enough to consider downsampling? Tens of thousands is a good rule of thumb, but ultimately this needs to be validated by checking that model performance doesn’t deteriorate as you train with less data. 


One way to downsample data is by selecting some observations randomly from the majority class and removing them from the dataset. There are some more technical, mathematically based methods, but random removal works very well in most cases.


**2. Upsampling**

Upsampling is basically the opposite of downsampling, and is done when the dataset doesn’t have a very large number of observations in the first place. Instead of removing observations from the majority class, you increase the number of observations in the minority class. 

There are a couple of ways to go about this. The first and easiest method is to duplicate samples of the minority class. Depending on how many such observations you have compared to the majority class, you might have to duplicate each sample several times over.

Another way is to create synthetic, unique observations of the minority class. On the surface, there seems to be something wrong about editing the dataset like this, but if the goal is simply to train a better-performing model, it can be a valid and useful technique. You can generate these synthetic observations from the observations that currently exist. For example, you can average two points of the minority class and add the result to the dataset as a sample of the minority class. This can even be done algorithmically using publicly available Python packages.


**How to implement it**


In both cases, upsampling and downsampling, it is important to leave a partition of test data that is unaltered by the sampling adjustment. You do this because you need to understand how well your model predicts on the actual class distribution observed in the world that your data represents. In the case of the spam detector example, it’s great if your model can score well on resampled data that is 80% not spam and 20% spam, but you need to know how it will work when deployed in the real world, where spam emails are much less frequent. This is why the test holdout data is not rebalanced.


**Consequences**


Manipulating the class distribution of your data doesn’t come without consequences. The first consequence is the risk of your model predicting the minority class more than it should. By class rebalancing to get your model to recognize the minority class, you might build a model that over-recognizes that class. That happens because, in training, it learned a data distribution that is not what it will in practice in the real world.

Changing the class distribution affects the underlying class probabilities learned by the model. Consider, for example, how the **Naive Bayes** algorithm works. To calculate the probability of a class, given the features, it uses the background probability of a class in the data.


7.3. Naive Bayes classifiers
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is a supervised classification technique based on Baye's theorem with an assumption of independence among predictor.When it comes to supervised machine learning techniques, Naive Bayes is a perfect example of that. Naive Bayes models remain relevant because they are simple, fast, and good predictors. In certain situations, Naive Bayes is also known to outperform much more advanced classification methods. Even if a more advanced model is required, producing a Naive Bayes model can also be a great starting point. Therefore, the Naive Bayes classifier is something that every data professional needs in their machine learning skill set.


A Naive Bayes model is a supervised learning technique used for classification problems. As with all supervised learning techniques, to create a Naive Bayes model you must have a response variable and a set of predictor variables to train the model. The Naive Bayes algorithm is based on Bayes’ Theorem, an equation that can be used to calculate the probability of an outcome or class, given the values of predictor variables. This value is known as the posterior probability. That probability is calculated using three values: 




:math:`\bullet` The probability of the outcome overall P(A)

:math:`\bullet` The probability of the value of the predictor variable P(B)

:math:`\bullet` The conditional probability P(B|A) (Note: P(B|A) is interpreted as the probability of B, given A.)


The probability of the outcome overall, P(A), is multiplied by the conditional probability, P(B|A). This result is then divided by the probability of the predictor variable, P(B), to obtain the posterior probability. 


**Bayes’ Theorem:**


.. math:: 
  :name: eq.213

   P(A|B)= \frac{P(B|A).P(A)}{P(B)}


The goal of Bayes’ Theorem is to find the probability of an event, A, given that another event B is true. In the context of a predictive model, the class label would be A and the predictor variable would be B. P(A) is considered the prior probability of event A before any evidence (feature) is seen. Then, P(A|B) is the posterior probability, or the probability of the class label after the evidence (feature) has been seen. 


In a predictive model, this calculation is carried out for each feature, for each class. Then the probabilities are multiplied together. The class with the highest resulting product is the model’s final prediction for that sample. These models make a number of assumptions about the data to work properly. One of the most important is the assumption that each predictor variable (different Bs in the formula) is independent from the others, conditional on the class. This is called conditional independence. Variables B and C are independent of one another on the condition that a third variable, A, exists such that:


.. math:: 
  :name: eq.214

   P(B|C,A)= P(B,A)


This equation can be interpreted as “the probability of B, given C and A, is equal to the probability of B, given A.” In other words, given A, introducing C does not change the probability of B. Note that two features can only be considered conditionally independent of each other when considered in relation to a third variable. Furthermore, it’s possible for variables B and C to be conditionally independent of one another with respect to A, but not with respect to another variable, say, Z. 


In Naive Bayes, the predictor variables (B and C in the equation above) are assumed to be conditionally independent of each other, given the target variable (A). This is an assumption that very often is not actually true. However, Naive Bayes models still often perform well in spite of the data violating the assumption. The assumption is made to simplify the model. Otherwise, long probabilistic chains would have to be calculated to determine the probability of a feature’s value with respect to the values of every other variable. 


Another assumption of the data is that no predictor variable has any more predictive power than any other predictor. In other words, the individual predictor variables are assumed to contribute equally to the model’s prediction. Like the assumption of class-conditional independence between the features, this assumption is also often violated by real-world data, but Naive Bayes still often proves a good model in spite of this. 


**Pros**

:math:`\bullet` Of all the classification algorithms that are still used today, Naive Bayes is one of the simplest. However, it is still able to produce valuable results. Its simplicity comes as an asset because it is one of the most straightforward algorithms to implement. In spite of their assumptions, Naive Bayes classifiers work quite well in many industry problems, most famously for document analysis/classification and spam filtering.


:math:`\bullet` Additionally, the training time for a Naive Bayes model can sometimes be drastically lower than for other models because the calculations that are needed to make it work are relatively cheap in terms of computer resource consumption. This also means it is highly scalable and able to work with large increases in the amount of data it must handle.

**Cons**


:math:`\bullet` One of the biggest problems with Naive Bayes is the data assumptions that were mentioned earlier. Few datasets have truly conditionally independent features—it is something that is very rare in the world today. However, Naive Bayes models can still perform well even if the assumption of conditional independence is violated. 

:math:`\bullet` Another issue that could arise is what is known as the “zero frequency” problem. This occurs when the dataset you’re using has no occurrences of a class label and some value of a predictor variable together. This would mean that there is a probability of zero. Since the final posterior probability is found by multiplying all of the individual probabilities together, the probability of zero would automatically make the result zero. Library implementations of the algorithms account for this by adding a negligible value to each variable count (usually 1) to ensure a non-zero probability.



7.3.1 Building a Bayes’ model
""""""""""""""""""""""""""""""""""""


We will build our own Naive Bayes model in this example. Naive Bayes models can be valuable to use any time you are doing work with predictions because they give you a way to account for new information. In today's world, where data is constantly evolving, modeling with Naive Bayes can help you adapt quickly and make more accurate predictions about what could occur.

In this example we will work for a firm that provides insights for management and coaches in the National Basketball Association (NBA).The league is interested in retaining players who can last in the high-pressure environment of professional basketball and help the team be successful over time.

The data for this activity consists of performance statistics from each player's rookie year. There are 1,341 observations, and each observation in the data represents a different player in the NBA. Your target variable is a Boolean value that indicates whether a given player will last in the league for five years.

**Feature engineering**

First we need to conduct feature engineering to determine which attributes in the data can best predict certain measures. We will analyze a subset of data that contains information about NBA players and their performance records. Next we will conduct feature engineering to determine which features will most effectively predict whether a player's NBA career will last at least five years. 



First we need to import the **panda** library: 

.. code-block:: python

	import pandas as pd


The dataset is a .csv file named nba-players.csv. It consists of performance records for a subset of NBA players.

.. code-block:: python

	data = pd.read_csv("nba-players.csv", index_col=0)


Here is the description for each column for this dataset: 

.. figure:: PNG/53.png
   :align: center

In the dataset, the name column is categorical, and the rest of the columns are numerical.

**Check for missing values**

Now, review the data to determine whether it contains any missing values. Begin by displaying the number of missing values in each column. After that, use isna() to check whether each value in the data is missing. Finally, use sum() to aggregate the number of missing values per column:


.. code-block:: python

	data.isna().sum()

We a see all columns in this dataset have 0 missing values.Checking for missing values is an important step in data exploration. Missing values are not particularly useful, so it's important to handle them by cleaning the data.

**Statistical tests**


Next,use a statistical technique to check the class balance in the data. To understand how balanced the dataset is in terms of class, display the percentage of values that belong to each class in the target column. In this context, class 1 indicates an NBA career duration of at least five years, while class 0 indicates an NBA career duration of less than five years.



.. code-block:: python

	# Display percentage (%) of values for each class (1, 0) represented in the target column of this dataset.

	data["target_5yrs"].value_counts(normalize=True)*100


Here is the output of the above line: 

.. code-block:: python

	1    62.014925
	0    37.985075

:math:`\bullet` About 62% of the values in the target columm belong to class 1, and about 38% of the values belong to class 0. In other words, about 62% of players represented by this data have an NBA career duration of at least five years, and about 38% do not.

:math:`\bullet` The dataset is not perfectly balanced, but an exact 50-50 split is a rare occurance in datasets, and a 62-38 split is not too imbalanced. However, if the majority class made up 90% or more of the dataset, then that would be of concern, and it would be prudent to address that issue through techniques like upsampling and downsampling.

.. note::

   If there is a lot more representation of one class than another, then the model may be biased toward the majority class. When this happens, the predictions may be inaccurate.


**Results and evaluation**


Now, perform feature engineering, with the goal of identifying and creating features that will serve as useful predictors for the target variable, target_5yrs

Here are a couple notes: 


You should avoid selecting the name column as a feature. A player's name is not helpful in determining their career duration. Moreover, it may not be ethical or fair to predict a player's career duration based on a name.

:math:`\bullet` The number of games a player has played in may not be as important in determining their career duration as the number of points they have earned. While you could say that someone who has played in more games may have more practice and experience, the points they earn during the games they played in would speak more to their performance as a player. This, in turn, would influence their career duration. So, the gp column on its own may not be a helpful feature. However, gp and pts could be combined to get the total number of points earned across the games played, and that result could be a helpful feature. That approach can be implemented later in the feature engineering process—in feature extraction.

:math:`\bullet` If the number of points earned across games will be extracted as a feature, then that could be combined with the number of minutes played across games (min) to extract another feature. This could be a measure of players' efficiency and could help in predicting players' career duration. min on its own may not be useful as a feature for the same reason as gp, mentioned above.

:math:`\bullet` There are three different columns that give information about field goals. The percent of field goals a player makes (ft) says more about their performance than the number of field goals they make (ftm) or the number of field goals they attempt (fta). The percent gives more context, as it takes into account both how many field goals a player successfully made and how many field goals they attempted in total. This allows for a more meaningful comparison between players. The same logic applies to the percent of three-point field goals made, as well as the percent of free throws made.

:math:`\bullet` There are columns for the number offensive rebounds (oreb), the number of defensive rebounds (dreb), and the number of rebounds overall (reb). Because the overall number of rebounds should already incorporate both offensive and defensive rebounds, it would make sense to use the overall as a feature.

:math:`\bullet` The number of assists (ast), steals (stl), blocks (blk), and turnovers (tov) also provide information about how well players are performing in games, and thus, could be helpful in predicting how long players last in the league.


Therefore, at this stage of the feature engineering process, it would be most effective to select the following columns:


.. code-block:: python

	gp, min, pts, fg, 3p, ft, reb, ast, stl, blk, tov.

Next, select the columns you want to proceed with. Make sure to include the target column, target_5yrs. Display the first few rows to confirm they are as expected:


.. code-block:: python

	# Select the columns to proceed with and save the DataFrame in new variable `selected_data`.
	# Include the target column, `target_5yrs`.


	selected_data = data[["gp", "min", "pts", "fg", "3p", "ft", "reb", "ast", "stl", "blk", "tov", "target_5yrs"]]



**Feature transformation**

An important aspect of feature transformation is feature encoding. If there are categorical columns that you would want to use as features, those columns should be transformed to be numerical. This technique is also known as feature encoding.


Many types of models are designed in a way that requires the data coming in to be numerical. So, transforming categorical features into numerical features is an important step.
In this particular dataset, name is the only categorical column and the other columns are numerical (discussed in the exemplar response to Question 2). Given that :math:`name` is not selected as a feature, all of the features that are selected at this point are already numerical and do not require transformation.


**Feature extraction**

:math:`\bullet` The gp, pts, min columns lend themselves to feature extraction.

:math:`\bullet` gp represents the total number of games a player has played in, and pts represents the average number of points the player has earned per game. It might be helpful to combine these columns to get the total number of points the player has earned across the games and use the result as a new feature, which could be added into a new column named total_points. The total points earned by a player can reflect their performance and shape their career longevity.

:math:`\bullet` The min column represents the number of minutes played across games. total_points could be combined with min to extract a new feature: points earned per minute. This can be considered a measure of player efficiency, which could shape career duration. This feature can be added into a column named efficiency.


Extract two features that you think would help predict target_5yrs. Then, create a new variable named 'extracted_data' that contains features from 'selected_data', as well as the features being extracted:


.. code-block:: python

	# Extract two features that would help predict target_5yrs.
	# Create a new variable named `extracted_data`.


	# Make a copy of `selected_data` 
	extracted_data = selected_data.copy()

	# Add a new column named `total_points`; 
	# Calculate total points earned by multiplying the number of games played by the average number of points earned per game
	extracted_data["total_points"] = extracted_data["gp"] * extracted_data["pts"]

	# Add a new column named `efficiency`. Calculate efficiency by dividing the total points earned by the total number of minutes played, which yields points per minute
	extracted_data["efficiency"] = extracted_data["total_points"] / extracted_data["min"]



Now, to prepare for the Naive Bayes model.Now we need to clean the extracted data and ensure ensure it is concise. Naive Bayes involves an assumption that features are independent of each other given the class. In order to satisfy that criteria, if certain features are aggregated to yield new features, it may be necessary to remove those original features. Therefore, drop the columns that were used to extract new features:



.. code-block:: python

	# Remove any columns from `extracted_data` that are no longer needed.
	# Remove `gp`, `pts`, and `min` from `extracted_data`.
	extracted_data = extracted_data.drop(columns=["gp", "pts", "min"])


Next, export the extracted data as a new .csv file. You will use this in a later lab:


.. code-block:: python

	# Export the extracted data.
	extracted_data.to_csv("extracted_nba_players_data.csv", index=0)


**Build a Naive Bayes model**


We conducted feature engineering to determine which features would most effectively predict a player's career duration. We will now use those insights to build a model that predicts whether a player will have an NBA career lasting five years or more.

The data for this activity consists of performance statistics from each player's rookie year. There are 1,341 observations, and each observation in the data represents a different player in the NBA. Your target variable is a Boolean value that indicates whether a given player will last in the league for five years. Since we previously performed feature engineering on this data, it is now ready for modeling.


First we need to import required packages. Begin with your import statements. Of particular note here are pandas and from sklearn, naive_bayes, model_selection, and metrics.



.. code-block:: python

	import pandas as pd
	from sklearn import naive_bayes
	from sklearn import model_selection
	from sklearn import metrics

We will use the dataset we created in the previous step (feature engineering): 

.. code-block:: python

	# Load extracted_nba_players.csv into a DataFrame called extracted_data.
	extracted_data = pd.read_csv('extracted_nba_players.csv')


**Model preparation**

We should separately define the target variable (target_5yrs) and the features. In other words, we need to isolate the target and predictor variables:


.. code-block:: python

	# Define the y (target) variable.
	y = extracted_data['target_5yrs']

	# Define the X (predictor) variables.
	X = extracted_data.drop('target_5yrs', axis = 1)


.. note::

   Given that the target variable contains both 1 and 0 indicates that it is binary and requires a model suitable for binary classification.


Now we can divide your data into a training set (75% of data) and test set (25% of data). This is an important step in the process, as it allows you to reserve a part of the data that the model has not observed. This tests how well the model generalizes—or performs—on new data.


.. code-block:: python

	# Perform the split operation on your data.
	# Assign the outputs as follows: X_train, X_test, y_train, y_test.
	X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=0)

Each training DataFrame contains 1,005 rows, while each test DataFrame contains 335 rows. Additionally, there are 10 columns in each X DataFrame, with only one column in each y DataFrame.

Using the assumption that your features are normally distributed and continuous, the **Gaussian Naive Bayes** algorithm is most appropriate for your data. While your data may not perfectly adhere to these assumptions, this model will still yield the most usable and accurate results.



.. note::

   There are several implementations of Naive Bayes in scikit-learn, all of which are found in the sklearn.naive_bayes module:

   :math:`\bullet` BernoulliNB:        Used for binary/Boolean features 

   :math:`\bullet` CategoricalNB: 	Used for categorical features

   :math:`\bullet` ComplementNB: 	Used for imbalanced datasets, often for text classification tasks

   :math:`\bullet` GaussianNB:		Used for continuous features, normally distributed features

   :math:`\bullet` MultinomialNB:	Used for multinomial (discrete) features


**Fit the model**

By creating your model, you will be drawing on your feature engineering work by training the classifier on the X_train DataFrame. You will use this to predict target_5yrs from y_train.

Start by defining **nb** to be the relevant algorithm from **sklearn.naive_bayes**. Then fit your model to your training data. Use this fitted model to create predictions for your test data.


.. code-block:: python

	# Assign `nb` to be the appropriate implementation of Naive Bayes.
	nb = naive_bayes.GaussianNB()

	# Fit the model on your training data.
	nb.fit(X_train, y_train)

	# Apply your model to predict on your test data. Call this "y_pred".
	y_pred = nb.predict(X_test)


**Results and evaluation**

To evaluate the data yielded from your model, you can leverage a series of metrics and evaluation techniques from scikit-learn by examining the actual observed values in the test set relative to your model's prediction. Specifically, print the accuracy score, precision score, recall score, and f1 score associated with your test data and predicted values.

:math:`F_{1}` Score

:math:`F_{1} Score` is a measurement that combines both precision and recall into a single expression, giving each equal importance. It is calculated as:

.. math:: 
  :name: eq.215

   F_{1}= 2. \frac{precision ⋅ recall}{precision + recall}

 

This combination is known as the harmonic mean. F1 score can range [0, 1], with zero being the worst and one being the best. The idea behind this metric is that it penalizes low values of either metric, which prevents one very strong factor—precision or recall—from “carrying” the other, when it is weaker.


.. note::

   The F1 score never exceeds the mean. In fact, it is only equal to the mean in a single case: when precision equals recall. 



.. code-block:: python

	# Print your accuracy score.
	print('accuracy score:'), print(metrics.accuracy_score(y_test, y_pred))

	# Print your precision score.
	print('precision score:'), print(metrics.precision_score(y_test, y_pred))

	# Print your recall score.
	print('recall score:'), print(metrics.recall_score(y_test, y_pred))

	# Print your f1 score.
	print('f1 score:'), print(metrics.f1_score(y_test, y_pred))

Here is the output:


.. code-block:: python

	accuracy score:
	0.6985074626865672
	precision score:
	0.8211920529801324
	recall score:
	0.6262626262626263
	f1 score:
	0.7106017191977076


:math:`\bullet` Based on the above results, The accuracy score for this model is 0.713, or 71.3% accurate.In classification problems, accuracy is useful to know but may not be the best metric by which to evaluate this model. While accuracy is often the most intuitive metric, it is a poor evaluation metric in some cases. In particular, if you have imbalanced classes, a model could appear accurate but be poor at balancing false positives and false negatives.


:math:`\bullet` Precision and recall scores are both useful to evaluate the correct predictive capability of a model because they balance the false positives and false negatives inherent in prediction.

:math:`\bullet` The model shows a precision score of 0.845, suggesting the model is quite good at predicting true positives—meaning the player will play longer than five years—while balancing false positives. The recall score of 0.6375 shows worse performance in predicting true negatives—where the player will not play for five years or more—while balancing false negatives.These two metrics combined can give a better assessment of model performance than accuracy does alone.

:math:`\bullet` The F1 score balances the precision and recall performance to give a combined assessment of how well this model delivers predictions. In this case, the F1 score is 0.7268, which suggests reasonable predictive power in this model.



**Confusion matrix**

a confusion matrix is a graphic that shows your model's true and false positives and negatives. It helps to create a visual representation of the components feeding into the metrics.

.. code-block:: python

	# Construct and display your confusion matrix.
	# Construct the confusion matrix for your predicted and test values.
	cm = metrics.confusion_matrix(y_test, y_pred)

	# Create the display for your confusion matrix.
	disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=nb.classes_)

	# Plot the visual in-line.
	disp.plot()

Here is the output: 


.. figure:: PNG/54.png
   :align: center


According to the above graph, there are a couple conclusions: 

:math:`\bullet` The top left to bottom right diagonal in the confusion matrix represents the correct predictions, and the ratio of these squares showcases the accuracy.

:math:`\bullet` The concentration of true positives stands out relative to false positives. This ratio is why the precision score is so high (0.845).

:math:`\bullet` True negatives and false negatives are closer in number, which explains the worse recall score



**Some notes**

**1.** The evaluation of the model is important to inform if the model has delivered accurate predictions.
Splitting the data was important for ensuring that there was new data for the model to test its predictive performance.

**2.** Each metric provided an evaluation from a different standpoint, and accuracy alone was not a strong way to evaluate the model.

**3.** Effective assessments balance the true/false positives versus true/false negatives through the confusion matrix and F1 score.

**Conclusions**


:math:`\bullet` The model created provides some value in predicting an NBA player's chances of playing for five years or more.

:math:`\bullet` Notably, the model performed better at predicting true positives than it did at predicting true negatives. In other words, it more accurately identified those players who will likely play for more than five years than it did those who likely will not.


7.4. K-means algorithm
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


K-means algorithm is an unsupervised learning technique used to cluster unlabeled data.


First we need to define **Centroid** :

**Centroid**

The center of a cluster is determined by mathematical mean of all the points in that cluster.


There 4 steps in implementation of the K-mean algorithm:


**1.** Initiate K centroids by randomly placing the centroids in space

**2.** Assign all points to their nearest centroid

**3.** Recalculate the centroid of each cluster based on the points assigned to it

**4.** Repeat the steps 2 and 3 until convergence


This is important K-means multiple times with different initial positions of the centroids to help avoid using a model that gets stuck in local minima. Fortunately, most machine learning packages have improved implementations of K-means that make it easier for you by removing this requirement. 

In scikit-learn, this implementation is called K-means++. K-means++ still randomly initializes centroids in the data, but it does so based on a probability calibration. Basically, it randomly chooses one point within the data to be the first centroid, then it uses other data points as centroids, selecting them pseudo-randomly. The probability that a point will be selected as a centroid increases the farther it is from other centroids. This helps to ensure that centroids aren’t initially placed very close together, which is when convergence in local minima is most likely to occur. 


7.4.1 Example of K-means for color compression
""""""""""""""""""""""""""""""""""""""""""""""""""""""""""

In this example, we will solve worked example of K-means on non-synthetic data. We will use K-means to cluster the pixels of a photograph of some tulips based on their encoded color values. We will explore how different values of k affect the clustering of the pixels, and thus the appearance of the photograph. 

As usual, we need to import the required packages. We will be using numpy and pandas for operations, and Plotly for 3-D visualization. Of particular note is Kmeans, which is scikit-learn's implementation of the K-means algorithm.: 

.. code-block:: python

	import numpy as np
	import pandas as pd
	import plotly.graph_objects as go
	from sklearn.cluster import KMeans

The "data" in this case is not a pandas dataframe. It's a photograph, which we'll convert into a numerical array:


.. code-block:: python

	img = plt.imread('using_kmeans_for_color_compression_tulips_photo.jpg')

We can see extract the dimensions of the photo and visualize the actual photo as follows:

.. code-block:: python

	# Display the photo and its shape
	print(img.shape)
	plt.imshow(img)
	plt.axis('off');

Here is the photo:

.. figure:: PNG/55.png
   :align: center

Here we have a photograph of some tulips. The shape of the image is 320 x 240 x 3. This can be interpreted as pixel information. Each dot on your screen is a pixel. This photograph has 320 vertical pixels and 240 horizontal pixels.

But what is the third dimension of "3"? This dimension refers to the values that encode the color of each pixel. Each pixel has 3 parameters: red (R), green (G), and blue (B), also known as its RGB values. For each color—R, G, and B—the encoded value can range from 0-255. This means that there are 256³, or 16,777,216 different combinations of RGB, each resulting in a unique color on your screen.


To prepare this data for modeling, we'll reshape it into an array, where each row represents a single pixel's RGB color values:


.. code-block:: python

	# Reshape the image so that each row represents a single pixel 
	# defined by three values: R, G, B
	img_flat = img.reshape(img.shape[0]*img.shape[1], 3)


If we want to see information of the color for the first 5 pixels: 


.. code-block:: python

	img_flat[:5, :]

Here is the output:

.. code-block:: python

	array([[211, 196,  41],
       	[199, 180,  24],
       	[179, 152,   0],
       	[186, 150,   0],
       	[187, 143,   0]], dtype=uint8)

**Plot the data in 3-D space**

Now we have an array that is 76,800 x 3. Each row is a single pixel's color values. Because we have only 3 columns (R-G-B), we can visualize this data in 3-dimensional space. Let's create a pandas dataframe to help us understand and visualize our data:


.. code-block:: python

	# Create a pandas df with r, g, and b as columns
	img_flat_df = pd.DataFrame(img_flat, columns = ['r', 'g', 'b'])

Now we can create the 3d plot:


.. code-block:: python


	# Create 3D plot where each pixel in the `img` is displayed in its actual color
	trace = go.Scatter3d(x = img_flat_df.r,
                     y = img_flat_df.g,
                     z = img_flat_df.b,
                     mode='markers',
                     marker=dict(size=1,
                                 color=['rgb({},{},{})'.format(r,g,b) for r,g,b 
                                        in zip(img_flat_df.r.values, 
                                               img_flat_df.g.values, 
                                               img_flat_df.b.values)],
                                 opacity=0.5))

	data = [trace]

	layout = go.Layout(margin=dict(l=0,
                               r=0,
                               b=0,
                               t=0),
                               )

	fig = go.Figure(data=data, layout=layout)
	fig.update_layout(scene = dict(
                    xaxis_title='R',
                    yaxis_title='G',
                    zaxis_title='B'),
                  )
	fig.show()


Here is the output:

.. figure:: PNG/56.png
   :align: center


In this graph, each dot represents a color/pixel that is in our original image of tulips. The more intense the color, the more dots are concentrated in that area. The most-represented colors in the graph are the most abundant colors in the photograph: mostly reds, greens, and yellows. 


We can train a K-means model on this data. The algorithm will create k clusters by minimizing the squared distances from each point to its nearest centroid. Let's first do an experiment. What would you expect to happen if we built a K-means model with just a single centroid (k = 1) and replaced each pixel in the photograph with the RGB value of that centroid? What would the photograph look like?


**Cluster the data: k = 1**


.. code-block:: python


	# Instantiate the model
	kmeans = KMeans(n_clusters=1, random_state=42).fit(img_flat)

	# Copy `img_flat` so we can modify it
	img_flat1 = img_flat.copy()

	# Replace each row in the original image with its closest cluster center
	for i in np.unique(kmeans.labels_):
    	    img_flat1[kmeans.labels_==i,:] = kmeans.cluster_centers_[i]

	# Reshape the data back to (640, 480, 3)
	img1 = img_flat1.reshape(img.shape)

	plt.imshow(img1)
	plt.axis('off');

Here is the out put: 


.. figure:: PNG/57.png
   :align: center

According to the steps mentioned in the procedure of K-mean implementation: 

**1.** We randomly placed our centroid in the colorspace.

**2.** We assigned each point to its nearest centroid. Since there was only one centroid, all points were assigned to the same centroid, and thus to the same cluster.

**3.** We updated the centroid's location to the mean location of all of its points. Again, since there is only a single centroid, it updated to the mean location of every point in the image.

**4.** Repeat until the model converges. In this case, it only took one iteration for the model to converge.


.. note::

   We then updated each pixel's RGB values to be the same as the centroid's. The result is the image of our tulips when every pixel is replaced with the average color. The average color of this photo was brown⁠—all the colors muddled together.


We can verify this for ourselves by manually calculating the average for each column in the flattened array. This will give us the average R value, G value, and B value.

.. code-block:: python

	# Calculate mean of each column in the flattened array
	column_means = img_flat.mean(axis=0)

	print('column means: ', column_means)


Here is the output:

.. code-block:: python

	column means:  [125.64397135  77.93165365  43.51584635]

Now, we can compare this to what the K-means model calculated as the final location of its one centroid.


.. code-block:: python

	print('cluster centers: ', kmeans.cluster_centers_)

The output of the above line is: 

.. code-block:: python

	column means:  [125.64397135  77.93165365  43.51584635]

Which is the same as the average of the values calculated manually. Now return to the 3-D rendering of our data, only this time we'll add the centroid:

.. code-block:: python



	# Create 3-D plot where each pixel in the `img` is displayed in its actual color
	trace = go.Scatter3d(x = img_flat_df.r,
                     y = img_flat_df.g,
                     z = img_flat_df.b,
                     mode='markers',
                     marker=dict(size=1,
                                 color=['rgb({},{},{})'.format(r,g,b) for 
                                        r,g,b in zip(img_flat_df.r.values, 
                                                     img_flat_df.g.values, 
                                                     img_flat_df.b.values)],
                                 opacity=0.5))

	data = [trace]

	layout = go.Layout(margin=dict(l=0,
                               r=0,
                               b=0,
                               t=0))

	fig = go.Figure(data=data, layout=layout)


	# Add centroid to chart
	centroid = kmeans.cluster_centers_[0].tolist()

	fig.add_trace(
    go.Scatter3d(x = [centroid[0]],
                 y = [centroid[1]],
                 z = [centroid[2]],
                 mode='markers',
                 marker=dict(size=7,
                             color=['rgb(125.79706706,77.8178776,42.58090169)'],
                             opacity=1)))
	fig.update_layout(scene = dict(
                    xaxis_title='R',
                    yaxis_title='G',
                    zaxis_title='B'),
                  )
	fig.show()


Here is the output:

.. figure:: PNG/57.png
   :align: center

We can see the centroid as a large circle in the middle of the colorspace. (If you can't, just click on the image and spin/zoom it.) Notice that this is the "center of gravity" of all the points in the graph.

Now let's try something else. Let's refit a K-means model to the data, this time using k = 3.



**Cluster the data: k = 3**

.. code-block:: python

	# Instantiate k-means model for 3 clusters
	kmeans3 = KMeans(n_clusters=3, random_state=42).fit(img_flat)

The .cluster_centers_ attribute returns an array where each element represents the coordinates of a centroid (i.e., their RGB values). We'll use these coordinates as we did previously to generate the colors that are represented by our centroids:

.. code-block:: python

	centers = kmeans3.cluster_centers_
	print (centers)


Here is the output:

.. code-block:: python

	array([[ 40.67836777,  50.5497578 ,  16.2448648 ],
       [202.22832829, 173.55005895, 109.60875583],
       [177.41608916,  41.64855117,  27.31525836]])

Let's now replace each pixel in the original image with the RGB value of the centroid to which it was assigned:

.. code-block:: python

	# Helper function to display our photograph when clustered into k clusters
	def cluster_image(k, img=img):
            '''
    	    Fits a K-means model to a photograph.
            Replaces photo's pixels with RGB values of model's centroids.
            Displays the updated image.

            Args:
      		k:    (int)          - Your selected K-value
      		img:  (numpy array)  - Your original image converted to a numpy array

    	    Returns:
      		The output of plt.imshow(new_img), where new_img is a new numpy array \
      		where each row of the original array has been replaced with the \ 
      		coordinates of its nearest centroid.
    	    '''

    	    img_flat = img.reshape(img.shape[0]*img.shape[1], 3)
    	    kmeans = KMeans(n_clusters = k, random_state = 42).fit(img_flat)
    	    new_img = img_flat.copy()
  
    	    for i in np.unique(kmeans.labels_):
        	new_img[kmeans.labels_ == i, :] = kmeans.cluster_centers_[i]
  
    	    new_img = new_img.reshape(img.shape)

    	    return plt.imshow(new_img), plt.axis('off');

	# Generate image when k=3
	cluster_image(3);

This is the output:


.. figure:: PNG/59.png
   :align: center

We now have a photo with just three colors. Each pixel's RGB values correspond to the values of its nearest centroid.

We can return once more to our 3-D colorspace. This time, we'll re-color each dot in the colorspace to correspond with the color of its centroid. This will allow us to see how the K-means algorithm clustered our data spatially.

K-means works best when the clusters are more circular, because it tries to minimize distance from point to centroid. It may be worth trying a different clustering algorithm if you want to cluster a long, narrow, continuous band of data.


Nonetheless, K-means successfully compresses the colors of this photograph. This process can be applied for any value of k. Here's the output of each photo for k = 2–10.



**Cluster the data: k = 2-10**

.. code-block:: python

	def cluster_image_grid(k, ax, img=img):
    	    '''
    	    Fits a K-means model to a photograph.
    	    Replaces photo's pixels with RGB values of model's centroids.
    	    Displays the updated image on an axis of a figure.

    	    Args:
      	    	k:    (int)          - Your selected K-value
      	    	ax:   (int)          - Index of the axis of the figure to plot to
      	    	img:  (numpy array)  - Your original image converted to a numpy array

    	    Returns:
      	    	A new image where each row of the ori array has been replaced with the \ 
      	    	coordinates of its nearest centroid.
    	    '''
    	    img_flat = img.reshape(img.shape[0]*img.shape[1], 3)
    	    kmeans = KMeans(n_clusters=k, random_state=42).fit(img_flat)
    	    new_img = img_flat.copy()

    	    for i in np.unique(kmeans.labels_):
        	new_img[kmeans.labels_==i, :] = kmeans.cluster_centers_[i]

    	    new_img = new_img.reshape(img.shape)
    	    ax.imshow(new_img)
    	    ax.axis('off')

	fig, axs = plt.subplots(3, 3)
	fig = matplotlib.pyplot.gcf()
	fig.set_size_inches(9, 12)
	axs = axs.flatten()
	k_values = np.arange(2, 11)
	for i, k in enumerate(k_values):
    	    cluster_image_grid(k, axs[i], img=img)
    	    axs[i].title.set_text('k=' + str(k))

Here is the output: 

.. figure:: PNG/60.png
   :align: center


From the above figure, it could be observed that as we group the data into more and more clusters, additional clusters beyond a certain point contribute less and less to your understanding of your data.



7.4.2 Evaluation of the K-mean
"""""""""""""""""""""""""""""""""

You know that the evaluation metrics you used for supervised learning models don’t apply to unsupervised learning models. This is because unsupervised learning model results cannot be categorized as “correct” or “incorrect.” While supervised learning models use predictor variables to predict a defined target variable, unsupervised learning methods have metrics that seek an underlying structure within the data.


Clustering models are a type of unsupervised learning that do this by grouping observations together. Data professionals often use **inertia** and **silhouette scores** to evaluate their clustering models and help them determine which groupings make sense. This reading reviews these concepts and examines them in greater detail.


**1. Inertia** 

Inertia is a measurement of intracluster distance. It indicates how compact the clusters are in a model. Specifically, inertia is the sum of the squared distance between each point and the centroid of the cluster that it’s assigned to. It can be represented by this formula, where:



:math:`\bullet` :math:`n` = the number of observations in the data, 

​
:math:`\bullet` :math:`x_{i}` = the location of a particular observation, 

​
:math:`\bullet` :math:`C_{k}` = the location of the centroid of cluster k, which is the cluster to which point  is assigned.


The inertia is defined as: 


.. math:: 
  :name: eq.117

   Inertia = \sum_{i=0}^n (x_{i}-C_{k})^2

.. note::

   The greater the inertia, the greater the distances between points and their centroids, which means the points within each cluster are farther apart from each other. Note, however, that inertia only measures intracluster distance. For the same dataset and the same number of clusters, lower inertia values are typically better than higher values, because low values indicate that points are closer together within their clusters. 


**Evaluating inertia**

Inertia is a useful metric to determine how well your clustering model identifies meaningful patterns in the data. But it’s generally not very useful by itself. If your model has an inertia of 53.25, is that good? It depends. The measurement becomes meaningful when it’s compared to the inertia values and k values of other models on the same data. As you increase the number of clusters (k), the inertia value will drop, but there comes a point where adding more clusters will have only small changes in inertia. And it's this transition that we need to detect.


**The elbow method**

The elbow method is a great way to this point of transition. It’s a way to help decide which clustering gives the most meaningful model of your data. It uses a line plot to visually compare the inertias of different models. With K-means models, this is done as a comparison between different values of k. Here’s an example:



.. figure:: PNG/61.png
   :align: center

Based on the above graph, it compares the inertias of nine different K-means models—one for each value of k from two through 10. It’s clear that inertia begins very high when the data is grouped into two clusters. The three-cluster model, however, has much lower inertia, creating a steep negative slope between two and three clusters. After that, the rate of inertial decline slows down dramatically, as indicated by the much flatter line in the plot. 


.. note::

   Please note that, you want inertia to be low, but if you add more and more clusters with only minimal improvement to inertia, you’re only adding complexity without capturing real structure in the data. 


**2. Silhouette analysis** 

A silhouette analysis is the comparison of different models’ silhouette scores. The silhouette score is the mean silhouette coefficient over all the observations in a model.To calculate a model’s silhouette score, first, a silhouette coefficient is calculated for each instance in the data. An instance’s silhouette coefficient is defined by the following formula, where:


:math:`\bullet` :math:`a` = the mean distance between the instance and each other instance in the same cluster 


:math:`\bullet` :math:`b` = the mean distance from the instance to each instance in the nearest other cluster (i.e., excluding the cluster 

:math:`\bullet` :math:`max(a,b)` = whichever value is greater between :math:`a` and :math:`b`


The silhouette score is defined as: 


.. math:: 
  :name: eq.118

   Silhouette\: Scores = \frac{(b-a)}{max(a,b)}

A silhouette coefficient can range between -1 and +1. 


:math:`\bullet` A value closer to +1 means that a point is close to other points in its own cluster and well separated from points in other clusters. 

:math:`\bullet` A value closer to zero means that a point is between clusters.


:math:`\bullet` A value closer to -1 means that a point is probably assigned to the wrong cluster, 

The greater the silhouette score, the better defined the model clusters, because the points in a given cluster are closer to each other, and the clusters themselves are more separated from each other.


.. note::

   Note that, unlike inertia, silhouette coefficients contain information about both intracluster distance (captured by the variable a) and intercluster distance (captured by the variable b).

As with inertia values, you can plot silhouette scores for different models to compare them against each other:


.. figure:: PNG/62.png
   :align: center


In this example, it’s evident that a three-cluster model has a higher silhouette score than any other model. Based on this diagram, the data is probably best grouped into three clusters. 

We can summarize the key points regarding the inertia and Silhouette score: 

**Inertia:**

:math:`\bullet` Measures intracluster distance

:math:`\bullet` Equal to the sum of the squared distance between each point and the centroid of the cluster that it’s assigned to

:math:`\bullet` Used in elbow plots

:math:`\bullet` All else equal, lower values are generally better

**Silhouette score:**

:math:`\bullet` Measures both intercluster distance and intracluster distance

:math:`\bullet` Equal to the average of all points’ silhouette coefficients

:math:`\bullet` Can be between -1 and +1 (greater values are better)


7.4.3 Build a K-means model
"""""""""""""""""""""""""""""""""

In this example, the dataset is a spreadsheet that includes datapoints across a sample size of 345 penguins, such as species, island, and sex. We will use a K-means clustering model to group this data and identify patterns that provide important insights about penguins

We should import the required packages at the first step:


.. code-block:: python

	# Import standard operational packages.
	import numpy as np
	import pandas as pd

	# Important tools for modeling and evaluation.
	from sklearn.cluster import KMeans
	from sklearn.metrics import silhouette_score
	from sklearn.preprocessing import StandardScaler

	# Import visualization packages.
	import matplotlib.pyplot as plt
	import seaborn as sns

Then we should load the data:

.. code-block:: python

	# Save the `pandas` DataFrame in variable `penguins`. 
	penguins = pd.read_csv("penguins.csv")

After loading the dataset, the next step is to prepare the data to be suitable for clustering. This includes:

:math:`\bullet` Exploring data

:math:`\bullet` Checking for missing values

:math:`\bullet` Encoding data

:math:`\bullet` Dropping a column

:math:`\bullet` Scaling the features using StandardScaler

To cluster penguins of multiple different species, determine how many different types of penguin species are in the dataset:


.. code-block:: python

	# Find out how many penguin types there are.
	penguins['species'].unique()


	# Find the count of each species type.
	penguins['species'].value_counts(dropna = False)


There are three types of species. Note the Chinstrap species is less common than the other species. This has a chance to affect K-means clustering as K-means performs best with similar sized groupings.

For purposes of clustering, pretend you don't know that there are three different types of species. Then, you can explore whether the algorithm can discover the different species. You might even find other relationships in the data.


**Check for missing values**

An assumption of K-means is that there are no missing values. Check for missing values in the rows of the data:


.. code-block:: python

	# Check for missing values.
	penguins.isnull().sum()

Now, drop the rows with missing values and save the resulting pandas DataFrame in a variable named penguins_subset:


.. code-block:: python

	# Drop rows with missing values.
	# Save DataFrame in variable `penguins_subset`.
	penguins_subset = penguins.dropna(axis=0).reset_index(drop = True)

Next, check to make sure that `penguins_subset` does not contain any missing values.

.. code-block:: python

	# Check for missing values.
	penguins_subset.isna().sum()

Now we can see that there is no row remained with missing values in the data.


Some versions of the penguins dataset have values encoded in the sex column as 'Male' and 'Female' instead of 'MALE' and 'FEMALE'. The code below will make sure all values are ALL CAPS:


.. code-block:: python

	penguins_subset['sex'] = penguins_subset['sex'].str.upper()


K-means needs numeric columns for clustering. Convert the categorical column 'sex' into numeric. There is no need to convert the 'species' column because it isn't being used as a feature in the clustering algorithm.


.. code-block:: python

	# Convert `sex` column from categorical to numeric.
	penguins_subset = pd.get_dummies(penguins_subset, drop_first = True, columns=['sex'])

Drop the categorical column island from the dataset. While it has value, we assume if penguins of the same species exhibit different physical characteristics based on sex. This doesn't include location.



.. code-block:: python

	# Drop the island column.
	penguins_subset = penguins_subset.drop(['island'], axis=1)

**Scale the features**

Because K-means uses distance between observations as its measure of similarity, it's important to scale the data before modeling. Use a third-party tool, such as scikit-learn's **StandardScaler** function. StandardScaler scales each point xᵢ by subtracting the mean observed value for that feature and dividing by the standard deviation: x-scaled = (xᵢ – mean(X)) / σ

This ensures that all variables have a mean of 0 and variance/standard deviation of 1.

.. note::

   Because the species column isn't a feature, it doesn't need to be scaled.


First, copy all the features except the 'species' column to a DataFrame X:


.. code-block:: python

	# Exclude `species` variable from X
	X = penguins_subset.drop(['species'], axis=1)

Scale the features in X using StandardScaler, and assign the scaled data to a new variable X_scaled.


.. code-block:: python

	#Scale the features.
	#Assign the scaled data to variable `X_scaled`.
	X_scaled = StandardScaler().fit_transform(X)


Now, fit K-means and evaluate inertia for different values of k. Because you may not know how many clusters exist in the data, start by fitting K-means and examining the inertia values for different values of k. To do this, write a function called kmeans_inertia that takes in num_clusters and x_vals (X_scaled) and returns a list of each k-value's inertia.

When using K-means inside the function, set the random_state to 42. This way, others can reproduce your results.


.. code-block:: python

	# Fit K-means and evaluate inertia for different values of k.
	num_clusters = [i for i in range(2, 11)]

	def kmeans_inertia(num_clusters, x_vals):
    	    """
    	    Accepts as arguments list of ints and data array. 
    	    Fits a KMeans model where k = each value in the list of ints. 
    	    Returns each k-value's inertia appended to a list.
    	    """
    	inertia = []
    	for num in num_clusters:
            kms = KMeans(n_clusters=num, random_state=42)
            kms.fit(x_vals)
            inertia.append(kms.inertia_)

    	return inertia

Use the kmeans_inertia function to return a list of inertia for k=2 to 10:


.. code-block:: python

	# Return a list of inertia for k=2 to 10.
	inertia = kmeans_inertia(num_clusters, X_scaled)
	print (inertia)

Here is the output of the above code which is the 10 inertia corresponding to the k=2 to k=10:

.. code-block:: python

	[885.6224143652249,
 	577.8284278107235,
 	386.1453442477329,
 	284.5464837898288,
 	217.92858573807678,
 	201.39287843423264,
 	185.461310432323,
 	173.4545211497985,
 	164.12001520260708]

Next, create a line plot that shows the relationship between num_clusters and inertia. Use either seaborn or matplotlib to visualize this relationship:


.. code-block:: python

	# Create a line plot.
	plot = sns.lineplot(x=num_clusters, y=inertia, marker = 'o')
	plot.set_xlabel("Number of clusters");
	plot.set_ylabel("Inertia");

Here is the output:


.. figure:: PNG/63.png
   :align: center

The plot seems to depict an elbow at six clusters, but there isn't a clear method for confirming that a six-cluster model is optimal. Therefore, the silhouette scores should be checked.


Now, evaluate the silhouette score using the **silhouette_score()** function. Silhouette scores are used to study the distance between clusters. Then, compare the silhouette score of each value of k, from 2 through 10. To do this, write a function called kmeans_sil that takes in num_clusters and x_vals (X_scaled) and returns a list of each k-value's silhouette score:


.. code-block:: python

	# Evaluate silhouette score.
	# Write a function to return a list of each k-value's score.

	def kmeans_sil(num_clusters, x_vals):
    	    """
    	    Accepts as arguments list of ints and data array. 
    	    Fits a KMeans model where k = each value in the list of ints.
    	    Calculates a silhouette score for each k value. 
    	    Returns each k-value's silhouette score appended to a list.
    	    """
    	    sil_score = []
    	    for num in num_clusters:
                kms = KMeans(n_clusters=num, random_state=42)
                kms.fit(x_vals)
                sil_score.append(silhouette_score(x_vals, kms.labels_))

    	    return sil_score


	sil_score = kmeans_sil(num_clusters, X_scaled)
	print (sil_score)


Here is the output of the above code which is the 10 silhouette score corresponding to the k=2 to k=10:

.. code-block:: python

	[0.44398088353055243,
	0.45101024097188375,
 	0.5080140996630784,
 	0.519998574860868,
 	0.5263224884981607,
	0.47774022332151733,
 	0.42219207326432245,
 	0.36062890821417276,
 	0.36172505634200175]

Next, create a line plot that shows the relationship between num_clusters and sil_score:


.. code-block:: python


	# Create a line plot.
	plot = sns.lineplot(x=num_clusters, y=sil_score, marker = 'o')
	plot.set_xlabel("# of clusters");
	plot.set_ylabel("Silhouette Score");


Here is the output:


.. figure:: PNG/64.png
   :align: center


Silhouette scores near 1 indicate that samples are far away from neighboring clusters. Scores close to 0 indicate that samples are on or very close to the decision boundary between two neighboring clusters.

The plot indicates that the silhouette score is closest to 1 when the data is partitioned into six clusters, although five clusters also yield a relatively good silhouette score.


To decide on an optimal k-value, fit a six-cluster model to the dataset.


.. code-block:: python

	# Fit a 6-cluster model.
	kmeans6 = KMeans(n_clusters=6, random_state=42)
	kmeans6.fit(X_scaled)

We can print the unique labels (clusters) of the fit model:



.. code-block:: python


	# Print unique labels.
	print('Unique labels:', np.unique(kmeans6.labels_))

The output is: 

.. code-block:: python

	Unique labels: [0 1 2 3 4 5]


Now, create a new column cluster that indicates cluster assignment in the DataFrame penguins_subset. It's important to understand the meaning of each cluster's labels, then decide whether the clustering makes sense.


.. code-block:: python

 	#Create a new column `cluster`.
	penguins_subset['cluster'] = kmeans6.labels_
	penguins_subset.head()


Use groupby to verify if any 'cluster' can be differentiated by 'species':


.. code-block:: python

	# Verify if any `cluster` can be differentiated by `species`.
	penguins_subset.groupby(by=['cluster', 'species']).size()

This is the output:

.. code-block:: python

	cluster  species  
	0        Gentoo       58
	1        Adelie       73
         Chinstrap     5
	2        Adelie       71
	3        Adelie        2
         Chinstrap    34
	4        Gentoo       61
	5        Chinstrap    29


Next, interpret the groupby outputs. Although the results of the groupby show that each 'cluster' can be differentiated by 'species', it is useful to visualize these results. The graph shows that each 'cluster' can be differentiated by 'species':

.. code-block:: python

	penguins_subset.groupby(by=['cluster', 'species']).size().plot.bar(title='Clusters differentiated by species',
                                                                   figsize=(6, 5),
                                                                   ylabel='Size',
                                                                   xlabel='(Cluster, Species)',color = 'turquoise');


This is the graph:


.. figure:: PNG/65.png
   :align: center

Use groupby to verify if each 'cluster' can be differentiated by 'species' AND 'sex_MALE':

.. code-block:: python

	# Verify if each `cluster` can be differentiated by `species` AND `sex_MALE`.
	penguins_subset.groupby(by=['cluster','species', 'sex_MALE']).size().sort_values(ascending = False)

This is what we get as the output:

.. code-block:: python

	cluster  species    sex_MALE
	1        Adelie     0           73
	2        Adelie     1           71
	4        Gentoo     1           61
	0        Gentoo     0           58
	3        Chinstrap  1           34
	5        Chinstrap  0           29
	1        Chinstrap  0            5
	3        Adelie     1            2

Even though clusters 1 and 3 weren't all one species or sex, the groupby indicates that the algorithm produced clusters mostly differentiated by species and sex.

Finally, interpret the groupby outputs and visualize these results. The graph shows that each 'cluster' can be differentiated by 'species' and 'sex_MALE'. Furthermore, each cluster is mostly comprised of one sex and one species:

.. code-block:: python

	penguins_subset.groupby(by=['cluster','species','sex_MALE']).size().unstack(level = 'species', fill_value=0).plot.bar(title='Clusters differentiated by species and sex',
                                                                                                                      	figsize=(6, 5),
                                                                                                                	ylabel='Size',
                                                                                                                      	xlabel='(Cluster, Sex)')
	plt.legend(bbox_to_anchor=(1.3, 1.0))


Here is the output: 

.. figure:: PNG/66.png
   :align: center


In summary we can state that:


:math:`\bullet` Inertia and silhouette score can be used to find the optimal value of clusters.

:math:`\bullet` Clusters can find natural groupings in data.



7.5. Tree-based learning
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

This is a type a supervised ML performing classification and regression tasks. Decision trees are a flowchart-like structure that uses branching paths to predict the outcomes of events, the probability of certain outcomes, or to reach a decision.They can be used for classification problems, where a specific class or outcome is predicted—like whether or not a sports team will win a game. They can also be used for regression problems, where a continuous variable is predicted—like the price of a car.


7.5.1 Structure of a classification tree
"""""""""""""""""""""""""""""""""""""""""""""

Decision trees only resemble actual trees if you flip them upside down, because they start with the root at the top and grow downward so the “leaves” are at the bottom. Decision trees are made of nodes. Nodes are groups of samples. There are different types of nodes, depending on how they function in the tree. The first node in a decision tree is called the **root node**. The first split always comes off of the root node, which divides the samples into two new nodes based on the values they contain for a particular feature.


These two new nodes are referred to as **child nodes** of the root. A child node is any node that results from a split. The node that the child splits from is known as the **parent node**. Each of these two new child nodes in turn splits the data again, based on a new criterion. This process continues until the nodes stop splitting. The bottom-level nodes that do not split are called **leaf nodes**. All the nodes above the leaf nodes are called **decision nodes**, because they all make a decision that sorts the data either to the left or to the right:



.. figure:: PNG/67.png
   :align: center

In a decision tree, the data is split and passed down through decision nodes until reaching a leaf node. A decision node is split on the criterion that minimizes the **impurity** of the classes in their resulting children. Impurity refers to the degree of mixture with respect to class.
Nodes with low impurity have many more of one class than any other. A perfect split would have no impurity in the resulting child nodes; it would partition the data with each child containing only a single class. The worst possible split would have high impurity in the resulting child nodes; both of the child nodes would have equal numbers of each class.


When building a tree and growing a new node, a set of potential split points is generated for every predictor variable in the dataset.The feature and split point that generate the purest child nodes are selected to partition the data.To determine the set of potential split points that will be considered for a variable, the algorithm first identifies what type of variable it is—such as categorical or continuous—and the range of values that exist for that variable. 


**Categorical variables**

If the predictor variable is categorical, the decision tree algorithm will consider splitting based on category. 



**Continuous variables**

If the predictor variable is continuous, splits can be made anywhere along the range of numbers that exist in the data. Often the potential split points are determined by sorting the values for the feature and taking the mean of each consecutive pair of values.


.. note::

   There can be any number of split points, and fewer split points can be considered to save computational resources and time. 


**Choosing splits: Gini impurity**

Generally, splits are better when each resulting child node contains many more samples of one class than any other because this means the split is effectively separating the classes—the primary job of the decision tree!
In such cases, the child nodes are said to have low impurity. The decision tree algorithm determines the split that will result in the lowest impurity among the child nodes by performing a calculation.


.. math:: 
  :name: eq.200

   Gini\: impurity = \sum_{i=0}^n P_{i}^2

Where :math:`i` refers to the classes and :math:`P_{i}` is the probability of samples belonging to class i in a given node.

In an example where we want to split a node including fruits based on the color: 

.. figure:: PNG/68.png
   :align: center


We can calculate the **Gini impurity** of each child node:

:math:`\bullet` For the “red=yes” child node: Gini impurity = :math:`1-(\frac{1}{3})^2-(\frac{2}{3})^2=0.445`

:math:`\bullet` For the “red=no” child node: Gini impurity = :math:`1-(\frac{3}{4})^2-(\frac{1}{4})^2=0.375`

.. note::

   Splitting would continue until all the leaves are pure or some imposed condition stops the splitting. 



7.5.2 Hyperparameter tuning
""""""""""""""""""""""""""""""""


Depending on the characteristics of both the data and the algorithm used to model it, a model might overfit or underfit the data. Remember, the aim of a predictive model is to identify underlying, intrinsic patterns and characteristics in data that are representative of all such distributions, and use these characteristics to make predictions on new data.

**Overfitting:**

Overfitting is when the model learns the training data so closely that it captures more than the intrinsic patterns of all such data distributions. This results in a model that scores very well on the training data but considerably worse on unseen data because it cannot generalize well.

**Underfitting:**

Underfitting is when the model does not learn the patterns and characteristics of the training data well, and consequently fails to make accurate predictions on new data. It’s typically easier to identify underfitting, because the model performs poorly on both training and test data. The best models neither underfit nor overfit the data. They identify intrinsic patterns within it, but do not capture randomness or noise. 

**Hyperparameter tuning**

One way of helping to achieve this balance is through the use of hyperparameters. Hyperparameters are aspects of a model that you set before the model is trained, andthat affect how the model fits the data. They are not derived from the data itself. Hyperparameter tuning is the process of adjusting the hyperparameters to build a model that best fits the data. 

There are many different hyperparameters available to control how a decision tree grows. Each hyperparameter affects something very specific related to the growth conditions. 

:math:`\bullet` One might affect what causes a node to split 

:math:`\bullet` One might another might limit how deep the tree is allowed to grow, 

:math:`\bullet` One might change the way node purity is calculated. 


**max_depth**

max_depth defines how deep the tree is allowed to grow. The depth of the tree is the distance, measured in number of levels, from the root node to the furthest leaf node. 


.. note::
   An unrestricted decision tree will continue splitting until every leaf node contains only a single class. It’s possible for a tree to grow so deep that leaves contain just a single sample. However, this overfits the model to the training data, and the performance on the testing data would probably be much worse. A tree that is not allowed to grow deeply enough will have high bias and fail to make accurate predictions. The best decision tree models are neither too shallow nor too deep, but just right. 


**min_samples_split**

min_samples_split is the minimum number of samples that a node must have for it to split into more nodes.The greater the value you use for min_samples_split, the sooner the tree will stop growing. The minimum possible value is two, because two is the smallest number that can be divided into two separate child nodes.


**min_samples_leaf**

min_samples_leaf is similar to min_samples_split, but with an important difference. Instead of defining how many samples the parent node must have before splitting, min_samples_leaf defines the minimum number of samples that must be in each child node after the parent splits.


**Grid search**


A grid search is a technique that will train a model for every combination of preset ranges of hyperparameter values. The aim is to find the combination of values that results in a model that both fits the training data well and generalizes well enough to predict accurately on unseen data.

.. note::

   With more hyperparameters and a more expansive array of values to search over, grid searches can quickly become computationally expensive. One helpful search strategy is to try a wider array of values for each hyperparameter


7.5.3 Model validation
""""""""""""""""""""""""""""""""

Fitting a model to training data and evaluating it on test data might be an adequate way of evaluating how well a single model generalizes to new data, but it’s not a recommended way to compare multiple models to determine which one is best. That’s because, by selecting the model that performs best on the test data, you never get a truly objective measure of future performance. The measure would be optimistic.Usually the test data is used to select a final model. However, there are better, more rigorous ways of evaluating models and selecting the best!

One such way is through a process called validation. Model validation is the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to better understand its strengths and limitations. 

**Validation sets**

The simplest way to maintain the objectivity of the test data is to create another partition in the data—a validation set—and save the test data for after you select the final model. The validation set is then used, instead of the test set, to compare different models. 

Here is one common way of splitting data, but note that these proportions are not required. You can split to whichever ratios make the most sense for your use case.

.. figure:: PNG/69.png
   :align: center

This method—using a separate validation set to compare models—is most commonly used when you have a very large dataset. The reason for this is that the more data you use for validation, the less you have for training and testing. However, if you don’t have enough validation data, then your models’ scores cannot be expected to give a reliable measure that you can use to select a model, because there’s a greater chance that the distributions in the validation data are not representative of those in the entire dataset. 


**Cross validation**

There is another approach to model validation that avoids having to split the data into three partitions (train / validate / test) in advance. Cross-validation makes more efficient use of the training data by splitting the training data into k number of “folds” (partitions), training a model on (k – 1) folds, and using the fold that was held out to get a validation score. The training process occurs k times, each time using a different fold as the validation set. At the end, the final validation score is the average of all k scores. This process is also commonly referred to as **k-fold cross validation**.

.. note::

   The cross-validation process maximizes the usefulness of your data with the goal of getting a more accurate measure of model performance. The more folds you use, the more thorough the validation. However, adding folds increases the time needed to train, and may not be useful beyond a certain point.

**How to select the best model**

Once you’ve trained and validated your candidate models, it’s time to select a champion. Of course, your models’ validation scores factor heavily into this decision, but score is seldom the only criterion.Often you’ll need to consider other factors too.

.. note::

   It’s not uncommon for a model with a slightly lower validation score to be selected over the highest-scoring model due to it being simpler, less computationally expensive, or more stable. 


Once you have selected a champion model, it’s time to evaluate it using the test data. The test data is used only for this final model. Your model’s score on this data is how you can expect the model to perform on completely new data. 

**Conclusion**

A rigorous approach to model development might use both cross-validation and validation. The cross-validation can be used to tune hyperparameters, while the separate validation set lets you compare the scores of different algorithms (e.g., logistic regression vs. Naive Bayes vs. decision tree) to select a champion model. Finally, the test set gives you a benchmark score for performance on new data.


7.5.4 Build a decision tree in Python
"""""""""""""""""""""""""""""""""""""""""

We want to build a decision tree model that makes predictions for a target based on multiple features. We have data from an airline which is interested in predicting whether a future customer would be satisfied with their services given customer feedback given previous customer feedback about their flight experience. In addition, we are interested in knowing which features are most important to customer satisfaction.

The data for this activity includes survey responses from 129,880 customers. It includes data points such as class, flight distance, and in-flight entertainment, among others.

Because this activity uses a dataset from the industry, you will need to conduct basic EDA, data cleaning, and other manipulations to prepare the data for modeling.

As usual, we need to import the required packages:


.. code-block:: python

	# Standard operational package imports
	import numpy as np
	import pandas as pd

	# Important imports for modeling and evaluation
	from sklearn.model_selection import train_test_split
	from sklearn.model_selection import GridSearchCV
	from sklearn.tree import DecisionTreeClassifier
	from sklearn.tree import plot_tree
	import sklearn.metrics as metrics

	# Visualization package imports
	import matplotlib.pyplot as plt
	import seaborn as sns


Then we need to load the dataset:


.. code-block:: python

	# RUN THIS CELL TO IMPORT YOUR DATA.
	df_original = pd.read_csv("Invistico_Airline.csv")

After loading the dataset, prepare the data to be suitable for decision tree classifiers. This includes:

:math:`\bullet` Exploring the data

:math:`\bullet` Checking for missing values

:math:`\bullet` Encoding the data

:math:`\bullet` Renaming a column

:math:`\bullet` Creating the training and testing data


We can see the types of the classes listed under **Class** column: 


.. code-block:: python

	df_original["Class"].unique()

In order to predict customer satisfaction, verify if the dataset is imbalanced. To do this, check the counts of each of the predicted labels:


.. code-block:: python

	df_original['satisfaction'].value_counts(dropna = False)

Here is the output:

.. code-block:: python

	satisfied       71087
	dissatisfied    58793

There are 71087 satisfied customers and 58793 dissatisfied customers.

The sklearn decision tree implementation does not support missing values. Check for missing values in the rows of the data:

.. code-block:: python

	df_original.isnull().sum()

This is important to check because if there are only a small number of missing values in the dataset, they can more safely be removed.

Now, we should drop the rows with missing values and save the resulting pandas DataFrame in a variable named df_subset:


.. code-block:: python

	df_subset = df_original.dropna(axis=0).reset_index(drop = True)

Four columns (satisfaction, Customer Type, Type of Travel, Class) are the pandas dtype object. Decision trees need numeric columns. Start by converting the ordinal Class column into numeric:



.. code-block:: python

	df_subset['Class'] = df_subset['Class'].map({"Business": 3, "Eco Plus": 2, "Eco": 1})

To represent the data in the **target variable** numerically, assign "satisfied" to the label 1 and "unsatisfied" to the label 0 in the satisfaction column:


.. code-block:: python

	df_subset['satisfaction'] = df_subset['satisfaction'].map({"satisfied": 1, "dissatisfied": 0})


There are other columns in the dataset that are still categorical. Be sure to convert categorical columns in the dataset into numeric:


.. code-block:: python

	df_subset = pd.get_dummies(df_subset, drop_first = True)


**Create the training and testing data**


Put 75% of the data into a training set and the remaining 25% into a testing set:


.. code-block:: python

	y = df_subset["satisfaction"]

	X = df_subset.copy()
	X = X.drop("satisfaction", axis = 1)

	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)

**Fit a decision tree classifier model to the data**


Make a decision tree instance called decision_tree and pass in 0 to the random_state parameter. This is only so that if other data professionals run this code, they get the same results. Fit the model on the training set, use the predict() function on the testing set, and assign those predictions to the variable dt_pred:


.. code-block:: python

	decision_tree = DecisionTreeClassifier(random_state=0)
	decision_tree.fit(X_train, y_train)
	dt_pred = decision_tree.predict(X_test)


.. note::

   Decision trees require no assumptions regarding the distribution of underlying data and don't require scaling of features.


**Results and evaluation**

Print out the decision tree model's accuracy, precision, recall, and F1 score:


.. code-block:: python

	print("Decision Tree")
	print("Accuracy:", "%.6f" % metrics.accuracy_score(y_test, dt_pred))
	print("Precision:", "%.6f" % metrics.precision_score(y_test, dt_pred))
	print("Recall:", "%.6f" % metrics.recall_score(y_test, dt_pred))
	print("F1 Score:", "%.6f" % metrics.f1_score(y_test, dt_pred))

Here is the output:

.. code-block:: python

	Decision Tree
	Accuracy: 0.935438
	Precision: 0.942859
	Recall: 0.939030
	F1 Score: 0.940940


Decision trees can be particularly susceptible to overfitting. Combining hyperparameter tuning and grid search can help ensure this doesn't happen. For instance, setting an appropriate value for max depth could potentially help reduce a decision tree's overfitting problem by limiting how deep a tree can grow.

**Confusion matrix**

We like to know the types of errors made by an algorithm. To obtain this information, produce a confusion matrix:


.. code-block:: python

	cm = metrics.confusion_matrix(y_test, dt_pred, labels = decision_tree.classes_)
	disp = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels = decision_tree.classes_)
	disp.plot()

Here is the output:


.. figure:: PNG/70.png
   :align: center


Based on the above confusion matrix, there are a high proportion of true positives and true negatives (where the matrix accurately predicted that the customer would be satisfied or dissatisfied, respectively).

The matrix also had a relatively low number of false positives and false negatives (where the matrix inaccurately predicted that the customer would be satisfied or dissatisfied, respectively.)


**Plot the decision tree**


Now we can examine the decision tree. Use plot_tree function to produce a visual representation of the tree to pinpoint where the splits in the data are occurring:

.. code-block:: python

	plt.figure(figsize=(20,12))
	plot_tree(decision_tree, max_depth=2, fontsize=14, feature_names=X.columns,filled=True);

Here is the output:

.. figure:: PNG/71.png
   :align: center


**Build a feature importance graph**

We can uncover which features might be most important to your decision tree model by building a feature importance graph:

.. code-block:: python

	importances = decision_tree.feature_importances_

	forest_importances = pd.Series(importances, index=X.columns)

	fig, ax = plt.subplots()
	forest_importances.plot.bar(ax=ax,color='m')

Here is the graph: 

.. figure:: PNG/72.png
   :align: center


The feature importance graph seems to confirm that 'Inflight entertainment', 'Seat comfort', and 'Ease of Online booking' are the most important features for this model.



**Hyperparameter tuning**

We will find the best values for the hyperparameters max_depth and min_samples_leaf using grid search and cross validation. Below are some values for the hyperparameters max_depth and min_samples_leaf:

.. code-block:: python

	tree_para = {'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40,50],
             'min_samples_leaf': [2,3,4,5,6,7,8,9, 10, 15, 20, 50]}

	scoring = {'accuracy', 'precision', 'recall', 'f1'}


**Check combinations of values**

Check every combination of values to examine which pair has the best evaluation metrics. 

:math:`\bullet` Make a decision tree instance called tuned_decision_tree with random_state=0

:math:`\bullet` make a GridSearchCV instance called clf

:math:`\bullet` make sure to refit the estimator using "f1", and fit the model on the training set


The above 3 steps could be coded as below:

.. code-block:: python

	tuned_decision_tree = DecisionTreeClassifier(random_state=0)

	clf = GridSearchCV(tuned_decision_tree, 
                   	   tree_para, 
                  	   scoring = scoring, 
                   	   cv=5, 
                   	   refit="f1")

	clf.fit(X_train, y_train)


Now we can use the best estimator tool to help uncover the best pair combination:


.. code-block:: python

	clf.best_estimator_

This is the output: 

.. code-block:: python

	DecisionTreeClassifier(max_depth=18, min_samples_leaf=2, random_state=0)

After running the DecisionTreeClassifier, the maximum depth is 18 and the minimum number of samples is two, meaning this is the best combination of values.

**Determine the "best" decision tree scores**

We can print out the decision tree model's accuracy, precision, recall, and F1 score:


.. code-block:: python

	results = pd.DataFrame(columns=['Model', 'F1', 'Recall', 'Precision', 'Accuracy'])

	def make_results(model_name, model_object):
    	"""
    	Accepts as arguments a model name (your choice - string) and
    	a fit GridSearchCV model object.

    	Returns a pandas df with the F1, recall, precision, and accuracy scores
    	for the model with the best mean F1 score across all validation folds.  
    	"""

            # Get all the results from the CV and put them in a df.
            cv_results = pd.DataFrame(model_object.cv_results_)

            # Isolate the row of the df with the max(mean f1 score).
            best_estimator_results = cv_results.iloc[cv_results['mean_test_f1'].idxmax(), :]

            # Extract accuracy, precision, recall, and f1 score from that row.
            f1 = best_estimator_results.mean_test_f1
            recall = best_estimator_results.mean_test_recall
            precision = best_estimator_results.mean_test_precision
            accuracy = best_estimator_results.mean_test_accuracy

            # Create a table of results.
            table = pd.DataFrame()
            table = table.append({'Model': model_name,
                                  'F1': f1,
                                  'Recall': recall,
                                  'Precision': precision,
                                  'Accuracy': accuracy},
                                 ignore_index=True)

            return table

	result_table = make_results("Tuned Decision Tree", clf)

	result_table

This is the output:

.. figure:: PNG/73.png
   :align: center

**Conclusions**

:math:`\bullet` Precision measures what proportion of predicted positives is truly positive. For example, if you wanted to not falsely claiming a customer is satisfied, precision would be a good metric. Assuming a customer is happy when they are really not might lead to customer churn.

:math:`\bullet` Recall measures the percentage of actual positives a model correctly identified (true positive). For this dataset, the airline might want to limit false negatives (actually satisfied people who are predicted to be unsatisfied). Assuming a customer is unhappy when the customer is happy can lead to the airline wasting resources trying to improve the customer experience of an already happy customer.


:math:`\bullet` F1 balances precision and recall. It is the harmonic mean of precision and recall, or their product divided by their sum.

:math:`\bullet` Decision trees accurately predicted satisfaction over 94 percent of the time.

:math:`\bullet` The confusion matrix is useful as it shows a similar number of true positives and true negatives.


7.5.5 Bagging 
"""""""""""""""

First we need to define two concepts including the **ensemble learning** and **base learner**:

**Ensemble learning**

Building multiple models and aggregating their predictions

**Base learner**

Each individual model that comprises an ensemble


The ensembles of base learners can combine to become powerful predictors. You learned about bagging,  and that it’s one of the more commonly used modeling strategies. 

**Bagging**

Bagging includes bootstrapping and aggregating, 

**Bootstrapping**

The bootstrapping refers to sampling with replacement. In ensemble modeling architectures, this means that for each base learner, the same observation can and will be sampled multiple times.Suppose you have a dataset of 1,000 observations, and you bootstrap sample it to generate a new dataset of 1,000 observations, on average, you should find about 632 of those observations in your sampled dataset (~63.2%).


**Aggregating**

Building a single model with bootstrapped data probably wouldn’t be very useful. To use the example above, if you start with 1,000 unique observations and use bootstrapping to create a sampled dataset of 1,000 observations, you’d only expect to get an average of 632 unique observations in that new dataset. This means that you’d lose whatever information was contained in the 368 observations that didn’t make it into the new sampled dataset.

This is when ensemble learning—or ensembling is helpful. In this example, those 368 observations might not make it into that particular sampled dataset, but if you keep repeating the bootstrapping process —once for each base learner—eventually your overall ensemble of base learners will see all of the observations. 


There are 3 main advantages of bagging:


**1.Reduces variance:** Standalone models can result in high variance. Aggregating base models’ predictions in an ensemble help reduce it.

**2.Fast:** Training can happen in parallel across CPU cores and even across different servers.

**3.Good for big data:** Bagging doesn’t require an entire training dataset to be stored in memory during model training. You can set the sample size for each bootstrap to a fraction of the overall data, train a base learner, and string these base learners together without ever reading in the entire dataset all at once. 



7.6. Random Forest
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Random forest is domination of **Bagging** and **Random Feature Sampling**

You know that bootstrap aggregating—or bagging—can be an effective way to make predictions by building many base learners that are each trained on bootstrapped data and then combining their results. If you build a bagging ensemble of decision trees but take it one step further by randomizing the features used to train each base learner, the result is called a **Random Forest**.

.. note::

   Random forest models uses randomness to reduce the likelihood that a given base learner will make the same mistakes as other base learners. 


To illustrate this, consider a dataset with five observations: 1, 2, 3, 4, and 5. If you were to create a new, bootstrapped dataset of five observations from this original data, it might look like 1, 1, 3, 5, 5. It’s still five observations long, but some observations are missing and some are counted twice. The result is that the base learners are trained on data that is randomized by observation.

Random forest goes further. It randomizes the data by features too. This means that if there are five available features: A, B, C, D, and E, you can set the model to only sample from a subset of them. In other words, each base learner will only have a limited number of features available to it, but what those features are will vary between learners.


.. note::

   This is possible for a model to perform better even with less bootstrap sample size and less features samples. 

7.6.1 Build a random forest model in Python
""""""""""""""""""""""""""""""""""""""""""""""


Here we want to build a random forest model using the data of the airline. We will train, tune, and evaluate a random forest model using data from spreadsheet of survey responses from 129,880 customers. It includes data points such as class, flight distance, and inflight entertainment. The random forest model will be used to predict whether a customer will be satisfied with their flight experience.

First, we need to import the relevant python libraries: 


.. code-block:: python


	# Import `numpy`, `pandas`, and `sklearn`.
	# Import the relevant functions from `sklearn.ensemble`, `sklearn.model_selection`, and `sklearn.metrics`.

 
	import numpy as np
	import pandas as pd 
	from sklearn.ensemble import RandomForestClassifier
	from sklearn.model_selection import train_test_split, PredefinedSplit, GridSearchCV
	from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score

Next we should import the data:


.. code-block:: python

	# RUN THIS CELL TO IMPORT YOUR DATA. 
	air_data = pd.read_csv("Invistico_Airline.csv")

**Data cleaning**

Now, we should start cleaning the data.We should check for missing values in the rows of the data. Start with .isna() to get Booleans indicating whether each value in the data is missing. Then, use .any(axis=1) to get Booleans indicating whether there are any missing values along the columns in each row. Finally, use .sum() to get the number of rows that contain missing values.

.. code-block:: python

	# Get Booleans to find missing values in data.
	# Get Booleans to find missing values along columns.
	# Get the number of rows that contain missing values.

	air_data.isna().any(axis=1).sum()

Now we can see that there are 393 rows with missing values. Drop the rows with missing values. This is an important step in data cleaning, as it makes the data more useful for analysis and regression. Then, save the resulting pandas DataFrame in a variable named air_data_subset:

.. code-block:: python

	# Drop missing values.
	# Save the DataFrame in variable `air_data_subset`.

	air_data_subset = air_data.dropna(axis=0)

Now we are sure the new dataset does not contain any missing values.

Next, convert the categorical features to indicator (one-hot encoded) features (the target variable, satisfaction, does not need to be encoded and will be extracted in a later step):


.. code-block:: python

	# Convert categorical features to one-hot encoded features.

	air_data_subset_dummies = pd.get_dummies(air_data_subset, 
                                         columns=['Customer Type','Type of Travel','Class'])



**Model building**

The first step to building your model is separating the labels (y) from the features (X):


.. code-block:: python

	# Separate the dataset into labels (y) and features (X).

	y = air_data_subset_dummies["satisfaction"]
	X = air_data_subset_dummies.drop("satisfaction", axis=1)

Once separated, split the data into train, validate, and test sets:

.. code-block:: python

	# Separate into train, validate, test sets.

	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)
	X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 0)

Now, fit and tune a random forest model with separate validation set. Begin by determining a set of hyperparameters for tuning the model using GridSearchCV:


.. code-block:: python

	# Determine set of hyperparameters.

	cv_params = {'n_estimators' : [50,100], 
              'max_depth' : [10,50],        
              'min_samples_leaf' : [0.5,1], 
              'min_samples_split' : [0.001, 0.01],
              'max_features' : ["sqrt"], 
              'max_samples' : [.5,.9]}

Next, create a list of split indices.

.. code-block:: python

	# Create list of split indices.

	split_index = [0 if x in X_val.index else -1 for x in X_train.index]
	custom_split = PredefinedSplit(split_index)



Now, instantiate your model:

.. code-block:: python

	rf = RandomForestClassifier(random_state=0)

Next, use GridSearchCV to search over the specified parameters:

.. code-block:: python

	# Search over specified parameters.

	rf_val = GridSearchCV(rf, cv_params, cv=custom_split, refit='f1', n_jobs = -1, verbose = 1)


Now, fit the model:

.. code-block:: python

	# Fit the model.

	rf_val.fit(X_train, y_train)

Here is the output of the above lines:

.. code-block:: python

	Fitting 1 folds for each of 32 candidates, totalling 32 fits
	CPU times: user 3.39 s, sys: 234 ms, total: 3.62 s
	Wall time: 17.7 s
	GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),
             	estimator=RandomForestClassifier(random_state=0), n_jobs=-1,
             	param_grid={'max_depth': [10, 50], 'max_features': ['sqrt'],
                         	'max_samples': [0.5, 0.9],
                         	'min_samples_leaf': [0.5, 1],
                         	'min_samples_split': [0.001, 0.01],
                         	'n_estimators': [50, 100]},
             	refit='f1', verbose=1)

Finally, obtain the optimal parameters:

.. code-block:: python

	# Obtain optimal parameters.

	rf_val.best_params_

Here is the output:

.. code-block:: python

	{'max_depth': 50,
 	'max_features': 'sqrt',
 	'max_samples': 0.9,
 	'min_samples_leaf': 1,
 	'min_samples_split': 0.001,
 	'n_estimators': 50}

**Results and evaluation**

We should use the selected model to predict on your test data. Use the optimal parameters found via GridSearchCV:

.. code-block:: python

	# Use optimal parameters on GridSearchCV.

	rf_opt = RandomForestClassifier(n_estimators = 50, max_depth = 50, 
                                	min_samples_leaf = 1, min_samples_split = 0.001,
                                	max_features="sqrt", max_samples = 0.9, random_state = 0)

Once again, fit the optimal model:


.. code-block:: python

	# Fit the optimal model.

	rf_opt.fit(X_train, y_train)

And predict on the test set using the optimal model:


.. code-block:: python

	# Predict on test set.

	y_pred = rf_opt.predict(X_test)


**Obtain performance scores**

**Evaluate the model**

Now that we have results, evaluate the model.There are the four basic parameters for evaluating the performance of a classification model:

	**1.** True positives (TP): These are correctly predicted positive values, which means the value of actual and predicted classes are positive.

	**2.** True negatives (TN): These are correctly predicted negative values, which means the value of the actual and predicted classes are negative.

	**3.** False positives (FP): This occurs when the value of the actual class is negative and the value of the predicted class is positive.

	**4.** False negatives (FN): This occurs when the value of the actual class is positive and the value of the predicted class in negative.

.. note::

	The goal is to minimize false positives and false negatives.


We review different types of scores again here: 

:math:`\bullet` **Accuracy:** (TP+TN/TP+FP+FN+TN): The ratio of correctly predicted observations to total observations.

:math:`\bullet` **Precision:** (TP/TP+FP): The ratio of correctly predicted positive observations to total predicted positive observations.

:math:`\bullet` **Recall:** (Sensitivity, TP/TP+FN): The ratio of correctly predicted positive observations to all observations in actual class.

:math:`\bullet` **F1 score:** The harmonic average of precision and recall, which takes into account both false positives and false negatives. 


.. code-block:: python

	# Get precision score.

	pc_test = precision_score(y_test, y_pred, pos_label = "satisfied")
	print("The precision score is {pc:.3f}".format(pc = pc_test))


Here is the output:

.. code-block:: python

	The precision score is 0.950


Then, collect the recall score:


.. code-block:: python

	# Get recall score.

	rc_test = recall_score(y_test, y_pred, pos_label = "satisfied")
	print("The recall score is {rc:.3f}".format(rc = rc_test))


Here is the output:

.. code-block:: python

	The recall score is 0.945

Next, obtain the accuracy score:


.. code-block:: python

	# Get accuracy score.

	ac_test = accuracy_score(y_test, y_pred)
	print("The accuracy score is {ac:.3f}".format(ac = ac_test))

Here is the output:

.. code-block:: python

	The accuracy score is 0.942

Finally, collect your F1-score:


.. code-block:: python

	# Get F1 score.

	f1_test = f1_score(y_test, y_pred, pos_label = "satisfied")
	print("The F1 score is {f1:.3f}".format(f1 = f1_test))

Here is the output:

.. code-block:: python

	The F1 score is 0.947

.. note::

   :math:`\bullet` The tuned random forest has higher scores overall, so it is the better model. Particularly, it shows a better F1 score than the decision tree model, which indicates that the random forest model may do better at classification when taking into account false positives and false negatives

   :math:`\bullet` A separate validation set is typically used for tuning a model, rather than using the test set. This also helps avoid the evaluation becoming biased.

As a conclusion, the random forest model yields a more effective performance than a decision tree model.



