

**6. Linear Regression**
================================================

.. note:: 

    All of required CSV files are available in the folder: **Linear Regression** 

6.1. Simple linear regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Using linear regression we define a linear relationship between a dependent variable (Y) and an independent variable (X): 

.. math:: 
  :name: eq.113

   Y= aX+b

In above equation, :math:`a` and :math:`b` are the slope and intercept parameters. 


The purpose of using linear regression is to explore the relationship between two continuous variables. To this end, we need to perform a complete simple linear regression analysis. Here are the main steps:

1. Creating and fitting a model, 
2. Checking model assumptions, 
3. Analyzing model performance, 
4. Interpreting model coefficients,

We have a dataset about influencer marketing. We want to  explore the relationship between marketing promotional budgets and sales. The dataset provided includes information about marketing campaigns across TV, radio, and social media, as well as how much revenue in sales was generated from these campaigns. Based on this information, we want to make decisions about where to focus future marketing efforts, so it is critical to have a clear understanding of the relationship between the different types of marketing and the revenue they generate.

First we should import required Python libraries including: pandas, pyplot from matplotlib, and seaborn.In addition Import the statsmodels.api Python module using its common abbreviation, sm, along with the ols() function from statsmodels.formula.api:


.. code-block:: python

	import pandas as pd
	import matplotlib.pyplot as plt
	import seaborn as sns

	# Import the statsmodel module.
	import statsmodels.api as sm

	# Import the ols function from statsmodels.
	from statsmodels.formula.api import ols




In the next step we need to load the dataset (modified_marketing_and_sales_data.csv):


.. code-block:: python

	data = pd.read_csv('modified_marketing_and_sales_data.csv')


The features in the data are:

:math:`\bullet` TV promotion budget (in millions of dollars)

:math:`\bullet` Social media promotion budget (in millions of dollars)

:math:`\bullet` Radio promotion budget (in millions of dollars)

:math:`\bullet` Sales (in millions of dollars)

Each row corresponds to an independent marketing promotion where the business invests in TV, Social_Media, and Radio promotions to increase Sales. We want to determine which feature most strongly predicts Sales so they have a better understanding of what promotions they should invest in in the future. To accomplish this, you'll construct a simple linear regression model that predicts sales using a single independent variable.


.. note::

   Several investigations are required before making the regression models including: 

   :math:`\bullet` Understanding which variables are present in the data

   :math:`\bullet` Reviewing the distribution of features, such as minimum, mean, and maximum values

   :math:`\bullet` Plotting the relationship between the independent and dependent variables to visualize which feature is the best choice for X

   :math:`\bullet` Identifying issues with the data, such as incorrect values (e.g., typos) or missing values



There are three continuous independent variables: TV, Radio, and Social_Media. To understand how heavily the business invests in each promotion type, use describe() to generate descriptive statistics for these three variables:


.. code-block:: python

	data[['TV','Radio','Social_Media']].describe()


Before fitting the model, ensure the Sales for each promotion (i.e., row) is present. If the Sales in a row is missing, that row isn't of much value to the simple linear regression model.

.. code-block:: python

	# Calculate the average missing rate in the sales column.
	missing_sales = data.Sales.isna().mean()

	# Convert the missing_sales from a decimal to a percentage and round to 2 decimal places.
	missing_sales = round(missing_sales*100, 2)

	# Display the results (missing_sales must be converted to a string to be concatenated in the print statement).
	print('Percentage of promotions missing Sales: ' +  str(missing_sales) + '%')



Here is the output: 


.. code-block:: python

	Percentage of promotions missing Sales: 0.13%

Next step we should remove the missing data meaning that the rows that do not contain any Sale value: 


.. code-block:: python

	# Subset the data to include rows where Sales is present.
	data = data.dropna(subset = ['Sales'], axis = 0)

Then we can visualize the distribution of the Sale value in a histogram:

.. code-block:: python

	fig = sns.histplot(data['Sales'],color='c')

	# Add a title
	fig.set_title('Distribution of Sales');

Here is the graph:


.. figure:: PNG/29.png
   :align: center

Now we can build the model. Create a pairplot to visualize the relationships between pairs of variables in the data. You will use this to visually determine which variable has the strongest linear relationship with Sales. This will help you select the X variable for the simple linear regression.

This could be done using this line of code: 

.. code-block:: python

	sns.pairplot(data);

Here is the graph:


.. figure:: PNG/30.png
   :align: center

TV clearly has the strongest linear relationship with Sales. You could draw a straight line through the scatterplot of TV and Sales that confidently estimates Sales using TV.

.. note::

   Radio and Sales appear to have a linear relationship, but there is larger variance than between TV and Sales.



.. code-block:: python

	sns.pairplot(data);


Now we can use the **Ordinary Least Squares** function to fit the model and then see the summary of the results:

.. code-block:: python

	# Define the OLS formula.
	ols_formula = 'Sales ~ TV'

	# Create an OLS model.
	OLS = ols(formula = ols_formula, data = data)

	# Fit the model.
	model = OLS.fit()

	# Save the results summary.
	model_results = model.summary()

	# Display the model results.
	model_results


Please note that when using simple linear regression, we should check four linear regression assumptions including:

1. Linearity
2. Independent Observations
3. Normality
4. Homoscedasticity (Constant variance)


**1. Linearity**

To check the linearity, we need create a scatter plot comparing the X variable (independent variable:TV) with the dependent variable (Sales). 

.. code-block:: python

	sns.scatterplot(x = data['TV'], y = data['Sales']);

Here is the output: 

.. figure:: PNG/31.png
   :align: center


There is a clear linear relationship between `TV` and `Sales`, meeting the linearity assumption.


**2. Independence**

The independent observation assumption states that each observation in the dataset is independent. In our data, each observation (Each row) is independent from one another, the independence assumption is not violated.


**3. Normality**

The normality assumption states that the errors are normally distributed. There are two ways to check the normality: 


:math:`\bullet` Histogram of the residuals. (Differences between the actual and predicted values)

:math:`\bullet` Q-Q plot of the residuals

.. code-block:: python


	residuals = model.resid

	# Create a 1x2 plot figure.
	fig, axes = plt.subplots(1, 2, figsize = (8,4))

	# Create a histogram with the residuals .
	sns.histplot(residuals, ax=axes[0], color='r')

	# Set the x label of the residual plot.
	axes[0].set_xlabel("Residual Value")

	# Set the title of the residual plot.
	axes[0].set_title("Histogram of Residuals")

	# Create a Q-Q plot of the residuals.
	sm.qqplot(residuals, line='s',ax = axes[1])

	# Set the title of the Q-Q plot.
	axes[1].set_title("Normal Q-Q plot")

	# Use matplotlib's tight_layout() function to add space between plots for a cleaner appearance.
	plt.tight_layout()

	# Show the plot.
	plt.show()


Here is the output:


.. figure:: PNG/32.png
   :align: center


The histogram of the residuals are approximately normally distributed, which supports that the normality assumption is met for this model.The residuals in the Q-Q plot form a straight line, further supporting that the normality assumption is met.

**3. Homoscedasticity**

The homoscedasticity (constant variance) assumption is that the residuals have a constant variance for all values of X.

Check that this assumption is not violated by creating a scatterplot with the fitted values and residuals. Add a line at  ùë¶=0 to visualize the variance of residuals above and below  ùë¶=0.


.. code-block:: python

	# Create a scatterplot with the fitted values from the model and the residuals.
	fig = sns.scatterplot(x = model.fittedvalues, y = model.resid)

	# Set the x-axis label.
	fig.set_xlabel("Fitted Values")

	# Set the y-axis label.
	fig.set_ylabel("Residuals")

	# Set the title.
	fig.set_title("Fitted Values v. Residuals")

	# Add a line at y = 0 to visualize the variance of residuals above and below 0.
	fig.axhline(0)

	# Show the plot.
	plt.show()


This is the output of the above lines: 

.. figure:: PNG/33.png
   :align: center


The variance of the residuals is constant across all ùëã. Thus, the assumption of homoscedasticity is met.


Display the OLS regression results from the fitted model object, which includes information about the dataset, model fit, and coefficients by running this line:


.. code-block:: python

	# Display the model_results defined above.
	model_results

Here is the output:

.. figure:: PNG/34.png
   :align: center

The R-squared on the top right of the output above measures the proportion of variation in the dependent variable (Y) explained by the independent variable (X). Using TV as X results in a simple linear regression model with  ùëÖ2=0.999. In other words, TV explains  99.9% of the variation in Sales.
It should be noted that The R-squared value will depend on the variable selected for X.


When TV is used as the independent variable X, the coefficient for the Intercept is -0.1263 and the coefficient for TV is 3.5614.When TV is used as the independent variable X, the linear equation is:

ùëå=Intercept+Slope‚àóùëã
 
Sales (in millions)=Intercept+Slope‚àóTV (in millions)
Sales (in millions)=‚àí0.1263+3.5614‚àóTV (in millions)


With regards to the above equation we can conclude when TV is used as the independent variable X, an increase of one million dollars for the TV promotional budget results in an estimated 3.5614 million dollars more in sales.


In addition, model coefficients are estimated. This means there is an amount of uncertainty in the estimate. A p-value and  95% confidence interval are provided with each coefficient to quantify the uncertainty for that coefficient estimate.


.. note::

	When TV is used as the independent variable, it has a p-value of  0.000 and a  95% confidence interval of [3.558,3.565]. This means there is a  95% chance the interval  [3.558,3.565] contains the true parameter value of the slope. These results indicate little uncertainty in the estimation of the slope of X. Therefore, the business can be confident in the impact TV has on Sales.


Finally we can conclude that: 

:math:`\bullet` The linear regression model estimates that 99.9% of the variation in sales is explained by the TV promotional budget. In other words, nearly all of the variation in sales can be explained by the TV promotional budget alone, making TV an excellent predictor of sales.

:math:`\bullet` Among the three available promotion types (TV, radio, and social media), TV has the strongest positive linear relationship with sales. According to the model, an increase of one million dollars for the TV promotional budget will result in an estimated 3.5614 million dollars more in sales. This is a very confident estimate, as the p-value for this coefficient estimate is small. Thus, the business should prioritize increasing the TV promotional budget over the radio and social media promotional budgets to increase sales.


:math:`\bullet` The interval (3.558 million, 3.565 million) has a 95% probability of containing the true estimate of the increase in sales for a one million dollar increase in the TV promotional budget. Therefore, the estimate provided in the previous bullet is very confident.


6.2. Multiple linear regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


multiple linear regression helps you estimate the linear relationship between one continuous dependent variable and two or more independent variables. This is a useful skill because it allows you to compare more than one variable to the variable you're measuring.

In this new data sheet, each row corresponds to an independent marketing promotion where their business uses TV, social media, radio, and influencer promotions to increase sales. We want to conduct a multiple linear regression analysis to estimate sales from a combination of independent variables. 


As usual, we need to import the required packages in the first step:

.. code-block:: python

	import pandas as pd
	import matplotlib.pyplot as plt
	import seaborn as sns
	import statsmodels.api as sm
	from statsmodels.formula.api import ols


Then we load the data: 



.. code-block:: python

	# Load the data.
	data = pd.read_csv('marketing_sales_data.csv')


The features in the data are:

:math:`\bullet` TV promotional budget (in "Low," "Medium," and "High" categories)

:math:`\bullet` Social media promotional budget (in millions of dollars)

:math:`\bullet` Radio promotional budget (in millions of dollars)

:math:`\bullet` Sales (in millions of dollars)

:math:`\bullet` Influencer size (in "Mega," "Macro," "Nano," and "Micro" categories)


The initial actions we need to take include:

:math:`\bullet` Understanding which variables are present in the data

:math:`\bullet` Reviewing the distribution of features, such as minimum, mean, and maximum values

:math:`\bullet` Plotting the relationship between the independent and dependent variables to visualize which features have a linear relationship

:math:`\bullet` Identifying issues with the data, such as incorrect values (e.g., typos) or missing values


We can create a pairplot to visualize the relationship between the continuous variable

.. code-block:: python

	# Create a pairplot of the data.
	sns.pairplot(data);


Here is the out put: 

.. figure:: PNG/35.png
   :align: center


According to the above graph, Radio and Social Media both appear to have linear relationships with Sales. Given this, Radio and Social Media may be useful as independent variables in a multiple linear regression model estimating Sales.

There are two categorical variables: TV and Influencer. To characterize the relationship between the categorical variables and Sales, find the mean Sales for each category in TV and the mean Sales for each category in Influence.


.. code-block:: python

	# Calculate the mean sales for each TV category. 
	print(data.groupby('TV')['Sales'].mean())

	print('')

	# Calculate the mean sales for each Influencer category .
	print(data.groupby('Influencer')['Sales'].mean())


This is the output: 

.. figure:: PNG/36.png
   :align: center


The average Sales for High TV promotions is considerably higher than for Medium and Low TV promotions. TV may be a strong predictor of Sales. However, the categories for Influencer have different average Sales, but the variation is not substantial. Influencer may be a weak predictor of Sales.These results can be investigated further when fitting the multiple linear regression model.


This dataset contains rows with missing values. To correct this, drop all rows that contain missing data.


.. code-block:: python

	# Drop rows that contain missing data and update the DataFrame.
	data = data.dropna(axis=0)


.. note::

	The ols() function doesn't run when variable names contain a space. Check that the column names in data do not contain spaces and fix them, if needed.


.. code-block:: python

	# Rename all columns in data that contain a space. 
	data = data.rename(columns={'Social Media': 'Social_Media'})

Now we can fit a multiple linear regression model that predicts sales:


.. code-block:: python


	# Define the OLS formula.
	ols_formula = 'Sales ~ C(TV) + Radio'
	OLS = ols(formula = ols_formula, data = data)

	# Fit the model.
	model = OLS.fit()

	# Save the results summary.
	model_results = model.summary()

	# Display the model results.
	model_results

This is the output: 

.. figure:: PNG/37.png
   :align: center


The most important conclusions could be listed as below: 

:math:`\bullet` TV was selected, as the analysis above showed a strong relationship between the TV promotional budget and the average Sales.

:math:`\bullet` Radio was selected because the pairplot showed a strong linear relationship between Radio and Sales.

:math:`\bullet` Social Media was not selected because it did not increase model performance and it was later determined to be correlated with another independent variable: Radio.

:math:`\bullet` Influencer was not selected because it did not show a strong relationship to Sales in the analysis above.


.. note::

	For multiple linear regression, there is an additional assumption added to the four simple linear regression assumptions: multicollinearity.

The assumption for the linear regression could be checked as below:

**1. Linearity**

We can create scatterplots to compare the continuous independent variable(s) with Sales to check the linearity assumption. In this regards, we can use the pairplot to verify the linearity assumption or create new scatterplots comparing the variables of interest.


.. code-block:: python


	# Create a scatterplot for each independent variable and the dependent variable.
	# Create a 1x2 plot figure.
	fig, axes = plt.subplots(1, 2, figsize = (8,4))

	# Create a scatterplot between Radio and Sales.
	sns.scatterplot(x = data['Radio'], y = data['Sales'],ax=axes[0], color = 'r')

	# Set the title of the first plot.
	axes[0].set_title("Radio and Sales")

	# Create a scatterplot between Social Media and Sales.
	sns.scatterplot(x = data['Social_Media'], y = data['Sales'],ax=axes[1], color = 'c')

	# Set the title of the second plot.
	axes[1].set_title("Social Media and Sales")

	# Set the xlabel of the second plot.
	axes[1].set_xlabel("Social Media")

	# Use matplotlib's tight_layout() function to add space between plots for a cleaner appearance.
	plt.tight_layout()

	plt.show()


Here is the output: 

.. figure:: PNG/38.png
   :align: center



The linearity assumption holds for Radio, as there is a clear linear relationship in the scatterplot between Radio and Sales. Social Media was not included in the multiple linear regression model above, but it does appear to have a linear relationship with Sales.



**2. Independence**


The independent observation assumption states that each observation in the dataset is independent. As each marketing promotion (i.e., row) is independent from one another, the independence assumption hols true. 


**3. Normality**

The normality assumption could e checked using histogram of the residuals or Q-Q plot of the residuals:


.. code-block:: python


	# Calculate the residuals.
	residuals = model.resid

	# Create a 1x2 plot figure.
	fig, axes = plt.subplots(1, 2, figsize = (8,4))

	# Create a histogram with the residuals. 
	sns.histplot(residuals, ax=axes[0])

	# Set the x label of the residual plot.
	axes[0].set_xlabel("Residual Value")

	# Set the title of the residual plot.
	axes[0].set_title("Histogram of Residuals")

	# Create a Q-Q plot of the residuals.
	sm.qqplot(residuals, line='s',ax = axes[1])

	# Set the title of the Q-Q plot.
	axes[1].set_title("Normal QQ Plot")

	# Use matplotlib's tight_layout() function to add space between plots for a cleaner appearance.
	plt.tight_layout()

	# Show the plot.
	plt.show()


Here is the output: 


.. figure:: PNG/39.png
   :align: center


According to the above graph, the histogram of the residuals are approximately normally distributed, which supports that the normality assumption is met for this model. The residuals in the Q-Q plot form a straight line, further supporting that this assumption is met.


**3. Constant variance**



We should create a scatterplot with the fitted values and residuals. Add a line at  ùë¶=0 to visualize the variance of residuals above and below  ùë¶=0:


.. code-block:: python


	# Create a scatterplot with the fitted values from the model and the residuals.
	fig = sns.scatterplot(x = model.fittedvalues, y = model.resid, color='g')

	# Set the x axis label.
	fig.set_xlabel("Fitted Values")

	# Set the y axis label.
	fig.set_ylabel("Residuals")

	# Set the title.
	fig.set_title("Fitted Values v. Residuals")

	# Add a line at y = 0 to visualize the variance of residuals above and below 0.
	fig.axhline(0)

	# Show the plot.
	plt.show()
	

Here is the output graph: 

.. figure:: PNG/40.png
   :align: center


The fitted values are in three groups because the categorical variable is dominating in this model, meaning that TV is the biggest factor that decides the sales.However, the variance where there are fitted values is similarly distributed, validating that the assumption is met.


**4. No multicollinearity**


.. note::

   The no multicollinearity assumption states that no two independent variables can be highly correlated with each other.Two common ways to check for multicollinearity are to:

	:math:`\bullet` Create scatterplots to show the relationship between pairs of independent variables

	:math:`\bullet` Use the variance inflation factor to detect multicollinearity


To check the variance inflation factor:


.. code-block:: python


	# Calculate the variance inflation factor (optional).
	# Import variance_inflation_factor from statsmodels.
	from statsmodels.stats.outliers_influence import variance_inflation_factor

	# Create a subset of the data with the continous independent variables. 
	X = data[['Radio','Social_Media']]

	# Calculate the variance inflation factor for each variable.
	vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

	# Create a DataFrame with the VIF results for the column names in X.
	df_vif = pd.DataFrame(vif, index=X.columns, columns = ['VIF'])

	# Display the VIF results.
	df_vif


Where the variance inflation factor is found equal to 4.93238 for both **Radio** and **Social_Media**.  


The model above only has one continuous independent variable (Only the Radio is continuous while TV is NOT), meaning there are no multicollinearity issues.If a model used both Radio and Social_Media as predictors, there would be a moderate linear relationship between Radio and Social_Media that violates the multicollinearity assumption. 

According to the summary of the model presented before: 


Using TV and Radio as the independent variables results in a multiple linear regression model with  :math:`R^{2}=0.904`. In other words, the model explains  90.4% of the variation in Sales. This makes the model an excellent predictor of Sales.

When TV and Radio are used to predict Sales, the model coefficients are:

:math:`\bullet` :math:`\beta_{0}=218.5261`

:math:`\bullet` :math:`\beta_{ùëáùëâùêøùëúùë§}=‚àí154.2971`

:math:`\bullet` :math:`\beta_{TùëâùëÄùëíùëëùëñùë¢ùëö}=‚àí75.3120`

:math:`\bullet` :math:`\beta_{ùëÖùëéùëëùëñùëú}=2.9669`

Now we can define the Sales as below:


.. math:: 
  :name: eq.115

   Sales= \beta_{0} + \beta_{ùëáùëâùêøùëúùë§} * X_{ùëáùëâùêøùëúùë§} +\beta_{ùëÄùëíùëëùëñùë¢ùëö} * X_{ùëáùëâùëÄùëíùëëùëñùë¢ùëö} + \beta_{ùëÖùëéùëëùëñùëú} * X_{ùëÖùëéùëëùëñùëú}

The default TV category for the model is High since there are coefficients for the other two TV categories, Medium and Low. Because the coefficients for the Medium and Low TV categories are negative, that means the average of sales is lower for Medium or Low TV categories compared to the High TV category when Radio is at the same level.


The p-value for all coefficients is  0.000, meaning all coefficients are statistically significant at  ùëù=0.05. The 95% confidence intervals for each coefficient should be reported when presenting results.

For example, there is a  95% chance that the interval  [‚àí163.979,‚àí144.616] contains the true parameter of the slope of  :math:`\beta_{ùëáùëâùêøùëúùë§}`, which is the estimated difference in promotion sales when a Low TV promotion is chosen instead of a High TV promotion.


.. note::


	Beta coefficients allow you to estimate the magnitude and direction (positive or negative) of the effect of each independent variable on the dependent variable. The coefficient estimates can be converted to explainable insights, such as the connection between an increase in TV promotional budgets and sales mentioned above.


6.3. Hypothesis testing
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^



Analysis of variance (commonly called ANOVA) is a group of statistical techniques that test the difference of means among three or more groups. It's a powerful tool for determining whether population means are different across groups. 


In the dataset used in this section, each row corresponds to an independent marketing promotion and there are 4 factors including TV, social media, radio, and influencer promotions to increase sales. We want to know if sales are significantly different among various **categorical variables** including TV and influencer promotion types.
In this regards, a one-way ANOVA test will be used to determine if there is a statistically significant difference in sales among groups.
Here are the steps we should take: 

:math:`\bullet` Using plots and descriptive statistics to select a categorical independent variable

:math:`\bullet` Creating and fitting a linear regression model with the selected categorical independent variable

:math:`\bullet` Checking model assumptions

:math:`\bullet` Performing and interpreting a one-way ANOVA test

:math:`\bullet` Comparing pairs of groups using an ANOVA post hoc test

:math:`\bullet` Interpreting model outputs and communicating the results to nontechnical stakeholders

First we need to import the required packages: 



.. code-block:: python

	# Import libraries and packages.
	import pandas as pd
	import matplotlib.pyplot as plt
	import seaborn as sns
	import statsmodels.api as sm
	from statsmodels.formula.api import ols
	from statsmodels.stats.multicomp import pairwise_tukeyhsd

The we load our dataset: 



.. code-block:: python

	# Load the data.
	data = pd.read_csv('marketing_sales_data.csv')


The features in the data are:

**1.** TV promotion budget (in Low, Medium, and High categories)

**2.** Social media promotion budget (in millions of dollars)

**3.** Radio promotion budget (in millions of dollars)

**4.** Sales (in millions of dollars)

**5.** Influencer size (in Mega, Macro, Nano, and Micro categories)



.. note::

   This is important to perform exploratory data analysis (EDA) before constructing a linear regression model for the below purposes:

        :math:`\bullet` To understand which variables are present in the data

	:math:`\bullet` To consider the distribution of features, such as minimum, mean, and maximum values

	:math:`\bullet` To plot the relationship between the independent and dependent variables and visualize which features have a linear relationship

	:math:`\bullet` To identify issues with the data, such as incorrect or missing values.


Now we can use a boxplot to determine how Sales vary based on the TV promotion budget category:


.. code-block:: python


	# Create a boxplot with TV and Sales.
	my_pal = {"Low": "c", "Medium": "y", "High":"m"}
	sns.boxplot(x = "TV", y = "Sales", data = data,palette=my_pal);


Here is the output of the above graph:


.. figure:: PNG/41.png
   :align: center



There is considerable variation in Sales across the TV groups. The significance of these differences can be tested with a one-way ANOVA.

Now, we can plot the same graph for the Influencer category:

.. code-block:: python


	# Create a boxplot with Influencer and Sales.
	sns.boxplot(x = "Influencer", y = "Sales", data = data);

The output of the above is presented here: 


.. figure:: PNG/42.png
   :align: center

According to the above graph, there is some variation in Sales across the Influencer groups, but it may not be significant.

It should be noted that the dataset may contain missing rows. To correct this, we should drop these rows:


.. code-block:: python

	# Drop rows that contain missing data and update the DataFrame.
	data = data.dropna(axis=0)


	# Confirm the data contain no missing values.
	data.isnull().sum(axis=1)


Now we want to fit a linear regression model that predicts Sales using one of the independent **categorical** variables in data:

.. code-block:: python


	# Define the OLS formula.
	ols_formula = 'Sales ~ C(TV)'

	# Create an OLS model.
	OLS = ols(formula = ols_formula, data = data)

	# Fit the model.
	model = OLS.fit()

	# Save the results summary.
	model_results = model.summary()

	# Display the model results.
	model_results

The output is here:


.. figure:: PNG/43.png
   :align: center

TV was selected as the analysis above showed a strong relationship between the TV promotion budget and the average Sales. Influencer was not selected because it did not show a strong relationship to Sales in the analysis.


Now we should check the model assumptions:

**1. Linearity**

Because your model does not have any continuous independent variables, the linearity assumption is not required.

**2. Independency**

The independent observation assumption states that each observation in the dataset is independent. As each marketing promotion (row) is independent from one another, the independence assumption is not violated.


**3. Normality**

To check the normality we can plot a histogram or create a Q-Q plot:


.. code-block:: python

	# Calculate the residuals.
	residuals = model.resid

	# Create a 1x2 plot figure.
	fig, axes = plt.subplots(1, 2, figsize = (8,4))

	# Create a histogram with the residuals.
	sns.histplot(residuals, ax=axes[0],color = 'm')

	# Set the x label of the residual plot.
	axes[0].set_xlabel("Residual Value")

	# Set the title of the residual plot.
	axes[0].set_title("Histogram of Residuals")

	# Create a QQ plot of the residuals.
	sm.qqplot(residuals, line='s',ax = axes[1])

	# Set the title of the QQ plot.
	axes[1].set_title("Normal QQ Plot")

	# Use matplotlib's tight_layout() function to add space between plots for a cleaner appearance.
	plt.tight_layout()

	# Show the plot.
	plt.show()


Here is the output:


.. figure:: PNG/44.png
   :align: center


According to the above graph, there is reasonable concern that the normality assumption is not met when TV is used as the independent variable predicting Sales. The normal q-q forms an 'S' that deviates off the red diagonal line, which is not desired behavior.


**4. Constant variance (homoscedasticity)**


To check this condition we can make a scatterplot (Residuals vs Fitted Values):


.. code-block:: python


	# Create a scatter plot with the fitted values from the model and the residuals.
	fig = sns.scatterplot(x = model.fittedvalues, y = model.resid)

	# Set the x axis label
	fig.set_xlabel("Fitted Values")

	# Set the y axis label
	fig.set_ylabel("Residuals")

	# Set the title
	fig.set_title("Fitted Values v. Residuals")

	# Add a line at y = 0 to visualize the variance of residuals above and below 0.
	fig.axhline(0)

	# Show the plot
	plt.show()

This is the output:


.. figure:: PNG/45.png
   :align: center


Based on the above graph, the variance where there are fitted values is similarly distributed, validating that the constant variance assumption is met.

Using TV as the independent variable results in a linear regression model with  :math:`R^{2}=0.871`. In other words, the model explains  86.1% of the variation in Sales. This makes the model an effective predictor of Sales.



.. note::


	The default TV category for the model is High, because there are coefficients for the other two TV categories, Medium and Low. According to the model, Sales with a Medium or Low TV category are lower on average than Sales with a High TV category. For example, the model predicts that a Low TV promotion would be 209.8691 (in millions of dollars) lower in Sales on average than a High TV promotion.


	The p-value for all coefficients is  0.000 , meaning all coefficients are statistically significant at  ùëù=0.05. The 95% confidence intervals for each coefficient should be reported when presenting results to stakeholders. For instance, there is a  95% chance the interval  [‚àí216.535,‚àí203.203] contains the true parameter of the slope of  ùõΩùëáùëâùêøùëúùë§ , which is the estimated difference in promotion sales when a Low TV promotion is chosen instead of a High TV promotion.



After fitting the model, we can run a one-way ANOVA test to determine whether there is a statistically significant difference in Sales among groups:

.. code-block:: python

	# Create an one-way ANOVA table for the fit model.
	sm.stats.anova_lm(model, typ = 1)

The output is presented here: 


.. figure:: PNG/46.png
   :align: center


In this example:

:math:`\bullet` The null hypothesis is that there is no difference in Sales based on the TV promotion budget.

:math:`\bullet` The alternative hypothesis is that there is a difference in Sales based on the TV promotion budget.


The F-test statistic is 1916.75 and the p-value is  1.38E‚àí253 (i.e., very small). Because the p-value is less than 0.05, you would reject the null hypothesis that there is no difference in Sales based on the TV promotion budget.


**ANOVA post hoc test**

We can apply ANOVA post hoc tests such as the Tukey‚Äôs HSD post hoc test to compare if there is a significant difference between each pair of categories for TV.



.. code-block:: python

	# Perform the Tukey's HSD post hoc test.
	tukey_oneway = pairwise_tukeyhsd(endog = data["Sales"], groups = data["TV"])

	# Display the results
	tukey_oneway.summary()

Here is the output: 

.. figure:: PNG/46.png
   :align: center

According to the above graph, The first row, which compares the High and Low TV groups, indicates that you can reject the null hypothesis that there is no significant difference between the Sales of these two groups.

We can also reject the null hypotheses for the two other pairwise comparisons that compare High to Medium and Low to Medium.


.. note::

   A post hoc test was conducted to determine which TV groups are different and how many are different from each other. This provides more detail than the **one-way ANOVA** results, **which can at most determine that at least one group is different**. Further, using the Tukey HSD controls for the increasing probability of incorrectly rejecting a null hypothesis from peforming multiple tests.


The following are estimates for the average difference between each pair of TV promotions:

:math:`\bullet` Estimated average difference between High and Low TV promotions: $209.87 million (with 95% confidence that the exact value for this average difference is between 201.89 and 216.84 million dollars).

:math:`\bullet` Estimated average difference between High and Medium TV promotions: $105.50 million (with 95% confidence that the exact value for this average difference is between 96.56 and 113.43 million dollars).

:math:`\bullet` Estimated average difference between Medium and Low TV promotions: $104.37 million (with 95% confidence that the exact value for this average difference is between 96.83 and 111.92 million dollars).


Finally, The linear regression model estimating Sales from TV had an R-squared of $0.871, making it a fairly accurate estimator. The model showed a statistically significant relationship between the TV promotion budget and Sales. The model estimated the following relationships:

:math:`\bullet` Using a high TV promotion budget instead of a medium TV promotion budget increased sales by 105.4952 million dollars (95% CI - 98.859, 112.131 million dollars).

:math:`\bullet` Using a high TV promotion budget instead of a low TV promotion budget increased sales by 209.8691 million dollars (95% CI - 203.203 million, 216.535 million dollars).


6.4. Logistic regression
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In this section we will conduct a bionomial logistic regression. 

**Bionomial Logistic Regression**: This is a technique that models probability of an observation falling into one of two categories based on one or more independent variable. The below figure, compares the linear regression and logistic regression models:


.. figure:: PNG/49.png
   :align: center


The data for this section come from an airline. We are interested in knowing if a better in-flight entertainment experience leads to higher customer satisfaction. So we will construct and evaluate a model that predicts whether a future customer would be satisfied with their services given previous customer feedback about their flight experience. 

The data for this activity is for a sample size of 129,880 customers. It includes data points such as class, flight distance, and in-flight entertainment, among others. The goal is to utilize a binomial logistic regression model to help the airline model and better understand this data.

Here are the steps we should take: 

:math:`\bullet` Importing packages and loading data

:math:`\bullet` Exploring the data and completing the cleaning process

:math:`\bullet` Building a binomial logistic regression model

:math:`\bullet` Evaluating a binomial logistic regression model using a confusion matrix

First we need to import the required packages: 

.. code-block:: python


	# Standard operational package imports.
	import numpy as np
	import pandas as pd

	# Important imports for preprocessing, modeling, and evaluation.
	from sklearn.preprocessing import OneHotEncoder
	from sklearn.model_selection import train_test_split
	from sklearn.linear_model import LogisticRegression
	import sklearn.metrics as metrics

	# Visualization package imports.
	import matplotlib.pyplot as plt
	import seaborn as sns


Next we should load the data:


.. code-block:: python

	df_original = pd.read_csv("Invistico_Airline.csv")


We need to prepare the data in way that they are suitable for binomial logistic regression analysis. Here are actions we should take regarding the data:


:math:`\bullet` Exploring the data

:math:`\bullet` Checking for missing values

:math:`\bullet` Encoding the data

:math:`\bullet` Renaming a column

:math:`\bullet` Creating the training and testing data


We can extract the information explaining how many customers in the dataset are satisfied before modeling:


.. code-block:: python


	df_original['satisfaction'].value_counts(dropna = False)


Based on the output of the above line we can see there were 71,087 satisfied customers and 58,793 dissatisfied customers. In other words, 54.7 percent (71,087/129,880) of customers were satisfied. 


An assumption of logistic regression models is that there are no missing values. Check for missing values in the rows of the data.


.. code-block:: python

	df_original.isnull().sum()


There are only 393 missing values out of the total of 129,880, so these are a small percentage of the total. This column might impact the relationship between entertainment and satisfaction. Drop the rows with missing values and save the resulting pandas DataFrame in a variable named df_subset:


.. code-block:: python

	df_subset = df_original.dropna(axis=0).reset_index(drop = True)

If you want to create a plot (sns.regplot) of your model to visualize results later, the independent variable Inflight entertainment cannot be "of type int" and the dependent variable satisfaction cannot be "of type object. "Make the Inflight entertainment" column "of type float."

.. code-block:: python

	df_subset = df_subset.astype({"Inflight entertainment": float})

In the next step, we need to convert the categorical column satisfaction into numeric through one-hot encoding:

.. code-block:: python

	df_subset['satisfaction'] = OneHotEncoder(drop='first').fit_transform(df_subset[['satisfaction']]).toarray()


To examine what one-hot encoding did to the DataFrame, output the first 100 rows of df_subset:

.. code-block:: python

	df_subset.head(100)

Under the satisfaction column, if the customer is satisfied, it will be appear as 1.0 and if is NOT satisfied it will appear as 0.0. 


Next we put 70% of the data into a training set and the remaining 30% into a testing set. Create an X and y DataFrame with only the necessary variables (We want to use inflight entertainment as the only independent variable):


.. code-block:: python

	X = df_subset[["Inflight entertainment"]]
	y = df_subset["satisfaction"]
	X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)

Now we build a logistic regression model and fit the model to the training data:

.. code-block:: python

	clf = LogisticRegression().fit(X_train,y_train)


Now we create a plot of your model to visualize results using the seaborn package:


.. code-block:: python

	sns.regplot(x="Inflight entertainment", y="satisfaction", data=df_subset, logistic=True, ci=None,color='m')



Here is the output: 


.. figure:: PNG/48.png
   :align: center


The graph seems to indicate that the higher the inflight entertainment value, the higher the customer satisfaction, though this is currently not the most informative plot. The graph currently doesn't provide much insight into the data points, as Inflight entertainment is categorical.



**Predict the outcome for the test dataset**

Now we input the holdout dataset into the predict function to get the predicted labels from the model.


.. code-block:: python

	# Save predictions.
	y_pred = clf.predict(X_test)
	print(y_pred)



.. note::

   When you decide to reject or fail to reject the null hypothesis, there are four possible outcomes‚Äìtwo represent correct choices, and two represent errors. You can: 


	:math:`\bullet` Reject the null hypothesis when it‚Äôs actually true (Type I error) (**False Positive**)

	:math:`\bullet` Reject the null hypothesis when it‚Äôs actually false (Correct) (**True Positive**)

	:math:`\bullet` Fail to reject the null hypothesis when it‚Äôs actually true (Correct) (**True Negative**)

	:math:`\bullet` Fail to reject the null hypothesis when it‚Äôs actually false (Type II error) (**False Negative**)


This is clarified more in details in this figure:



.. figure:: PNG/50.png
   :align: center



.. note::

   There are three metrics that we use to evaluate the model prediction including: 

	:math:`\bullet` **Precision:** Proportion of the positive prediction that were actually positive = :math:`\frac {True Positive}{True Positive+False Positive}`

	
	:math:`\bullet` **Recall:** Proportion of the positives that the model was able to identify correctly = :math:`\frac {True Positive}{True Positive+False Negative}`

	:math:`\bullet` **Accuracy:** Proportion of the data points that were categorized correctly = :math:`\frac {True Positive+True Negative}{Total Predictions}`



We can print out the model's accuracy, precision, recall, and F1 score:


.. code-block:: python

	print("Accuracy:", "%.6f" % metrics.accuracy_score(y_test, y_pred))
	print("Precision:", "%.6f" % metrics.precision_score(y_test, y_pred))
	print("Recall:", "%.6f" % metrics.recall_score(y_test, y_pred))

The output is: 


.. code-block:: python

	Accuracy: 0.801529
	Precision: 0.816142
	Recall: 0.821530


**Produce a confusion matrix**

Confusion matrix is a graphical representation of how accurate a classified is at predicting the lables for a categorical variable. The confusion matrix is shown in this figure: 

.. figure:: PNG/51.png
   :align: center


Now we want to know the types of errors made by an algorithm. To obtain this information, produce a confusion matrix:

.. code-block:: python

	cm = metrics.confusion_matrix(y_test, y_pred, labels = clf.classes_)
	disp = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels = clf.classes_)
	disp.plot()


Here is the output:

.. figure:: PNG/52.png
   :align: center

The lower left and top right corners are both under 4,000, which are relatively low numbers. Based on what we know from the data and interpreting the matrix, it's clear that these numbers relate to false positives and false negatives.Additionally, the other two quadrants‚Äîthe true positives and true negatives‚Äîare both high numbers above 13,000.



.. note::

   Using more than a single independent variable in the model training process could improve model performance. This is because other variables, like Departure Delay in Minutes, seem like they could potentially influence customer satisfaction.










